{"cells":[{"cell_type":"markdown","metadata":{"id":"7mKK4Rd8vhPS"},"source":["#  **Assignment 1**\n","\n","**Your name: <enter 朱柏諺 Brian>**\n","\n","**Registered email: brianchu1021@gmail.com**\n","\n","**Class: E1**\n","\n","In this homework, you will build a model for predicting house prices. For this, you will use real house sale data from a [Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).\n","\n","Your task is to:\n","1. Develop a better model to reduce the prediction error. You can find some hints on the last section.\n","2. Submit your results to Kaggle and take a sceenshot of your score. Then insert here the screenshot of your result.\n","\n","It is important that you start as earlier as possible. Tuning hyper-parameters takes time, and Kaggle limits the number of submissions per day."]},{"cell_type":"code","execution_count":1281,"metadata":{"id":"GRQwEMHBvhPU","scrolled":true},"outputs":[],"source":["# If pandas is not installed, please uncomment the following line:\n","# !pip install pandas\n","\n","%matplotlib inline\n","import os\n","import numpy as np\n","import torch\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"bwaYbzRsaZVM"},"source":["## Preliminary Setup"]},{"cell_type":"code","execution_count":1282,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1702446507057,"user":{"displayName":"Joc Cing Tay","userId":"08673849622691221515"},"user_tz":-480},"id":"nl_s9EDyaWxm","outputId":"5cf5a781-a924-4aa2-fb05-f055ee6dae04"},"outputs":[{"name":"stdout","output_type":"stream","text":["No GPU available, using CPU.\n"]}],"source":["n_cudas = torch.cuda.device_count()\n","for i in range(n_cudas):\n","    print(f'{i+1} GPU: {torch.cuda.get_device_name(i)}')\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"No GPU available, using CPU.\")"]},{"cell_type":"markdown","metadata":{"id":"9Lcu72WatdXT"},"source":["## Get the training and test dataset from Kaggle"]},{"cell_type":"code","execution_count":1283,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1265,"status":"ok","timestamp":1702446511769,"user":{"displayName":"Joc Cing Tay","userId":"08673849622691221515"},"user_tz":-480},"id":"HTZ5o7sPYjQX","outputId":"032c2127-cb88-44fb-fd22-b6050540246d"},"outputs":[],"source":["# try:\n","# import google.colab\n","# import requests\n","# url = 'https://raw.githubusercontent.com/joccing/ICT303-assignment1/master/config.py'\n","# r = requests.get(url, allow_redirects=True)\n","# open('config.py', 'wb').write(r.content)\n","# except ModuleNotFoundError:\n","#     pass\n","# from config_module import *\n","# config_data()"]},{"cell_type":"markdown","metadata":{"id":"NEef4084vhPV"},"source":["Download the train and test dataset from [Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) (in the Data tab). You will need to sign it to access the data. These are two Comma Separated Values (CSV) files containing training and test data respectively we use Pandas.\n","\n","Save them into your google drive. To read them into your code, use the following command (from panda)"]},{"cell_type":"code","execution_count":1284,"metadata":{"id":"xoPKaTR1vhPW"},"outputs":[],"source":["train_data = pd.read_csv('data/train.csv')  # change the name and the path as needed\n","test_data  = pd.read_csv('data/test.csv')"]},{"cell_type":"markdown","metadata":{"id":"BjAF_MkDvhPW"},"source":["The training data set includes 1,460 examples, 80 features, and 1 label. The test data contains 1,459 examples and 80 features. You can check the content of the files by opening them in Excel or by using the following two commands"]},{"cell_type":"code","execution_count":1285,"metadata":{"id":"xZqqYl11vhPW"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1460, 81)\n","(1459, 80)\n"]}],"source":["print(train_data.shape)\n","print(test_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"te8qM3wFvhPX"},"source":["Let’s take a look at the first 4 and last 2 features as well as the label (SalePrice) from the first 4 examples:"]},{"cell_type":"code","execution_count":1286,"metadata":{"id":"0qmO3cWrvhPd"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>MSSubClass</th>\n","      <th>MSZoning</th>\n","      <th>LotFrontage</th>\n","      <th>SaleType</th>\n","      <th>SaleCondition</th>\n","      <th>SalePrice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>65.0</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>208500</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>80.0</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>181500</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>68.0</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>223500</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>70</td>\n","      <td>RL</td>\n","      <td>60.0</td>\n","      <td>WD</td>\n","      <td>Abnorml</td>\n","      <td>140000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n","0   1          60       RL         65.0       WD        Normal     208500\n","1   2          20       RL         80.0       WD        Normal     181500\n","2   3          60       RL         68.0       WD        Normal     223500\n","3   4          70       RL         60.0       WD       Abnorml     140000"]},"execution_count":1286,"metadata":{},"output_type":"execute_result"}],"source":["train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]"]},{"cell_type":"markdown","metadata":{"id":"Rytz3kDFvhPd"},"source":["We can see that in each example, the first feature is the ID. This helps the model identify each training example. While this is convenient, it doesn't carry any information for prediction purposes. Hence we remove it from the dataset before feeding the data into the network."]},{"cell_type":"code","execution_count":1287,"metadata":{"id":"Fd1k2_UAvhPe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n","       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n","       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n","       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n","       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n","       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n","       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n","       'MoSold', 'YrSold'],\n","      dtype='object')\n"]},{"data":{"text/plain":["(2919, 79)"]},"execution_count":1287,"metadata":{},"output_type":"execute_result"}],"source":["all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n","print(all_features.dtypes[all_features.dtypes != 'object'].index)\n","all_features.shape"]},{"cell_type":"markdown","metadata":{"id":"1MbbJCesdao-"},"source":["Non Numeric Feature Example"]},{"cell_type":"code","execution_count":1288,"metadata":{"id":"abf5wEZvdfD5"},"outputs":[{"data":{"text/plain":["0       RL\n","1       RL\n","2       RL\n","3       RL\n","4       RL\n","        ..\n","1454    RM\n","1455    RM\n","1456    RL\n","1457    RL\n","1458    RL\n","Name: MSZoning, Length: 2919, dtype: object"]},"execution_count":1288,"metadata":{},"output_type":"execute_result"}],"source":["all_features['MSZoning']"]},{"cell_type":"markdown","metadata":{"id":"1niqZY8axpDP"},"source":["## **1. Accessing and Reading the Kaggle Data Sets**\n","\n","The competition data is separated into training and test sets. Each record includes the property values of the house and attributes such as street type, year of construction, roof type, basement condition.\n","\n","The data includes multiple data types, including integers (year of construction), discrete labels (roof type), floating point numbers, etc.; Some data is missing and is thus labeled 'na'.\n","\n","The price of each house, namely the label, is only included in the training data set (it is a competition after all). The 'Data' tab on the competition tab has links to download the data.\n","\n","We will read and process the data using `pandas`, an [efficient data analysis toolkit](http://pandas.pydata.org/pandas-docs/stable/). It is already part of colab."]},{"cell_type":"markdown","metadata":{"id":"2TCTGRndvhPe"},"source":["## **2. Data Preprocessing**\n","\n","As stated above, we have a wide variety of datatypes. Before we feed it into a  network we need to perform some amount of processing.\n","\n","Let's start with the **numerical features**. We begin by replacing missing values with the mean. This is a reasonable strategy if features are missing at random. To adjust them to a common scale we rescale them to zero mean and unit variance. This is accomplished as follows:\n","\n","$$x \\leftarrow \\frac{x - \\mu}{\\sigma}.$$\n","\n","This process is called **normalization** or **standardisation**. The reason for normalizing the data is that it brings all features to the same order of magnitude. After all, we do not know *a priori* which features are likely to be relevant. Hence it makes sense to treat them equally.\n","\n","Write below the code that does the normalization for each numerical features:"]},{"cell_type":"markdown","metadata":{},"source":["## Data Balancing"]},{"cell_type":"code","execution_count":1289,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1687265119415,"user":{"displayName":"Joc Cing Tay","userId":"09116233010871039567"},"user_tz":-480},"id":"QE65QBNzvhPf","outputId":"52cf3125-7e3b-4bf6-a22a-e09a096ab151"},"outputs":[{"name":"stderr","output_type":"stream","text":["dist_matrix:   7%|6         | 10/151 [00:00<00:12, 11.35it/s]"]},{"name":"stderr","output_type":"stream","text":["dist_matrix: 100%|##########| 151/151 [00:12<00:00, 12.19it/s]\n","synth_matrix: 100%|##########| 151/151 [00:04<00:00, 35.50it/s]\n","r_index: 100%|##########| 126/126 [00:01<00:00, 99.80it/s]\n"]}],"source":["import smogn\n","# source: https://github.com/nickkunz/smogn/blob/master/examples/smogn_example_3_adv.ipynb\n","rg_mtrx = [\n","\n","    [35000,  1, 0],  ## over-sample (\"minority\")\n","    [125000, 0, 0],  ## under-sample (\"majority\")\n","    [200000, 0, 0],  ## under-sample\n","    [250000, 0, 0],  ## under-sample\n","]\n","# conduct smogn\n","train_data = smogn.smoter(\n","    \n","    ## main arguments\n","    data = train_data,           ## pandas dataframe\n","    y = 'SalePrice',          ## string ('header name')\n","    k = 9,                    ## positive integer (k < n)\n","    pert = 0.04,              ## real number (0 < R < 1)\n","    samp_method = 'balance',  ## string ('balance' or 'extreme')\n","    drop_na_col = True,       ## boolean (True or False)\n","    drop_na_row = True,       ## boolean (True or False)\n","    replace = False,          ## boolean (True or False)\n","\n","    ## phi relevance arguments\n","    rel_thres = 0.10,         ## real number (0 < R < 1)\n","    rel_method = 'manual',    ## string ('auto' or 'manual')\n","    rel_xtrm_type = 'both', ## unused (rel_method = 'manual')\n","    rel_coef = 1.50,        ## unused (rel_method = 'manual')\n","    rel_ctrl_pts_rg = rg_mtrx ## 2d array (format: [x, y])\n",")\n"]},{"cell_type":"code","execution_count":1290,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1309, 60) (1309, 1)\n"]}],"source":["from numpy import nan\n","train_data.reset_index(drop=True, inplace=True)\n","train_features = train_data.iloc[:, 1:-1].copy(deep=True)\n","train_labels   = train_data.iloc[train_features.index,-1:].copy(deep = True)\n","# train_features.replace('NaN', nan)\n","train_features.dropna(thresh=7, inplace=True) # drop rows having more than 7 missing values\n","# train_features.reset_index(drop = True, inplace = True)\n","train_labels = train_labels.iloc[train_features.index,-1:]\n","# remove outliers\n","# from sklearn.ensemble import IsolationForest\n","# numeric_features = train_features.dtypes[train_features.dtypes != 'object'].index\n","# clf = IsolationForest(max_samples = 100, random_state = 42)\n","# clf.fit(train_features[numeric_features])\n","# y_noano = clf.predict(train_features[numeric_features])\n","# y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n","\n","# train_features = train_features.iloc[y_noano[y_noano['Top'] == 1].index.values]\n","\n","n_train = train_features.shape[0]\n","print(train_features.shape, train_labels.shape)\n","train_labels.reset_index(drop=True, inplace=True)\n","train_features.reset_index(drop=True, inplace=True)\n","test_features = test_data.iloc[:, 1:]\n","test_features = test_features[train_features.columns]\n","all_features = pd.concat([train_features, test_features], axis=0)\n","numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n","# all_features[numeric_features].fillna(0, inplace=True)"]},{"cell_type":"code","execution_count":1291,"metadata":{"id":"ih40N6ikvhPf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of all features: (2768, 236)\n","Shape of all training label: (1309, 1)\n"]}],"source":["# Dummy_na=True refers to a missing value being a legal eigenvalue, and creates an indicative feature for it.\n","all_features = pd.get_dummies(all_features, dummy_na=True)\n","print(f'Shape of all features: {all_features.shape}')\n","print(f'Shape of all training label: {train_labels.shape}')\n","# print(all_features)\n","# print(f'Shape of non-processed training: {train_data.shape}')\n","# print(f'Shape of non-processed test: {test_data.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["## Missing Value Handling using MICE (numeric)\n","(Though not using mean has a better performance lol)"]},{"cell_type":"code","execution_count":1292,"metadata":{},"outputs":[],"source":["\n","import miceforest as mf\n","numeric_features = all_features.dtypes[all_features.dtypes != 'bool'].index\n","# all_features.\n","# print(numeric_features)\n","for col in numeric_features:\n","    all_features[col] = pd.to_numeric(all_features[col])\n","    # all_features[col] = all_features[col].fillna(all_features[col].mean())\n","# for col in set(all_features.columns).difference(set(numeric_features)):\n","#     all_features[col] = pd.Categorical(all_features[col])\n","\n","# print(all_features.shape)\n","# print(list(all_features.isna().sum(axis=0)))\n","\n","kds = mf.ImputationKernel(\n","  all_features[numeric_features],\n","  random_state=5\n",")\n","# Run the MICE algorithm for 5 iterations\n","kds.mice(10)\n","\n","all_features[numeric_features] = kds.complete_data()\n","\n","\n","# Set missing values to 0\n","# all_features = all_features.fillna(0)\n","all_features.shape\n","\n","# Normalize the numerical features\n","all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))"]},{"cell_type":"code","execution_count":1293,"metadata":{"id":"VoYULym-mSv4"},"outputs":[],"source":["# Examine data after normalizing numeric features\n","# all_features.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]\n","# print(all_features)"]},{"cell_type":"markdown","metadata":{"id":"lazSNir9vhPf"},"source":["Next we deal with the **discrete values**. This includes variables such as 'MSZoning'. We replace them by a one-hot encoding in the same manner as how we transformed multiclass classification data into a vector of $0$ and $1$. For instance, 'MSZoning' assumes the values 'RL' and 'RM'. They map into vectors $(1,0)$ and $(0,1)$ respectively. Pandas does this automatically for us."]},{"cell_type":"code","execution_count":1294,"metadata":{"id":"jFyuKq4Bn9E5"},"outputs":[],"source":["# Examine data after one hot encoding\n","# all_features.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]"]},{"cell_type":"code","execution_count":1295,"metadata":{"id":"rSNx812EoIcu"},"outputs":[],"source":["# all_features['MSZoning_RL']"]},{"cell_type":"markdown","metadata":{"id":"n8WR4gEbvhPg"},"source":["You can see that this conversion increases the number of features from 79 to 331.\n","\n","Finally, via the `values` attribute we can extract the NumPy format from the Pandas dataframe and prepare them for training."]},{"cell_type":"code","execution_count":1296,"metadata":{"id":"TJmbnw2cvhPg"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","(1309, 236)\n","(1459, 236)\n"]}],"source":["print((all_features['Street_nan'].isna().sum()))\n","# all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n","# the train feature are in all_features[:n_train].values - need to convert them into a pytorch tensor??\n","train_features = np.array(all_features[:n_train].values).astype(np.float32)\n","print(train_features.shape)\n","# the test feature are in all_features[n_train:].values - need to convert them into a pytorch tensor??\n","test_features  = np.array(all_features[n_train:].values).astype(np.float32)\n","print(test_features.shape)\n","# the train labels are in train_data.SalePrice.values - need to convert them into a pytorch tensor??\n","train_labels   = np.array(train_labels).reshape((-1, 1)).astype(np.float32)\n","# print(pd.DataFrame(test_features).isna().sum().sum())\n","# print(pd.DataFrame(train_features).isna().sum())\n","# print(pd.DataFrame(train_labels).isna().sum())"]},{"cell_type":"markdown","metadata":{"id":"yqWxMu7qvhPg"},"source":["## **3. Training**\n","\n","### **3.1. Loss function**\n","To get started, train an MLP model with squared loss. This will obviously not lead to a competition winning submission but it provides a sanity check to see whether there's meaningful information in the data. It also amounts to a minimum baseline of how well we should expect any 'fancy' model to work.\n","\n","To this end, use the codes you developed in Lab 4. However, we may need to test with a better loss function to achieve better results. In fact, house prices, like shares, are relative. That is, we probably care more about the relative error $\\frac{y - \\hat{y}}{y}$ than about the absolute error. For instance, getting a house price wrong by USD 100,000 is terrible in Rural Ohio, where the value of the house is USD 125,000. On the other hand, if we err by this amount in Los Altos Hills, California, we can be proud of the accuracy of our model (the median house price there exceeds 4 million).\n","\n","One way to address this problem is to measure the discrepancy between the predicted price and the correct price in the logarithm of the price estimates. In fact, this is also the error that is being used to measure the quality in the Kaggle competition. After all, a small value $\\delta$ of $\\log y - \\log \\hat{y}$ translates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$. This leads to the following loss function:\n","\n","$$L = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}$$"]},{"cell_type":"markdown","metadata":{},"source":["## RMSE Loss\n","Use RMSE to train the model"]},{"cell_type":"code","execution_count":1297,"metadata":{},"outputs":[],"source":["from torch import nn\n","\n","class RMSLELoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","        \n","    def forward(self, pred, actual):\n","        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"]},{"cell_type":"markdown","metadata":{},"source":["## MLP Model & Trainer"]},{"cell_type":"code","execution_count":1298,"metadata":{},"outputs":[],"source":["\n","\n","class MLP(nn.Module):\n","  '''\n","    Multilayer Perceptron.\n","  '''\n","  \n","  def __init__(self, inputSize=32 * 32 * 3, outputSize=10, lr=0.01, weight_decay = 0):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","      nn.Flatten(),\n","      nn.Linear(inputSize, 128),\n","      nn.Dropout(0.1),\n","      nn.ReLU(),\n","      nn.Linear(128, 64),\n","      nn.Dropout(0.2),\n","      nn.ReLU(),\n","      nn.Linear(64, 32),\n","      nn.ReLU(),\n","      nn.Dropout(0.1),\n","      nn.Linear(32, 16),\n","      nn.ReLU(),\n","      nn.Dropout(0.1),\n","      nn.Linear(16, 8),\n","      nn.ReLU(),\n","      nn.Dropout(0.1),\n","      nn.Linear(8, outputSize),\n","    )\n","    # Setting the learning rate\n","    self.lr = lr\n","    self.weight_decay = weight_decay\n","\n","  ## The forward step\n","  def forward(self, X):\n","    # Computes the output given the input X\n","    return self.layers(X)\n","\n","  ## The loss function - Here, we will use MSLE\n","  def loss(self, y_hat, y):\n","    fn = RMSLELoss()\n","    return fn(y_hat, y)\n","\n","  ## The optimization algorithm\n","  #  Let's this time use Adam, which is the most commonly used optimizer in neural networks\n","  def configure_optimizers(self):\n","    # return torch.optim.SGD(self.parameters(), self.lr)\n","    return torch.optim.Adam(self.parameters(), self.lr, weight_decay=self.weight_decay)\n","\n","class Trainer:\n","    def __init__(self, n_epochs=3):\n","        self.max_epochs = n_epochs\n","        self.train_losses = []  # 保存每個 epoch 的訓練損失\n","        return\n","\n","    def fit(self, model, trainloader, validloader=None):\n","        self.optimizer = model.configure_optimizers()\n","        self.model = model\n","\n","        for epoch in range(self.max_epochs):\n","            self.fit_epoch(trainloader)\n","\n","            # 計算並保存訓練損失\n","            avg_train_loss = sum(self.train_losses[-len(trainloader):]) / len(trainloader)\n","            print(f'Epoch {epoch + 1}/{self.max_epochs}, Average Training Loss: {avg_train_loss:.4f}')\n","\n","            if validloader is not None:\n","                # 如果有提供 validation DataLoader，則進行 validation 評估\n","                valid_rmsle = self.evaluate_rmsle(model, validloader)\n","                print(f'Validation RMSLE: {valid_rmsle:.4f}')\n","\n","        print(\"Training process has finished\")\n","\n","    def fit_epoch(self, trainloader):\n","        self.model.train()\n","        current_loss = 0.0\n","\n","        for i, data in enumerate(trainloader):\n","            inputs, target = data\n","            self.optimizer.zero_grad()\n","            outputs = self.model(inputs)\n","            loss = self.model.loss(outputs, target)\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            current_loss += loss.item()\n","\n","            if i % 500 == 499:\n","                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n","                current_loss = 0.0\n","\n","        # 保存本 epoch 的訓練損失\n","        self.train_losses.append(current_loss)\n","\n","    def evaluate_rmsle(self, model, dataloader):\n","        model.eval()\n","        total_loss = 0.0\n","        total_samples = 0\n","\n","        with torch.no_grad():\n","            for inputs, targets in dataloader:\n","                outputs = model(inputs)\n","                loss = model.loss(outputs, targets)\n","                total_loss += loss.item()\n","                total_samples += len(targets)\n","\n","        rmsle = torch.sqrt(torch.tensor(total_loss / total_samples))\n","        return rmsle.item()"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":1299,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","class HousePriceDataset(Dataset):\n","    def __init__(self, x_tensor, y_tensor):\n","        if(type(x_tensor) == np.ndarray):\n","            self.x = x_tensor\n","            self.y = y_tensor\n","        else:\n","            self.x = x_tensor.values\n","            self.y = y_tensor.values\n","        \n","    def __getitem__(self, index):\n","        return (self.x[index], self.y[index])\n","    def __len__(self):\n","        return len(self.x)"]},{"cell_type":"markdown","metadata":{"id":"6s0f8gDVvhPh"},"source":["### **3.2. k-Fold Cross-Validation**\n","\n","To get good performance, you usually need to select the network architecture. This includes the type of network, the number of layers, the number of hidden neurons in each layer, the learning rate, the batch size, etc. These are called **hyper paramters**.\n","\n","However, it is difficult to know which configuration of the hyperparameters above would achieve the best performance. Thus in practice, we need to try multiple configurations (i.e., choices of the hyperparameters), evaluate the performance of each configuration, and pick the one that gives the best performance.\n","\n","To assess the performance of a given configuratation, you need to take the training set and randomly divide it into two subsets:\n","- one subset will be called training set (usually 80% of the entire data set),\n","- the second one will be called validation set (usually 20% of the entire data set.\n","\n","Then, train your model on the training set and test it on the validation set. Repeat this process $k$ times and average the performance over all the run. This will then be the performance of this configuration. This process is called **$k-$fold cross validation**. $k$ is usually chosen to be $5$ or $10$.\n","\n","Run this procedure for each of the configurations you defined and pick the one that gives the best performance.\n","\n","To use it, first we need a function that returns the i-th fold of the data in a k-fold cross-validation procedure. It proceeds by slicing out the i-th segment as validation data and returning the rest as training data.\n","\n","Note - this is not the most efficient way of handling data and we would use something much smarter if the amount of data was considerably larger. But this would obscure the function of the code considerably and we thus omit it."]},{"cell_type":"code","execution_count":1300,"metadata":{"id":"-DMHupgSvhPi"},"outputs":[],"source":["def get_k_fold_data(k, i, X, y):\n","    assert k > 1\n","    fold_size = X.shape[0] // k\n","    # print(\"fold size \", fold_size)\n","    X_train, y_train = None, None\n","\n","    for j in range(k):\n","        # print(f'{j} of {k}')\n","        idx = slice(j * fold_size, (j + 1) * fold_size)\n","        # print(\"slice: \", idx)\n","        X_part, y_part = X[idx], y[idx]\n","        # print(X_part)\n","        # print(y_part)\n","        if j == i:\n","            X_valid, y_valid = X_part, y_part\n","        elif X_train is None:\n","            X_train, y_train = X_part, y_part\n","        else:\n","            X_train = pd.concat([pd.DataFrame(X_train), pd.DataFrame(X_part)], ignore_index=True)\n","            y_train = pd.concat([pd.DataFrame(y_train), pd.DataFrame(y_part)], ignore_index=True)\n","    return X_train, y_train, X_valid, y_valid"]},{"cell_type":"markdown","metadata":{"id":"5Ix6umPTvhPi"},"source":["The training and verification error averages are returned when we train $k$ times in the k-fold cross-validation."]},{"cell_type":"code","execution_count":1301,"metadata":{"id":"0JTOCGL0vhPi"},"outputs":[],"source":["def k_fold(k, X_train, y_train, num_epochs,\n","           learning_rate, weight_decay, batch_size):\n","\n","    train_sum, valid_sum = 0, 0\n","\n","    for i in range(k):\n","\n","        data = get_k_fold_data(k, i, X_train, y_train)\n","        print(data[0].shape, data[1].shape)\n","        net = MLP(inputSize=data[0].shape[1], outputSize=1, lr=learning_rate, weight_decay=weight_decay)\n","        trainer = Trainer(n_epochs=num_epochs)\n","        trainloader = torch.utils.data.DataLoader(HousePriceDataset(data[0], data[1]), batch_size=batch_size, shuffle=True)\n","        validloader = torch.utils.data.DataLoader(HousePriceDataset(data[2], data[3]), batch_size=batch_size, shuffle=False)\n","\n","        trainer.fit(net, trainloader, validloader)\n","\n","        # 在每個 fold 完成後，根據 trainer 中保存的 train_losses 計算 train 和 validation 的 RMSLE\n","        avg_train_rmsle = trainer.evaluate_rmsle(net, trainloader)\n","        avg_valid_rmsle = trainer.evaluate_rmsle(net, validloader)\n","\n","        train_sum += avg_train_rmsle\n","        valid_sum += avg_valid_rmsle\n","\n","        print('fold %d, train rmsle: %f, valid rmsle: %f' % (\n","            i, avg_train_rmsle, avg_valid_rmsle))\n","\n","    return train_sum / k, valid_sum / k"]},{"cell_type":"markdown","metadata":{"id":"KDsWEFXDvhPi"},"source":["Finding a good choice of the hyperparameters can take quite some time, depending on how many things one wants to optimize over. Within reason the $k$-fold crossvalidation approach is resilient against multiple testing. However, if we were to try out an unreasonably large number of options it might fail since we might just get lucky on the validation split with a particular set of hyperparameters."]},{"cell_type":"code","execution_count":1302,"metadata":{"id":"IcUVNvK4vhPj"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1044, 236) (1044, 1)\n","Epoch 1/300, Average Training Loss: 11.4874\n","Validation RMSLE: 0.2124\n","Epoch 2/300, Average Training Loss: 22.9272\n","Validation RMSLE: 0.2120\n","Epoch 3/300, Average Training Loss: 34.2927\n","Validation RMSLE: 0.2110\n","Epoch 4/300, Average Training Loss: 33.9843\n","Validation RMSLE: 0.2086\n","Epoch 5/300, Average Training Loss: 33.3431\n","Validation RMSLE: 0.2043\n","Epoch 6/300, Average Training Loss: 32.2130\n","Validation RMSLE: 0.1982\n","Epoch 7/300, Average Training Loss: 30.5628\n","Validation RMSLE: 0.1910\n","Epoch 8/300, Average Training Loss: 28.5544\n","Validation RMSLE: 0.1836\n","Epoch 9/300, Average Training Loss: 26.4037\n","Validation RMSLE: 0.1764\n","Epoch 10/300, Average Training Loss: 24.3165\n","Validation RMSLE: 0.1696\n","Epoch 11/300, Average Training Loss: 22.3711\n","Validation RMSLE: 0.1632\n","Epoch 12/300, Average Training Loss: 20.6046\n","Validation RMSLE: 0.1573\n","Epoch 13/300, Average Training Loss: 18.9971\n","Validation RMSLE: 0.1518\n","Epoch 14/300, Average Training Loss: 17.5873\n","Validation RMSLE: 0.1467\n","Epoch 15/300, Average Training Loss: 16.3069\n","Validation RMSLE: 0.1418\n","Epoch 16/300, Average Training Loss: 15.1436\n","Validation RMSLE: 0.1373\n","Epoch 17/300, Average Training Loss: 14.0694\n","Validation RMSLE: 0.1329\n","Epoch 18/300, Average Training Loss: 13.1065\n","Validation RMSLE: 0.1287\n","Epoch 19/300, Average Training Loss: 12.2268\n","Validation RMSLE: 0.1247\n","Epoch 20/300, Average Training Loss: 11.3998\n","Validation RMSLE: 0.1209\n","Epoch 21/300, Average Training Loss: 10.6135\n","Validation RMSLE: 0.1171\n","Epoch 22/300, Average Training Loss: 9.8965\n","Validation RMSLE: 0.1135\n","Epoch 23/300, Average Training Loss: 9.2139\n","Validation RMSLE: 0.1099\n","Epoch 24/300, Average Training Loss: 8.5701\n"]},{"name":"stdout","output_type":"stream","text":["Validation RMSLE: 0.1064\n","Epoch 25/300, Average Training Loss: 7.9657\n","Validation RMSLE: 0.1030\n","Epoch 26/300, Average Training Loss: 7.4158\n","Validation RMSLE: 0.0997\n","Epoch 27/300, Average Training Loss: 6.8846\n","Validation RMSLE: 0.0964\n","Epoch 28/300, Average Training Loss: 6.3728\n","Validation RMSLE: 0.0931\n","Epoch 29/300, Average Training Loss: 5.8864\n","Validation RMSLE: 0.0899\n","Epoch 30/300, Average Training Loss: 5.4467\n","Validation RMSLE: 0.0867\n","Epoch 31/300, Average Training Loss: 5.0313\n","Validation RMSLE: 0.0835\n","Epoch 32/300, Average Training Loss: 4.6228\n","Validation RMSLE: 0.0804\n","Epoch 33/300, Average Training Loss: 4.2371\n","Validation RMSLE: 0.0773\n","Epoch 34/300, Average Training Loss: 3.8632\n","Validation RMSLE: 0.0743\n","Epoch 35/300, Average Training Loss: 3.5223\n","Validation RMSLE: 0.0713\n","Epoch 36/300, Average Training Loss: 3.1975\n","Validation RMSLE: 0.0683\n","Epoch 37/300, Average Training Loss: 2.9162\n","Validation RMSLE: 0.0654\n","Epoch 38/300, Average Training Loss: 2.6423\n","Validation RMSLE: 0.0626\n","Epoch 39/300, Average Training Loss: 2.3999\n","Validation RMSLE: 0.0598\n","Epoch 40/300, Average Training Loss: 2.1974\n","Validation RMSLE: 0.0572\n","Epoch 41/300, Average Training Loss: 2.0087\n","Validation RMSLE: 0.0547\n","Epoch 42/300, Average Training Loss: 1.8669\n","Validation RMSLE: 0.0523\n","Epoch 43/300, Average Training Loss: 1.7278\n","Validation RMSLE: 0.0502\n","Epoch 44/300, Average Training Loss: 1.6576\n","Validation RMSLE: 0.0483\n","Epoch 45/300, Average Training Loss: 1.5759\n","Validation RMSLE: 0.0466\n","Epoch 46/300, Average Training Loss: 1.5131\n","Validation RMSLE: 0.0451\n","Epoch 47/300, Average Training Loss: 1.4465\n","Validation RMSLE: 0.0438\n","Epoch 48/300, Average Training Loss: 1.4140\n","Validation RMSLE: 0.0426\n","Epoch 49/300, Average Training Loss: 1.3740\n","Validation RMSLE: 0.0416\n","Epoch 50/300, Average Training Loss: 1.3351\n","Validation RMSLE: 0.0407\n","Epoch 51/300, Average Training Loss: 1.2827\n","Validation RMSLE: 0.0399\n","Epoch 52/300, Average Training Loss: 1.2414\n","Validation RMSLE: 0.0391\n","Epoch 53/300, Average Training Loss: 1.2121\n","Validation RMSLE: 0.0384\n","Epoch 54/300, Average Training Loss: 1.1782\n","Validation RMSLE: 0.0376\n","Epoch 55/300, Average Training Loss: 1.1375\n","Validation RMSLE: 0.0369\n","Epoch 56/300, Average Training Loss: 1.1075\n","Validation RMSLE: 0.0362\n","Epoch 57/300, Average Training Loss: 1.0737\n","Validation RMSLE: 0.0355\n","Epoch 58/300, Average Training Loss: 1.0596\n","Validation RMSLE: 0.0348\n","Epoch 59/300, Average Training Loss: 1.0326\n","Validation RMSLE: 0.0341\n","Epoch 60/300, Average Training Loss: 1.0259\n","Validation RMSLE: 0.0334\n","Epoch 61/300, Average Training Loss: 1.0137\n","Validation RMSLE: 0.0327\n","Epoch 62/300, Average Training Loss: 1.0048\n","Validation RMSLE: 0.0321\n","Epoch 63/300, Average Training Loss: 0.9844\n","Validation RMSLE: 0.0315\n","Epoch 64/300, Average Training Loss: 0.9577\n","Validation RMSLE: 0.0310\n","Epoch 65/300, Average Training Loss: 0.9394\n","Validation RMSLE: 0.0305\n","Epoch 66/300, Average Training Loss: 0.9391\n","Validation RMSLE: 0.0301\n","Epoch 67/300, Average Training Loss: 0.9447\n","Validation RMSLE: 0.0297\n","Epoch 68/300, Average Training Loss: 0.9444\n","Validation RMSLE: 0.0294\n","Epoch 69/300, Average Training Loss: 0.9454\n","Validation RMSLE: 0.0291\n","Epoch 70/300, Average Training Loss: 0.9453\n","Validation RMSLE: 0.0288\n","Epoch 71/300, Average Training Loss: 0.9388\n","Validation RMSLE: 0.0286\n","Epoch 72/300, Average Training Loss: 0.9203\n","Validation RMSLE: 0.0284\n","Epoch 73/300, Average Training Loss: 0.9142\n","Validation RMSLE: 0.0282\n","Epoch 74/300, Average Training Loss: 0.9155\n","Validation RMSLE: 0.0280\n","Epoch 75/300, Average Training Loss: 0.9140\n","Validation RMSLE: 0.0278\n","Epoch 76/300, Average Training Loss: 0.9117\n","Validation RMSLE: 0.0276\n","Epoch 77/300, Average Training Loss: 0.9044\n","Validation RMSLE: 0.0275\n","Epoch 78/300, Average Training Loss: 0.8949\n","Validation RMSLE: 0.0273\n","Epoch 79/300, Average Training Loss: 0.8966\n","Validation RMSLE: 0.0272\n","Epoch 80/300, Average Training Loss: 0.8935\n","Validation RMSLE: 0.0270\n","Epoch 81/300, Average Training Loss: 0.8948\n","Validation RMSLE: 0.0269\n","Epoch 82/300, Average Training Loss: 0.8864\n","Validation RMSLE: 0.0268\n","Epoch 83/300, Average Training Loss: 0.8726\n","Validation RMSLE: 0.0267\n","Epoch 84/300, Average Training Loss: 0.8838\n","Validation RMSLE: 0.0266\n","Epoch 85/300, Average Training Loss: 0.8654\n","Validation RMSLE: 0.0265\n","Epoch 86/300, Average Training Loss: 0.8765\n","Validation RMSLE: 0.0265\n","Epoch 87/300, Average Training Loss: 0.8618\n","Validation RMSLE: 0.0264\n","Epoch 88/300, Average Training Loss: 0.8623\n","Validation RMSLE: 0.0263\n","Epoch 89/300, Average Training Loss: 0.8540\n","Validation RMSLE: 0.0262\n","Epoch 90/300, Average Training Loss: 0.8502\n","Validation RMSLE: 0.0261\n","Epoch 91/300, Average Training Loss: 0.8630\n","Validation RMSLE: 0.0260\n","Epoch 92/300, Average Training Loss: 0.8639\n","Validation RMSLE: 0.0260\n","Epoch 93/300, Average Training Loss: 0.8653\n","Validation RMSLE: 0.0259\n","Epoch 94/300, Average Training Loss: 0.8623\n","Validation RMSLE: 0.0258\n","Epoch 95/300, Average Training Loss: 0.8659\n","Validation RMSLE: 0.0257\n","Epoch 96/300, Average Training Loss: 0.8640\n","Validation RMSLE: 0.0257\n","Epoch 97/300, Average Training Loss: 0.8494\n","Validation RMSLE: 0.0256\n","Epoch 98/300, Average Training Loss: 0.8418\n","Validation RMSLE: 0.0255\n","Epoch 99/300, Average Training Loss: 0.8433\n","Validation RMSLE: 0.0255\n","Epoch 100/300, Average Training Loss: 0.8490\n","Validation RMSLE: 0.0254\n","Epoch 101/300, Average Training Loss: 0.8526\n","Validation RMSLE: 0.0254\n","Epoch 102/300, Average Training Loss: 0.8429\n","Validation RMSLE: 0.0253\n","Epoch 103/300, Average Training Loss: 0.8390\n","Validation RMSLE: 0.0253\n","Epoch 104/300, Average Training Loss: 0.8386\n","Validation RMSLE: 0.0253\n","Epoch 105/300, Average Training Loss: 0.8379\n","Validation RMSLE: 0.0252\n","Epoch 106/300, Average Training Loss: 0.8372\n","Validation RMSLE: 0.0252\n","Epoch 107/300, Average Training Loss: 0.8277\n","Validation RMSLE: 0.0251\n","Epoch 108/300, Average Training Loss: 0.8285\n","Validation RMSLE: 0.0251\n","Epoch 109/300, Average Training Loss: 0.8393\n","Validation RMSLE: 0.0250\n","Epoch 110/300, Average Training Loss: 0.8425\n","Validation RMSLE: 0.0250\n","Epoch 111/300, Average Training Loss: 0.8475\n","Validation RMSLE: 0.0250\n","Epoch 112/300, Average Training Loss: 0.8257\n","Validation RMSLE: 0.0249\n","Epoch 113/300, Average Training Loss: 0.8225\n","Validation RMSLE: 0.0249\n","Epoch 114/300, Average Training Loss: 0.8106\n","Validation RMSLE: 0.0249\n","Epoch 115/300, Average Training Loss: 0.8231\n","Validation RMSLE: 0.0249\n","Epoch 116/300, Average Training Loss: 0.8224\n","Validation RMSLE: 0.0248\n","Epoch 117/300, Average Training Loss: 0.8255\n","Validation RMSLE: 0.0248\n","Epoch 118/300, Average Training Loss: 0.8177\n","Validation RMSLE: 0.0248\n","Epoch 119/300, Average Training Loss: 0.8173\n","Validation RMSLE: 0.0247\n","Epoch 120/300, Average Training Loss: 0.8233\n","Validation RMSLE: 0.0247\n","Epoch 121/300, Average Training Loss: 0.8347\n","Validation RMSLE: 0.0246\n","Epoch 122/300, Average Training Loss: 0.8299\n","Validation RMSLE: 0.0245\n","Epoch 123/300, Average Training Loss: 0.8234\n","Validation RMSLE: 0.0245\n","Epoch 124/300, Average Training Loss: 0.8124\n","Validation RMSLE: 0.0245\n","Epoch 125/300, Average Training Loss: 0.8174\n","Validation RMSLE: 0.0245\n","Epoch 126/300, Average Training Loss: 0.8252\n","Validation RMSLE: 0.0245\n","Epoch 127/300, Average Training Loss: 0.8331\n","Validation RMSLE: 0.0244\n","Epoch 128/300, Average Training Loss: 0.8374\n","Validation RMSLE: 0.0244\n","Epoch 129/300, Average Training Loss: 0.8450\n","Validation RMSLE: 0.0245\n","Epoch 130/300, Average Training Loss: 0.8477\n","Validation RMSLE: 0.0245\n","Epoch 131/300, Average Training Loss: 0.8361\n","Validation RMSLE: 0.0245\n","Epoch 132/300, Average Training Loss: 0.8282\n","Validation RMSLE: 0.0244\n","Epoch 133/300, Average Training Loss: 0.8201\n","Validation RMSLE: 0.0244\n","Epoch 134/300, Average Training Loss: 0.8247\n","Validation RMSLE: 0.0243\n","Epoch 135/300, Average Training Loss: 0.8227\n","Validation RMSLE: 0.0243\n","Epoch 136/300, Average Training Loss: 0.8302\n","Validation RMSLE: 0.0242\n","Epoch 137/300, Average Training Loss: 0.8307\n","Validation RMSLE: 0.0242\n","Epoch 138/300, Average Training Loss: 0.8174\n","Validation RMSLE: 0.0242\n","Epoch 139/300, Average Training Loss: 0.8063\n","Validation RMSLE: 0.0242\n","Epoch 140/300, Average Training Loss: 0.7974\n","Validation RMSLE: 0.0242\n","Epoch 141/300, Average Training Loss: 0.8078\n","Validation RMSLE: 0.0242\n","Epoch 142/300, Average Training Loss: 0.8119\n","Validation RMSLE: 0.0241\n","Epoch 143/300, Average Training Loss: 0.8117\n","Validation RMSLE: 0.0241\n","Epoch 144/300, Average Training Loss: 0.8059\n","Validation RMSLE: 0.0241\n","Epoch 145/300, Average Training Loss: 0.8145\n","Validation RMSLE: 0.0241\n","Epoch 146/300, Average Training Loss: 0.8143\n","Validation RMSLE: 0.0241\n","Epoch 147/300, Average Training Loss: 0.8278\n","Validation RMSLE: 0.0241\n","Epoch 148/300, Average Training Loss: 0.8149\n","Validation RMSLE: 0.0241\n","Epoch 149/300, Average Training Loss: 0.8174\n","Validation RMSLE: 0.0241\n","Epoch 150/300, Average Training Loss: 0.8036\n","Validation RMSLE: 0.0241\n","Epoch 151/300, Average Training Loss: 0.8033\n","Validation RMSLE: 0.0240\n","Epoch 152/300, Average Training Loss: 0.8109\n","Validation RMSLE: 0.0240\n","Epoch 153/300, Average Training Loss: 0.8086\n","Validation RMSLE: 0.0240\n","Epoch 154/300, Average Training Loss: 0.7954\n","Validation RMSLE: 0.0240\n","Epoch 155/300, Average Training Loss: 0.7802\n","Validation RMSLE: 0.0240\n","Epoch 156/300, Average Training Loss: 0.7959\n","Validation RMSLE: 0.0240\n","Epoch 157/300, Average Training Loss: 0.8213\n","Validation RMSLE: 0.0240\n","Epoch 158/300, Average Training Loss: 0.8379\n","Validation RMSLE: 0.0239\n","Epoch 159/300, Average Training Loss: 0.8262\n","Validation RMSLE: 0.0239\n","Epoch 160/300, Average Training Loss: 0.8351\n","Validation RMSLE: 0.0239\n","Epoch 161/300, Average Training Loss: 0.8306\n","Validation RMSLE: 0.0239\n","Epoch 162/300, Average Training Loss: 0.8327\n","Validation RMSLE: 0.0239\n","Epoch 163/300, Average Training Loss: 0.8052\n","Validation RMSLE: 0.0239\n","Epoch 164/300, Average Training Loss: 0.8034\n","Validation RMSLE: 0.0239\n","Epoch 165/300, Average Training Loss: 0.7951\n","Validation RMSLE: 0.0238\n","Epoch 166/300, Average Training Loss: 0.7979\n","Validation RMSLE: 0.0238\n","Epoch 167/300, Average Training Loss: 0.7929\n","Validation RMSLE: 0.0238\n","Epoch 168/300, Average Training Loss: 0.8043\n","Validation RMSLE: 0.0238\n","Epoch 169/300, Average Training Loss: 0.8086\n","Validation RMSLE: 0.0238\n","Epoch 170/300, Average Training Loss: 0.8152\n","Validation RMSLE: 0.0238\n","Epoch 171/300, Average Training Loss: 0.8070\n","Validation RMSLE: 0.0238\n","Epoch 172/300, Average Training Loss: 0.8005\n","Validation RMSLE: 0.0237\n","Epoch 173/300, Average Training Loss: 0.8025\n","Validation RMSLE: 0.0237\n","Epoch 174/300, Average Training Loss: 0.8105\n","Validation RMSLE: 0.0237\n","Epoch 175/300, Average Training Loss: 0.8122\n","Validation RMSLE: 0.0237\n","Epoch 176/300, Average Training Loss: 0.8065\n","Validation RMSLE: 0.0237\n","Epoch 177/300, Average Training Loss: 0.7976\n","Validation RMSLE: 0.0237\n","Epoch 178/300, Average Training Loss: 0.8072\n","Validation RMSLE: 0.0237\n","Epoch 179/300, Average Training Loss: 0.8160\n","Validation RMSLE: 0.0237\n","Epoch 180/300, Average Training Loss: 0.8258\n","Validation RMSLE: 0.0237\n","Epoch 181/300, Average Training Loss: 0.8155\n","Validation RMSLE: 0.0237\n","Epoch 182/300, Average Training Loss: 0.8083\n","Validation RMSLE: 0.0237\n","Epoch 183/300, Average Training Loss: 0.7976\n","Validation RMSLE: 0.0237\n","Epoch 184/300, Average Training Loss: 0.8025\n","Validation RMSLE: 0.0237\n","Epoch 185/300, Average Training Loss: 0.7976\n","Validation RMSLE: 0.0237\n","Epoch 186/300, Average Training Loss: 0.8085\n","Validation RMSLE: 0.0237\n","Epoch 187/300, Average Training Loss: 0.8026\n","Validation RMSLE: 0.0237\n","Epoch 188/300, Average Training Loss: 0.8041\n","Validation RMSLE: 0.0237\n","Epoch 189/300, Average Training Loss: 0.8026\n","Validation RMSLE: 0.0237\n","Epoch 190/300, Average Training Loss: 0.8019\n","Validation RMSLE: 0.0237\n","Epoch 191/300, Average Training Loss: 0.8186\n","Validation RMSLE: 0.0237\n","Epoch 192/300, Average Training Loss: 0.8144\n","Validation RMSLE: 0.0237\n","Epoch 193/300, Average Training Loss: 0.8175\n","Validation RMSLE: 0.0237\n","Epoch 194/300, Average Training Loss: 0.8046\n","Validation RMSLE: 0.0237\n","Epoch 195/300, Average Training Loss: 0.8041\n","Validation RMSLE: 0.0236\n","Epoch 196/300, Average Training Loss: 0.8120\n","Validation RMSLE: 0.0236\n","Epoch 197/300, Average Training Loss: 0.8055\n","Validation RMSLE: 0.0236\n","Epoch 198/300, Average Training Loss: 0.8124\n","Validation RMSLE: 0.0236\n","Epoch 199/300, Average Training Loss: 0.8130\n","Validation RMSLE: 0.0235\n","Epoch 200/300, Average Training Loss: 0.8232\n","Validation RMSLE: 0.0235\n","Epoch 201/300, Average Training Loss: 0.8178\n","Validation RMSLE: 0.0235\n","Epoch 202/300, Average Training Loss: 0.8025\n","Validation RMSLE: 0.0235\n","Epoch 203/300, Average Training Loss: 0.7916\n","Validation RMSLE: 0.0235\n","Epoch 204/300, Average Training Loss: 0.7860\n","Validation RMSLE: 0.0235\n","Epoch 205/300, Average Training Loss: 0.7977\n","Validation RMSLE: 0.0235\n","Epoch 206/300, Average Training Loss: 0.7967\n","Validation RMSLE: 0.0235\n","Epoch 207/300, Average Training Loss: 0.8072\n","Validation RMSLE: 0.0235\n","Epoch 208/300, Average Training Loss: 0.7952\n","Validation RMSLE: 0.0235\n","Epoch 209/300, Average Training Loss: 0.8025\n","Validation RMSLE: 0.0235\n","Epoch 210/300, Average Training Loss: 0.7909\n","Validation RMSLE: 0.0235\n","Epoch 211/300, Average Training Loss: 0.7875\n","Validation RMSLE: 0.0235\n","Epoch 212/300, Average Training Loss: 0.7830\n","Validation RMSLE: 0.0234\n","Epoch 213/300, Average Training Loss: 0.7864\n","Validation RMSLE: 0.0234\n","Epoch 214/300, Average Training Loss: 0.7950\n","Validation RMSLE: 0.0234\n","Epoch 215/300, Average Training Loss: 0.7861\n","Validation RMSLE: 0.0234\n","Epoch 216/300, Average Training Loss: 0.7940\n","Validation RMSLE: 0.0234\n","Epoch 217/300, Average Training Loss: 0.7939\n","Validation RMSLE: 0.0235\n","Epoch 218/300, Average Training Loss: 0.8029\n","Validation RMSLE: 0.0235\n","Epoch 219/300, Average Training Loss: 0.7970\n","Validation RMSLE: 0.0235\n","Epoch 220/300, Average Training Loss: 0.7981\n","Validation RMSLE: 0.0235\n","Epoch 221/300, Average Training Loss: 0.7935\n","Validation RMSLE: 0.0234\n","Epoch 222/300, Average Training Loss: 0.8039\n","Validation RMSLE: 0.0234\n","Epoch 223/300, Average Training Loss: 0.8068\n","Validation RMSLE: 0.0234\n","Epoch 224/300, Average Training Loss: 0.8111\n","Validation RMSLE: 0.0234\n","Epoch 225/300, Average Training Loss: 0.7970\n","Validation RMSLE: 0.0234\n","Epoch 226/300, Average Training Loss: 0.7982\n","Validation RMSLE: 0.0234\n","Epoch 227/300, Average Training Loss: 0.7923\n","Validation RMSLE: 0.0234\n","Epoch 228/300, Average Training Loss: 0.7981\n","Validation RMSLE: 0.0234\n","Epoch 229/300, Average Training Loss: 0.7980\n","Validation RMSLE: 0.0234\n","Epoch 230/300, Average Training Loss: 0.7998\n","Validation RMSLE: 0.0234\n","Epoch 231/300, Average Training Loss: 0.7905\n","Validation RMSLE: 0.0234\n","Epoch 232/300, Average Training Loss: 0.7946\n","Validation RMSLE: 0.0234\n","Epoch 233/300, Average Training Loss: 0.7987\n","Validation RMSLE: 0.0234\n","Epoch 234/300, Average Training Loss: 0.7993\n","Validation RMSLE: 0.0234\n","Epoch 235/300, Average Training Loss: 0.7928\n","Validation RMSLE: 0.0234\n","Epoch 236/300, Average Training Loss: 0.7881\n","Validation RMSLE: 0.0234\n","Epoch 237/300, Average Training Loss: 0.7878\n","Validation RMSLE: 0.0234\n","Epoch 238/300, Average Training Loss: 0.7901\n","Validation RMSLE: 0.0235\n","Epoch 239/300, Average Training Loss: 0.8060\n","Validation RMSLE: 0.0236\n","Epoch 240/300, Average Training Loss: 0.8052\n","Validation RMSLE: 0.0235\n","Epoch 241/300, Average Training Loss: 0.8055\n","Validation RMSLE: 0.0234\n","Epoch 242/300, Average Training Loss: 0.7940\n","Validation RMSLE: 0.0234\n","Epoch 243/300, Average Training Loss: 0.8063\n","Validation RMSLE: 0.0234\n","Epoch 244/300, Average Training Loss: 0.8011\n","Validation RMSLE: 0.0234\n","Epoch 245/300, Average Training Loss: 0.8124\n","Validation RMSLE: 0.0234\n","Epoch 246/300, Average Training Loss: 0.7997\n","Validation RMSLE: 0.0234\n","Epoch 247/300, Average Training Loss: 0.8002\n","Validation RMSLE: 0.0234\n","Epoch 248/300, Average Training Loss: 0.7894\n","Validation RMSLE: 0.0234\n","Epoch 249/300, Average Training Loss: 0.7837\n","Validation RMSLE: 0.0234\n","Epoch 250/300, Average Training Loss: 0.7780\n","Validation RMSLE: 0.0234\n","Epoch 251/300, Average Training Loss: 0.7708\n","Validation RMSLE: 0.0234\n","Epoch 252/300, Average Training Loss: 0.7687\n","Validation RMSLE: 0.0234\n","Epoch 253/300, Average Training Loss: 0.7645\n","Validation RMSLE: 0.0234\n","Epoch 254/300, Average Training Loss: 0.7750\n","Validation RMSLE: 0.0234\n","Epoch 255/300, Average Training Loss: 0.7909\n","Validation RMSLE: 0.0234\n","Epoch 256/300, Average Training Loss: 0.7965\n","Validation RMSLE: 0.0235\n","Epoch 257/300, Average Training Loss: 0.7829\n","Validation RMSLE: 0.0234\n","Epoch 258/300, Average Training Loss: 0.7780\n","Validation RMSLE: 0.0234\n","Epoch 259/300, Average Training Loss: 0.7862\n","Validation RMSLE: 0.0234\n","Epoch 260/300, Average Training Loss: 0.7985\n","Validation RMSLE: 0.0234\n","Epoch 261/300, Average Training Loss: 0.8116\n","Validation RMSLE: 0.0234\n","Epoch 262/300, Average Training Loss: 0.8077\n","Validation RMSLE: 0.0234\n","Epoch 263/300, Average Training Loss: 0.8013\n","Validation RMSLE: 0.0234\n","Epoch 264/300, Average Training Loss: 0.7775\n","Validation RMSLE: 0.0234\n","Epoch 265/300, Average Training Loss: 0.7779\n","Validation RMSLE: 0.0234\n","Epoch 266/300, Average Training Loss: 0.7864\n","Validation RMSLE: 0.0234\n","Epoch 267/300, Average Training Loss: 0.8001\n","Validation RMSLE: 0.0234\n","Epoch 268/300, Average Training Loss: 0.7961\n","Validation RMSLE: 0.0234\n","Epoch 269/300, Average Training Loss: 0.8014\n","Validation RMSLE: 0.0234\n","Epoch 270/300, Average Training Loss: 0.7978\n","Validation RMSLE: 0.0234\n","Epoch 271/300, Average Training Loss: 0.8065\n","Validation RMSLE: 0.0234\n","Epoch 272/300, Average Training Loss: 0.7845\n","Validation RMSLE: 0.0234\n","Epoch 273/300, Average Training Loss: 0.7926\n","Validation RMSLE: 0.0234\n","Epoch 274/300, Average Training Loss: 0.7876\n","Validation RMSLE: 0.0234\n","Epoch 275/300, Average Training Loss: 0.7968\n","Validation RMSLE: 0.0234\n","Epoch 276/300, Average Training Loss: 0.7804\n","Validation RMSLE: 0.0234\n","Epoch 277/300, Average Training Loss: 0.7852\n","Validation RMSLE: 0.0234\n","Epoch 278/300, Average Training Loss: 0.7854\n","Validation RMSLE: 0.0234\n","Epoch 279/300, Average Training Loss: 0.7904\n","Validation RMSLE: 0.0234\n","Epoch 280/300, Average Training Loss: 0.7879\n","Validation RMSLE: 0.0234\n","Epoch 281/300, Average Training Loss: 0.7884\n","Validation RMSLE: 0.0234\n","Epoch 282/300, Average Training Loss: 0.7874\n","Validation RMSLE: 0.0234\n","Epoch 283/300, Average Training Loss: 0.7756\n","Validation RMSLE: 0.0233\n","Epoch 284/300, Average Training Loss: 0.7795\n","Validation RMSLE: 0.0233\n","Epoch 285/300, Average Training Loss: 0.7833\n","Validation RMSLE: 0.0234\n","Epoch 286/300, Average Training Loss: 0.7887\n","Validation RMSLE: 0.0234\n","Epoch 287/300, Average Training Loss: 0.7892\n","Validation RMSLE: 0.0234\n","Epoch 288/300, Average Training Loss: 0.7753\n","Validation RMSLE: 0.0233\n","Epoch 289/300, Average Training Loss: 0.7728\n","Validation RMSLE: 0.0233\n","Epoch 290/300, Average Training Loss: 0.7775\n","Validation RMSLE: 0.0233\n","Epoch 291/300, Average Training Loss: 0.7885\n","Validation RMSLE: 0.0233\n","Epoch 292/300, Average Training Loss: 0.7962\n","Validation RMSLE: 0.0234\n","Epoch 293/300, Average Training Loss: 0.7853\n","Validation RMSLE: 0.0234\n","Epoch 294/300, Average Training Loss: 0.7952\n","Validation RMSLE: 0.0234\n","Epoch 295/300, Average Training Loss: 0.7851\n","Validation RMSLE: 0.0234\n","Epoch 296/300, Average Training Loss: 0.7905\n","Validation RMSLE: 0.0234\n","Epoch 297/300, Average Training Loss: 0.7755\n","Validation RMSLE: 0.0234\n","Epoch 298/300, Average Training Loss: 0.7794\n","Validation RMSLE: 0.0234\n","Epoch 299/300, Average Training Loss: 0.7775\n","Validation RMSLE: 0.0233\n","Epoch 300/300, Average Training Loss: 0.7846\n","Validation RMSLE: 0.0233\n","Training process has finished\n","fold 0, train rmsle: 0.020265, valid rmsle: 0.023326\n","(1044, 236) (1044, 1)\n","Epoch 1/300, Average Training Loss: 11.9571\n","Validation RMSLE: 0.2166\n","Epoch 2/300, Average Training Loss: 23.8720\n","Validation RMSLE: 0.2163\n","Epoch 3/300, Average Training Loss: 35.7420\n","Validation RMSLE: 0.2157\n","Epoch 4/300, Average Training Loss: 35.5596\n","Validation RMSLE: 0.2145\n","Epoch 5/300, Average Training Loss: 35.2394\n","Validation RMSLE: 0.2124\n","Epoch 6/300, Average Training Loss: 34.6477\n","Validation RMSLE: 0.2089\n","Epoch 7/300, Average Training Loss: 33.6758\n","Validation RMSLE: 0.2038\n","Epoch 8/300, Average Training Loss: 32.2876\n","Validation RMSLE: 0.1976\n","Epoch 9/300, Average Training Loss: 30.5532\n","Validation RMSLE: 0.1909\n","Epoch 10/300, Average Training Loss: 28.5931\n","Validation RMSLE: 0.1842\n","Epoch 11/300, Average Training Loss: 26.5954\n","Validation RMSLE: 0.1777\n","Epoch 12/300, Average Training Loss: 24.6563\n","Validation RMSLE: 0.1715\n","Epoch 13/300, Average Training Loss: 22.8712\n","Validation RMSLE: 0.1656\n","Epoch 14/300, Average Training Loss: 21.2255\n","Validation RMSLE: 0.1601\n","Epoch 15/300, Average Training Loss: 19.7225\n","Validation RMSLE: 0.1549\n","Epoch 16/300, Average Training Loss: 18.3774\n","Validation RMSLE: 0.1500\n","Epoch 17/300, Average Training Loss: 17.1462\n","Validation RMSLE: 0.1454\n","Epoch 18/300, Average Training Loss: 16.0226\n","Validation RMSLE: 0.1411\n","Epoch 19/300, Average Training Loss: 14.9903\n","Validation RMSLE: 0.1369\n","Epoch 20/300, Average Training Loss: 14.0497\n","Validation RMSLE: 0.1330\n","Epoch 21/300, Average Training Loss: 13.1708\n","Validation RMSLE: 0.1292\n","Epoch 22/300, Average Training Loss: 12.3412\n","Validation RMSLE: 0.1255\n","Epoch 23/300, Average Training Loss: 11.5577\n","Validation RMSLE: 0.1220\n","Epoch 24/300, Average Training Loss: 10.8531\n","Validation RMSLE: 0.1186\n","Epoch 25/300, Average Training Loss: 10.1882\n","Validation RMSLE: 0.1153\n","Epoch 26/300, Average Training Loss: 9.5624\n","Validation RMSLE: 0.1121\n","Epoch 27/300, Average Training Loss: 8.9633\n","Validation RMSLE: 0.1090\n","Epoch 28/300, Average Training Loss: 8.3930\n","Validation RMSLE: 0.1059\n","Epoch 29/300, Average Training Loss: 7.8660\n","Validation RMSLE: 0.1029\n","Epoch 30/300, Average Training Loss: 7.3715\n","Validation RMSLE: 0.0999\n","Epoch 31/300, Average Training Loss: 6.9129\n","Validation RMSLE: 0.0970\n","Epoch 32/300, Average Training Loss: 6.4606\n","Validation RMSLE: 0.0942\n","Epoch 33/300, Average Training Loss: 6.0368\n","Validation RMSLE: 0.0913\n","Epoch 34/300, Average Training Loss: 5.6118\n","Validation RMSLE: 0.0885\n","Epoch 35/300, Average Training Loss: 5.2286\n","Validation RMSLE: 0.0858\n","Epoch 36/300, Average Training Loss: 4.8658\n","Validation RMSLE: 0.0830\n","Epoch 37/300, Average Training Loss: 4.5275\n","Validation RMSLE: 0.0803\n","Epoch 38/300, Average Training Loss: 4.2042\n","Validation RMSLE: 0.0776\n","Epoch 39/300, Average Training Loss: 3.8747\n","Validation RMSLE: 0.0750\n","Epoch 40/300, Average Training Loss: 3.5832\n","Validation RMSLE: 0.0723\n","Epoch 41/300, Average Training Loss: 3.3060\n","Validation RMSLE: 0.0697\n","Epoch 42/300, Average Training Loss: 3.0539\n","Validation RMSLE: 0.0672\n","Epoch 43/300, Average Training Loss: 2.8328\n","Validation RMSLE: 0.0647\n","Epoch 44/300, Average Training Loss: 2.6012\n","Validation RMSLE: 0.0622\n","Epoch 45/300, Average Training Loss: 2.4144\n","Validation RMSLE: 0.0598\n","Epoch 46/300, Average Training Loss: 2.2123\n","Validation RMSLE: 0.0575\n","Epoch 47/300, Average Training Loss: 2.0556\n","Validation RMSLE: 0.0553\n","Epoch 48/300, Average Training Loss: 1.9175\n","Validation RMSLE: 0.0532\n","Epoch 49/300, Average Training Loss: 1.8223\n","Validation RMSLE: 0.0513\n","Epoch 50/300, Average Training Loss: 1.7675\n","Validation RMSLE: 0.0495\n","Epoch 51/300, Average Training Loss: 1.6991\n","Validation RMSLE: 0.0479\n","Epoch 52/300, Average Training Loss: 1.6347\n","Validation RMSLE: 0.0465\n","Epoch 53/300, Average Training Loss: 1.5652\n","Validation RMSLE: 0.0452\n","Epoch 54/300, Average Training Loss: 1.5333\n","Validation RMSLE: 0.0441\n","Epoch 55/300, Average Training Loss: 1.4967\n","Validation RMSLE: 0.0431\n","Epoch 56/300, Average Training Loss: 1.4467\n","Validation RMSLE: 0.0422\n","Epoch 57/300, Average Training Loss: 1.4068\n","Validation RMSLE: 0.0414\n","Epoch 58/300, Average Training Loss: 1.3786\n","Validation RMSLE: 0.0407\n","Epoch 59/300, Average Training Loss: 1.3551\n","Validation RMSLE: 0.0400\n","Epoch 60/300, Average Training Loss: 1.2975\n","Validation RMSLE: 0.0394\n","Epoch 61/300, Average Training Loss: 1.2758\n","Validation RMSLE: 0.0387\n","Epoch 62/300, Average Training Loss: 1.2516\n","Validation RMSLE: 0.0381\n","Epoch 63/300, Average Training Loss: 1.2469\n","Validation RMSLE: 0.0375\n","Epoch 64/300, Average Training Loss: 1.1990\n","Validation RMSLE: 0.0368\n","Epoch 65/300, Average Training Loss: 1.1716\n","Validation RMSLE: 0.0362\n","Epoch 66/300, Average Training Loss: 1.1638\n","Validation RMSLE: 0.0356\n","Epoch 67/300, Average Training Loss: 1.1573\n","Validation RMSLE: 0.0349\n","Epoch 68/300, Average Training Loss: 1.1347\n","Validation RMSLE: 0.0343\n","Epoch 69/300, Average Training Loss: 1.1073\n","Validation RMSLE: 0.0336\n","Epoch 70/300, Average Training Loss: 1.0838\n","Validation RMSLE: 0.0330\n","Epoch 71/300, Average Training Loss: 1.0709\n","Validation RMSLE: 0.0324\n","Epoch 72/300, Average Training Loss: 1.0486\n","Validation RMSLE: 0.0319\n","Epoch 73/300, Average Training Loss: 1.0458\n","Validation RMSLE: 0.0314\n","Epoch 74/300, Average Training Loss: 1.0481\n","Validation RMSLE: 0.0309\n","Epoch 75/300, Average Training Loss: 1.0257\n","Validation RMSLE: 0.0305\n","Epoch 76/300, Average Training Loss: 0.9870\n","Validation RMSLE: 0.0301\n","Epoch 77/300, Average Training Loss: 0.9762\n","Validation RMSLE: 0.0298\n","Epoch 78/300, Average Training Loss: 0.9843\n","Validation RMSLE: 0.0295\n","Epoch 79/300, Average Training Loss: 0.9997\n","Validation RMSLE: 0.0292\n","Epoch 80/300, Average Training Loss: 0.9885\n","Validation RMSLE: 0.0289\n","Epoch 81/300, Average Training Loss: 0.9785\n","Validation RMSLE: 0.0287\n","Epoch 82/300, Average Training Loss: 0.9712\n","Validation RMSLE: 0.0285\n","Epoch 83/300, Average Training Loss: 0.9683\n","Validation RMSLE: 0.0283\n","Epoch 84/300, Average Training Loss: 0.9559\n","Validation RMSLE: 0.0282\n","Epoch 85/300, Average Training Loss: 0.9517\n","Validation RMSLE: 0.0281\n","Epoch 86/300, Average Training Loss: 0.9483\n","Validation RMSLE: 0.0279\n","Epoch 87/300, Average Training Loss: 0.9570\n","Validation RMSLE: 0.0278\n","Epoch 88/300, Average Training Loss: 0.9591\n","Validation RMSLE: 0.0277\n","Epoch 89/300, Average Training Loss: 0.9454\n","Validation RMSLE: 0.0276\n","Epoch 90/300, Average Training Loss: 0.9449\n","Validation RMSLE: 0.0276\n","Epoch 91/300, Average Training Loss: 0.9488\n","Validation RMSLE: 0.0275\n","Epoch 92/300, Average Training Loss: 0.9507\n","Validation RMSLE: 0.0274\n","Epoch 93/300, Average Training Loss: 0.9439\n","Validation RMSLE: 0.0274\n","Epoch 94/300, Average Training Loss: 0.9221\n","Validation RMSLE: 0.0273\n","Epoch 95/300, Average Training Loss: 0.9309\n","Validation RMSLE: 0.0273\n","Epoch 96/300, Average Training Loss: 0.9291\n","Validation RMSLE: 0.0272\n","Epoch 97/300, Average Training Loss: 0.9285\n","Validation RMSLE: 0.0271\n","Epoch 98/300, Average Training Loss: 0.9111\n","Validation RMSLE: 0.0271\n","Epoch 99/300, Average Training Loss: 0.9096\n","Validation RMSLE: 0.0270\n","Epoch 100/300, Average Training Loss: 0.9350\n","Validation RMSLE: 0.0270\n","Epoch 101/300, Average Training Loss: 0.9463\n","Validation RMSLE: 0.0269\n","Epoch 102/300, Average Training Loss: 0.9566\n","Validation RMSLE: 0.0269\n","Epoch 103/300, Average Training Loss: 0.9418\n","Validation RMSLE: 0.0269\n","Epoch 104/300, Average Training Loss: 0.9324\n","Validation RMSLE: 0.0269\n","Epoch 105/300, Average Training Loss: 0.8968\n","Validation RMSLE: 0.0268\n","Epoch 106/300, Average Training Loss: 0.8992\n","Validation RMSLE: 0.0267\n","Epoch 107/300, Average Training Loss: 0.8989\n","Validation RMSLE: 0.0266\n","Epoch 108/300, Average Training Loss: 0.9345\n","Validation RMSLE: 0.0266\n","Epoch 109/300, Average Training Loss: 0.9130\n","Validation RMSLE: 0.0265\n","Epoch 110/300, Average Training Loss: 0.9172\n","Validation RMSLE: 0.0265\n","Epoch 111/300, Average Training Loss: 0.9042\n","Validation RMSLE: 0.0265\n","Epoch 112/300, Average Training Loss: 0.9090\n","Validation RMSLE: 0.0264\n","Epoch 113/300, Average Training Loss: 0.9148\n","Validation RMSLE: 0.0264\n","Epoch 114/300, Average Training Loss: 0.9007\n","Validation RMSLE: 0.0264\n","Epoch 115/300, Average Training Loss: 0.9194\n","Validation RMSLE: 0.0263\n","Epoch 116/300, Average Training Loss: 0.9085\n","Validation RMSLE: 0.0263\n","Epoch 117/300, Average Training Loss: 0.9213\n","Validation RMSLE: 0.0262\n","Epoch 118/300, Average Training Loss: 1.0455\n","Validation RMSLE: 0.0262\n","Epoch 119/300, Average Training Loss: 1.0370\n","Validation RMSLE: 0.0261\n","Epoch 120/300, Average Training Loss: 1.0575\n","Validation RMSLE: 0.0262\n","Epoch 121/300, Average Training Loss: 0.9148\n","Validation RMSLE: 0.0262\n","Epoch 122/300, Average Training Loss: 0.9230\n","Validation RMSLE: 0.0262\n","Epoch 123/300, Average Training Loss: 0.9039\n","Validation RMSLE: 0.0262\n","Epoch 124/300, Average Training Loss: 0.9114\n","Validation RMSLE: 0.0262\n","Epoch 125/300, Average Training Loss: 0.9168\n","Validation RMSLE: 0.0262\n","Epoch 126/300, Average Training Loss: 0.9048\n","Validation RMSLE: 0.0261\n","Epoch 127/300, Average Training Loss: 0.9081\n","Validation RMSLE: 0.0260\n","Epoch 128/300, Average Training Loss: 1.0434\n","Validation RMSLE: 0.0260\n","Epoch 129/300, Average Training Loss: 1.0437\n","Validation RMSLE: 0.0260\n","Epoch 130/300, Average Training Loss: 1.0351\n","Validation RMSLE: 0.0259\n","Epoch 131/300, Average Training Loss: 0.9026\n","Validation RMSLE: 0.0259\n","Epoch 132/300, Average Training Loss: 0.8934\n","Validation RMSLE: 0.0259\n","Epoch 133/300, Average Training Loss: 0.8905\n","Validation RMSLE: 0.0259\n","Epoch 134/300, Average Training Loss: 0.8835\n","Validation RMSLE: 0.0259\n","Epoch 135/300, Average Training Loss: 0.9107\n","Validation RMSLE: 0.0260\n","Epoch 136/300, Average Training Loss: 0.9065\n","Validation RMSLE: 0.0261\n","Epoch 137/300, Average Training Loss: 0.8961\n","Validation RMSLE: 0.0260\n","Epoch 138/300, Average Training Loss: 0.8743\n","Validation RMSLE: 0.0259\n","Epoch 139/300, Average Training Loss: 0.8666\n","Validation RMSLE: 0.0257\n","Epoch 140/300, Average Training Loss: 0.8648\n","Validation RMSLE: 0.0256\n","Epoch 141/300, Average Training Loss: 0.8761\n","Validation RMSLE: 0.0256\n","Epoch 142/300, Average Training Loss: 0.8860\n","Validation RMSLE: 0.0256\n","Epoch 143/300, Average Training Loss: 0.8950\n","Validation RMSLE: 0.0256\n","Epoch 144/300, Average Training Loss: 0.8906\n","Validation RMSLE: 0.0257\n","Epoch 145/300, Average Training Loss: 1.0389\n","Validation RMSLE: 0.0256\n","Epoch 146/300, Average Training Loss: 1.0309\n","Validation RMSLE: 0.0257\n","Epoch 147/300, Average Training Loss: 1.0266\n","Validation RMSLE: 0.0257\n","Epoch 148/300, Average Training Loss: 0.8730\n","Validation RMSLE: 0.0256\n","Epoch 149/300, Average Training Loss: 0.8834\n","Validation RMSLE: 0.0255\n","Epoch 150/300, Average Training Loss: 0.8798\n","Validation RMSLE: 0.0255\n","Epoch 151/300, Average Training Loss: 0.8839\n","Validation RMSLE: 0.0254\n","Epoch 152/300, Average Training Loss: 0.8597\n","Validation RMSLE: 0.0254\n","Epoch 153/300, Average Training Loss: 0.8599\n","Validation RMSLE: 0.0254\n","Epoch 154/300, Average Training Loss: 0.8541\n","Validation RMSLE: 0.0254\n","Epoch 155/300, Average Training Loss: 0.8674\n","Validation RMSLE: 0.0254\n","Epoch 156/300, Average Training Loss: 0.8672\n","Validation RMSLE: 0.0255\n","Epoch 157/300, Average Training Loss: 0.8649\n","Validation RMSLE: 0.0253\n","Epoch 158/300, Average Training Loss: 0.8734\n","Validation RMSLE: 0.0254\n","Epoch 159/300, Average Training Loss: 0.8752\n","Validation RMSLE: 0.0255\n","Epoch 160/300, Average Training Loss: 0.8796\n","Validation RMSLE: 0.0256\n","Epoch 161/300, Average Training Loss: 0.8685\n","Validation RMSLE: 0.0256\n","Epoch 162/300, Average Training Loss: 0.8640\n","Validation RMSLE: 0.0254\n","Epoch 163/300, Average Training Loss: 0.8566\n","Validation RMSLE: 0.0252\n","Epoch 164/300, Average Training Loss: 0.8639\n","Validation RMSLE: 0.0251\n","Epoch 165/300, Average Training Loss: 0.8628\n","Validation RMSLE: 0.0250\n","Epoch 166/300, Average Training Loss: 0.8598\n","Validation RMSLE: 0.0251\n","Epoch 167/300, Average Training Loss: 0.8430\n","Validation RMSLE: 0.0251\n","Epoch 168/300, Average Training Loss: 0.8586\n","Validation RMSLE: 0.0252\n","Epoch 169/300, Average Training Loss: 0.8712\n","Validation RMSLE: 0.0253\n","Epoch 170/300, Average Training Loss: 0.8858\n","Validation RMSLE: 0.0254\n","Epoch 171/300, Average Training Loss: 0.8668\n","Validation RMSLE: 0.0254\n","Epoch 172/300, Average Training Loss: 0.8529\n","Validation RMSLE: 0.0253\n","Epoch 173/300, Average Training Loss: 0.8504\n","Validation RMSLE: 0.0251\n","Epoch 174/300, Average Training Loss: 0.8419\n","Validation RMSLE: 0.0250\n","Epoch 175/300, Average Training Loss: 0.8432\n","Validation RMSLE: 0.0249\n","Epoch 176/300, Average Training Loss: 0.8379\n","Validation RMSLE: 0.0250\n","Epoch 177/300, Average Training Loss: 0.8446\n","Validation RMSLE: 0.0252\n","Epoch 178/300, Average Training Loss: 0.8577\n","Validation RMSLE: 0.0252\n","Epoch 179/300, Average Training Loss: 1.0199\n","Validation RMSLE: 0.0251\n","Epoch 180/300, Average Training Loss: 1.0175\n","Validation RMSLE: 0.0251\n","Epoch 181/300, Average Training Loss: 1.0058\n","Validation RMSLE: 0.0250\n","Epoch 182/300, Average Training Loss: 0.8440\n","Validation RMSLE: 0.0250\n","Epoch 183/300, Average Training Loss: 0.8517\n","Validation RMSLE: 0.0250\n","Epoch 184/300, Average Training Loss: 0.8526\n","Validation RMSLE: 0.0250\n","Epoch 185/300, Average Training Loss: 0.8553\n","Validation RMSLE: 0.0250\n","Epoch 186/300, Average Training Loss: 0.8687\n","Validation RMSLE: 0.0250\n","Epoch 187/300, Average Training Loss: 0.8651\n","Validation RMSLE: 0.0250\n","Epoch 188/300, Average Training Loss: 0.8736\n","Validation RMSLE: 0.0249\n","Epoch 189/300, Average Training Loss: 0.8497\n","Validation RMSLE: 0.0248\n","Epoch 190/300, Average Training Loss: 0.8524\n","Validation RMSLE: 0.0248\n","Epoch 191/300, Average Training Loss: 0.8296\n","Validation RMSLE: 0.0249\n","Epoch 192/300, Average Training Loss: 0.8276\n","Validation RMSLE: 0.0250\n","Epoch 193/300, Average Training Loss: 0.8312\n","Validation RMSLE: 0.0249\n","Epoch 194/300, Average Training Loss: 0.8538\n","Validation RMSLE: 0.0249\n","Epoch 195/300, Average Training Loss: 0.8623\n","Validation RMSLE: 0.0248\n","Epoch 196/300, Average Training Loss: 0.8600\n","Validation RMSLE: 0.0248\n","Epoch 197/300, Average Training Loss: 0.8585\n","Validation RMSLE: 0.0248\n","Epoch 198/300, Average Training Loss: 0.8566\n","Validation RMSLE: 0.0249\n","Epoch 199/300, Average Training Loss: 0.8513\n","Validation RMSLE: 0.0249\n","Epoch 200/300, Average Training Loss: 0.8368\n","Validation RMSLE: 0.0247\n","Epoch 201/300, Average Training Loss: 0.8388\n","Validation RMSLE: 0.0247\n","Epoch 202/300, Average Training Loss: 0.8438\n","Validation RMSLE: 0.0247\n","Epoch 203/300, Average Training Loss: 0.8409\n","Validation RMSLE: 0.0247\n","Epoch 204/300, Average Training Loss: 0.8279\n","Validation RMSLE: 0.0247\n","Epoch 205/300, Average Training Loss: 0.8392\n","Validation RMSLE: 0.0248\n","Epoch 206/300, Average Training Loss: 0.8413\n","Validation RMSLE: 0.0249\n","Epoch 207/300, Average Training Loss: 0.8600\n","Validation RMSLE: 0.0249\n","Epoch 208/300, Average Training Loss: 0.8371\n","Validation RMSLE: 0.0250\n","Epoch 209/300, Average Training Loss: 0.8414\n","Validation RMSLE: 0.0249\n","Epoch 210/300, Average Training Loss: 0.8455\n","Validation RMSLE: 0.0248\n","Epoch 211/300, Average Training Loss: 0.8517\n","Validation RMSLE: 0.0246\n","Epoch 212/300, Average Training Loss: 0.8575\n","Validation RMSLE: 0.0246\n","Epoch 213/300, Average Training Loss: 0.8464\n","Validation RMSLE: 0.0246\n","Epoch 214/300, Average Training Loss: 0.8587\n","Validation RMSLE: 0.0248\n","Epoch 215/300, Average Training Loss: 0.8522\n","Validation RMSLE: 0.0249\n","Epoch 216/300, Average Training Loss: 0.8433\n","Validation RMSLE: 0.0248\n","Epoch 217/300, Average Training Loss: 0.8331\n","Validation RMSLE: 0.0247\n","Epoch 218/300, Average Training Loss: 0.8215\n","Validation RMSLE: 0.0247\n","Epoch 219/300, Average Training Loss: 0.8248\n","Validation RMSLE: 0.0246\n","Epoch 220/300, Average Training Loss: 0.8348\n","Validation RMSLE: 0.0246\n","Epoch 221/300, Average Training Loss: 0.8288\n","Validation RMSLE: 0.0247\n","Epoch 222/300, Average Training Loss: 0.8401\n","Validation RMSLE: 0.0247\n","Epoch 223/300, Average Training Loss: 0.8391\n","Validation RMSLE: 0.0248\n","Epoch 224/300, Average Training Loss: 0.8624\n","Validation RMSLE: 0.0250\n","Epoch 225/300, Average Training Loss: 0.8717\n","Validation RMSLE: 0.0250\n","Epoch 226/300, Average Training Loss: 0.8601\n","Validation RMSLE: 0.0248\n","Epoch 227/300, Average Training Loss: 0.8528\n","Validation RMSLE: 0.0246\n","Epoch 228/300, Average Training Loss: 0.8344\n","Validation RMSLE: 0.0245\n","Epoch 229/300, Average Training Loss: 0.8444\n","Validation RMSLE: 0.0245\n","Epoch 230/300, Average Training Loss: 0.8420\n","Validation RMSLE: 0.0246\n","Epoch 231/300, Average Training Loss: 0.8422\n","Validation RMSLE: 0.0247\n","Epoch 232/300, Average Training Loss: 0.8288\n","Validation RMSLE: 0.0247\n","Epoch 233/300, Average Training Loss: 0.8242\n","Validation RMSLE: 0.0247\n","Epoch 234/300, Average Training Loss: 0.8287\n","Validation RMSLE: 0.0246\n","Epoch 235/300, Average Training Loss: 0.8405\n","Validation RMSLE: 0.0246\n","Epoch 236/300, Average Training Loss: 0.8503\n","Validation RMSLE: 0.0246\n","Epoch 237/300, Average Training Loss: 0.8452\n","Validation RMSLE: 0.0248\n","Epoch 238/300, Average Training Loss: 0.8465\n","Validation RMSLE: 0.0248\n","Epoch 239/300, Average Training Loss: 0.8446\n","Validation RMSLE: 0.0248\n","Epoch 240/300, Average Training Loss: 0.8453\n","Validation RMSLE: 0.0247\n","Epoch 241/300, Average Training Loss: 0.8507\n","Validation RMSLE: 0.0246\n","Epoch 242/300, Average Training Loss: 0.8618\n","Validation RMSLE: 0.0246\n","Epoch 243/300, Average Training Loss: 0.8612\n","Validation RMSLE: 0.0247\n","Epoch 244/300, Average Training Loss: 0.8547\n","Validation RMSLE: 0.0248\n","Epoch 245/300, Average Training Loss: 0.8446\n","Validation RMSLE: 0.0248\n","Epoch 246/300, Average Training Loss: 0.8395\n","Validation RMSLE: 0.0246\n","Epoch 247/300, Average Training Loss: 0.8285\n","Validation RMSLE: 0.0245\n","Epoch 248/300, Average Training Loss: 0.8237\n","Validation RMSLE: 0.0245\n","Epoch 249/300, Average Training Loss: 0.8327\n","Validation RMSLE: 0.0245\n","Epoch 250/300, Average Training Loss: 0.8444\n","Validation RMSLE: 0.0246\n","Epoch 251/300, Average Training Loss: 0.8587\n","Validation RMSLE: 0.0247\n","Epoch 252/300, Average Training Loss: 0.8628\n","Validation RMSLE: 0.0248\n","Epoch 253/300, Average Training Loss: 0.8564\n","Validation RMSLE: 0.0247\n","Epoch 254/300, Average Training Loss: 0.8631\n","Validation RMSLE: 0.0246\n","Epoch 255/300, Average Training Loss: 0.8573\n","Validation RMSLE: 0.0246\n","Epoch 256/300, Average Training Loss: 0.8593\n","Validation RMSLE: 0.0246\n","Epoch 257/300, Average Training Loss: 0.9906\n","Validation RMSLE: 0.0246\n","Epoch 258/300, Average Training Loss: 0.9912\n","Validation RMSLE: 0.0247\n","Epoch 259/300, Average Training Loss: 0.9973\n","Validation RMSLE: 0.0247\n","Epoch 260/300, Average Training Loss: 0.8444\n","Validation RMSLE: 0.0247\n","Epoch 261/300, Average Training Loss: 0.8449\n","Validation RMSLE: 0.0246\n","Epoch 262/300, Average Training Loss: 0.8387\n","Validation RMSLE: 0.0245\n","Epoch 263/300, Average Training Loss: 0.8378\n","Validation RMSLE: 0.0245\n","Epoch 264/300, Average Training Loss: 0.8430\n","Validation RMSLE: 0.0245\n","Epoch 265/300, Average Training Loss: 0.8375\n","Validation RMSLE: 0.0245\n","Epoch 266/300, Average Training Loss: 0.8267\n","Validation RMSLE: 0.0245\n","Epoch 267/300, Average Training Loss: 0.8139\n","Validation RMSLE: 0.0246\n","Epoch 268/300, Average Training Loss: 0.8139\n","Validation RMSLE: 0.0247\n","Epoch 269/300, Average Training Loss: 0.8147\n","Validation RMSLE: 0.0247\n","Epoch 270/300, Average Training Loss: 0.8136\n","Validation RMSLE: 0.0246\n","Epoch 271/300, Average Training Loss: 0.8201\n","Validation RMSLE: 0.0244\n","Epoch 272/300, Average Training Loss: 0.8366\n","Validation RMSLE: 0.0243\n","Epoch 273/300, Average Training Loss: 0.8320\n","Validation RMSLE: 0.0243\n","Epoch 274/300, Average Training Loss: 0.8343\n","Validation RMSLE: 0.0244\n","Epoch 275/300, Average Training Loss: 0.8465\n","Validation RMSLE: 0.0247\n","Epoch 276/300, Average Training Loss: 0.8537\n","Validation RMSLE: 0.0250\n","Epoch 277/300, Average Training Loss: 0.8605\n","Validation RMSLE: 0.0249\n","Epoch 278/300, Average Training Loss: 0.8480\n","Validation RMSLE: 0.0247\n","Epoch 279/300, Average Training Loss: 0.8567\n","Validation RMSLE: 0.0246\n","Epoch 280/300, Average Training Loss: 0.8500\n","Validation RMSLE: 0.0245\n","Epoch 281/300, Average Training Loss: 0.8484\n","Validation RMSLE: 0.0245\n","Epoch 282/300, Average Training Loss: 0.8353\n","Validation RMSLE: 0.0245\n","Epoch 283/300, Average Training Loss: 0.8380\n","Validation RMSLE: 0.0244\n","Epoch 284/300, Average Training Loss: 0.8265\n","Validation RMSLE: 0.0244\n","Epoch 285/300, Average Training Loss: 0.8448\n","Validation RMSLE: 0.0244\n","Epoch 286/300, Average Training Loss: 0.8374\n","Validation RMSLE: 0.0245\n","Epoch 287/300, Average Training Loss: 0.8481\n","Validation RMSLE: 0.0246\n","Epoch 288/300, Average Training Loss: 0.8406\n","Validation RMSLE: 0.0247\n","Epoch 289/300, Average Training Loss: 0.8485\n","Validation RMSLE: 0.0246\n","Epoch 290/300, Average Training Loss: 0.8600\n","Validation RMSLE: 0.0245\n","Epoch 291/300, Average Training Loss: 0.8655\n","Validation RMSLE: 0.0245\n","Epoch 292/300, Average Training Loss: 0.8692\n","Validation RMSLE: 0.0245\n","Epoch 293/300, Average Training Loss: 0.8731\n","Validation RMSLE: 0.0244\n","Epoch 294/300, Average Training Loss: 0.8810\n","Validation RMSLE: 0.0245\n","Epoch 295/300, Average Training Loss: 0.8672\n","Validation RMSLE: 0.0246\n","Epoch 296/300, Average Training Loss: 0.8471\n","Validation RMSLE: 0.0247\n","Epoch 297/300, Average Training Loss: 0.8293\n","Validation RMSLE: 0.0248\n","Epoch 298/300, Average Training Loss: 0.8348\n","Validation RMSLE: 0.0247\n","Epoch 299/300, Average Training Loss: 0.8355\n","Validation RMSLE: 0.0246\n","Epoch 300/300, Average Training Loss: 0.8440\n","Validation RMSLE: 0.0246\n","Training process has finished\n","fold 1, train rmsle: 0.020881, valid rmsle: 0.024597\n","(1044, 236) (1044, 1)\n","Epoch 1/300, Average Training Loss: 11.4444\n","Validation RMSLE: 0.2097\n","Epoch 2/300, Average Training Loss: 22.8579\n","Validation RMSLE: 0.2093\n","Epoch 3/300, Average Training Loss: 34.2253\n","Validation RMSLE: 0.2087\n","Epoch 4/300, Average Training Loss: 34.0760\n","Validation RMSLE: 0.2077\n","Epoch 5/300, Average Training Loss: 33.8137\n","Validation RMSLE: 0.2055\n","Epoch 6/300, Average Training Loss: 33.3271\n","Validation RMSLE: 0.2018\n","Epoch 7/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 8/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 9/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 10/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 11/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 12/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 13/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 14/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 15/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 16/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 17/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 18/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 19/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 20/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 21/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 22/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 23/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 24/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 25/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 26/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 27/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 28/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 29/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 30/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 31/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 32/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 33/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 34/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 35/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 36/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 37/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 38/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 39/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 40/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 41/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 42/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 43/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 44/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 45/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 46/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 47/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 48/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 49/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 50/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 51/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 52/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 53/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 54/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 55/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 56/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 57/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 58/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 59/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 60/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 61/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 62/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 63/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 64/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 65/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 66/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 67/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 68/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 69/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 70/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 71/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 72/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 73/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 74/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 75/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 76/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 77/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 78/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 79/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 80/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 81/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 82/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 83/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 84/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 85/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 86/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 87/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 88/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 89/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 90/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 91/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 92/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 93/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 94/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 95/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 96/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 97/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 98/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 99/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 100/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 101/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 102/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 103/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 104/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 105/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 106/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 107/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 108/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 109/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 110/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 111/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 112/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 113/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 114/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 115/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 116/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 117/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 118/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 119/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 120/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 121/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 122/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 123/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 124/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 125/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 126/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 127/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 128/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 129/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 130/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 131/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 132/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 133/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 134/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 135/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 136/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 137/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 138/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 139/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 140/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 141/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 142/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 143/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 144/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 145/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 146/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 147/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 148/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 149/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 150/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 151/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 152/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 153/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 154/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 155/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 156/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 157/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 158/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 159/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 160/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 161/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 162/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 163/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 164/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 165/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 166/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 167/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 168/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 169/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 170/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 171/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 172/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 173/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 174/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 175/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 176/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 177/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 178/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 179/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 180/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 181/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 182/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 183/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 184/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 185/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 186/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 187/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 188/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 189/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 190/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 191/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 192/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 193/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 194/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 195/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 196/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 197/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 198/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 199/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 200/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 201/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 202/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 203/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 204/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 205/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 206/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 207/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 208/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 209/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 210/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 211/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 212/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 213/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 214/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 215/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 216/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 217/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 218/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 219/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 220/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 221/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 222/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 223/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 224/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 225/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 226/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 227/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 228/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 229/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 230/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 231/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 232/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 233/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 234/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 235/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 236/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 237/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 238/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 239/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 240/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 241/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 242/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 243/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 244/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 245/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 246/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 247/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 248/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 249/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 250/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 251/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 252/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 253/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 254/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 255/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 256/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 257/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 258/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 259/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 260/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 261/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 262/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 263/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 264/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 265/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 266/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 267/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 268/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 269/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 270/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 271/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 272/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 273/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 274/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 275/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 276/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 277/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 278/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 279/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 280/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 281/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 282/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 283/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 284/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 285/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 286/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 287/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 288/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 289/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 290/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 291/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 292/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 293/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 294/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 295/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 296/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 297/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 298/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 299/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Epoch 300/300, Average Training Loss: nan\n","Validation RMSLE: nan\n","Training process has finished\n","fold 2, train rmsle: nan, valid rmsle: nan\n","(1044, 236) (1044, 1)\n","Epoch 1/300, Average Training Loss: 11.5554\n","Validation RMSLE: 0.2064\n","Epoch 2/300, Average Training Loss: 23.0776\n","Validation RMSLE: 0.2061\n","Epoch 3/300, Average Training Loss: 34.5558\n","Validation RMSLE: 0.2053\n","Epoch 4/300, Average Training Loss: 34.3696\n","Validation RMSLE: 0.2032\n","Epoch 5/300, Average Training Loss: 33.9830\n","Validation RMSLE: 0.1997\n","Epoch 6/300, Average Training Loss: 33.2628\n","Validation RMSLE: 0.1948\n","Epoch 7/300, Average Training Loss: 32.1360\n","Validation RMSLE: 0.1890\n","Epoch 8/300, Average Training Loss: 30.6796\n","Validation RMSLE: 0.1827\n","Epoch 9/300, Average Training Loss: 29.0110\n","Validation RMSLE: 0.1763\n","Epoch 10/300, Average Training Loss: 27.2041\n","Validation RMSLE: 0.1696\n","Epoch 11/300, Average Training Loss: 25.4110\n","Validation RMSLE: 0.1630\n","Epoch 12/300, Average Training Loss: 23.6208\n","Validation RMSLE: 0.1568\n","Epoch 13/300, Average Training Loss: 22.0030\n","Validation RMSLE: 0.1510\n","Epoch 14/300, Average Training Loss: 20.4662\n","Validation RMSLE: 0.1455\n","Epoch 15/300, Average Training Loss: 19.0968\n","Validation RMSLE: 0.1402\n","Epoch 16/300, Average Training Loss: 17.8010\n","Validation RMSLE: 0.1351\n","Epoch 17/300, Average Training Loss: 16.6429\n","Validation RMSLE: 0.1302\n","Epoch 18/300, Average Training Loss: 15.5758\n","Validation RMSLE: 0.1255\n","Epoch 19/300, Average Training Loss: 14.6053\n","Validation RMSLE: 0.1210\n","Epoch 20/300, Average Training Loss: 13.6777\n","Validation RMSLE: 0.1165\n","Epoch 21/300, Average Training Loss: 12.8096\n","Validation RMSLE: 0.1122\n","Epoch 22/300, Average Training Loss: 11.9989\n","Validation RMSLE: 0.1079\n","Epoch 23/300, Average Training Loss: 11.2217\n","Validation RMSLE: 0.1036\n","Epoch 24/300, Average Training Loss: 10.5094\n","Validation RMSLE: 0.0995\n","Epoch 25/300, Average Training Loss: 9.8097\n","Validation RMSLE: 0.0954\n","Epoch 26/300, Average Training Loss: 9.1542\n","Validation RMSLE: 0.0913\n","Epoch 27/300, Average Training Loss: 8.5089\n","Validation RMSLE: 0.0872\n","Epoch 28/300, Average Training Loss: 7.9332\n","Validation RMSLE: 0.0831\n","Epoch 29/300, Average Training Loss: 7.3707\n","Validation RMSLE: 0.0790\n","Epoch 30/300, Average Training Loss: 6.8507\n","Validation RMSLE: 0.0748\n","Epoch 31/300, Average Training Loss: 6.3295\n","Validation RMSLE: 0.0706\n","Epoch 32/300, Average Training Loss: 5.8337\n","Validation RMSLE: 0.0663\n","Epoch 33/300, Average Training Loss: 5.3374\n","Validation RMSLE: 0.0620\n","Epoch 34/300, Average Training Loss: 4.8758\n","Validation RMSLE: 0.0576\n","Epoch 35/300, Average Training Loss: 4.4578\n","Validation RMSLE: 0.0530\n","Epoch 36/300, Average Training Loss: 4.0570\n","Validation RMSLE: 0.0483\n","Epoch 37/300, Average Training Loss: 3.6765\n","Validation RMSLE: 0.0433\n","Epoch 38/300, Average Training Loss: 3.2992\n","Validation RMSLE: 0.0382\n","Epoch 39/300, Average Training Loss: 2.9692\n","Validation RMSLE: 0.0328\n","Epoch 40/300, Average Training Loss: 2.6605\n","Validation RMSLE: 0.0271\n","Epoch 41/300, Average Training Loss: 2.3951\n","Validation RMSLE: 0.0219\n","Epoch 42/300, Average Training Loss: 2.1349\n","Validation RMSLE: 0.0195\n","Epoch 43/300, Average Training Loss: 1.9166\n","Validation RMSLE: 0.0218\n","Epoch 44/300, Average Training Loss: 1.7153\n","Validation RMSLE: 0.0253\n","Epoch 45/300, Average Training Loss: 1.5681\n","Validation RMSLE: 0.0282\n","Epoch 46/300, Average Training Loss: 1.4354\n","Validation RMSLE: 0.0303\n","Epoch 47/300, Average Training Loss: 1.3361\n","Validation RMSLE: 0.0316\n","Epoch 48/300, Average Training Loss: 1.2652\n","Validation RMSLE: 0.0322\n","Epoch 49/300, Average Training Loss: 1.2094\n","Validation RMSLE: 0.0320\n","Epoch 50/300, Average Training Loss: 1.1826\n","Validation RMSLE: 0.0312\n","Epoch 51/300, Average Training Loss: 1.1410\n","Validation RMSLE: 0.0300\n","Epoch 52/300, Average Training Loss: 1.1037\n","Validation RMSLE: 0.0285\n","Epoch 53/300, Average Training Loss: 1.0749\n","Validation RMSLE: 0.0273\n","Epoch 54/300, Average Training Loss: 1.0542\n","Validation RMSLE: 0.0265\n","Epoch 55/300, Average Training Loss: 1.0387\n","Validation RMSLE: 0.0263\n","Epoch 56/300, Average Training Loss: 1.0044\n","Validation RMSLE: 0.0264\n","Epoch 57/300, Average Training Loss: 1.1029\n","Validation RMSLE: 0.0268\n","Epoch 58/300, Average Training Loss: 1.0821\n","Validation RMSLE: 0.0272\n","Epoch 59/300, Average Training Loss: 1.0821\n","Validation RMSLE: 0.0274\n","Epoch 60/300, Average Training Loss: 0.9385\n","Validation RMSLE: 0.0274\n","Epoch 61/300, Average Training Loss: 0.9351\n","Validation RMSLE: 0.0273\n","Epoch 62/300, Average Training Loss: 0.9281\n","Validation RMSLE: 0.0271\n","Epoch 63/300, Average Training Loss: 0.9263\n","Validation RMSLE: 0.0269\n","Epoch 64/300, Average Training Loss: 0.9346\n","Validation RMSLE: 0.0268\n","Epoch 65/300, Average Training Loss: 0.9353\n","Validation RMSLE: 0.0267\n","Epoch 66/300, Average Training Loss: 0.9422\n","Validation RMSLE: 0.0266\n","Epoch 67/300, Average Training Loss: 0.9315\n","Validation RMSLE: 0.0265\n","Epoch 68/300, Average Training Loss: 0.9268\n","Validation RMSLE: 0.0264\n","Epoch 69/300, Average Training Loss: 0.9265\n","Validation RMSLE: 0.0262\n","Epoch 70/300, Average Training Loss: 0.9282\n","Validation RMSLE: 0.0261\n","Epoch 71/300, Average Training Loss: 0.9250\n","Validation RMSLE: 0.0259\n","Epoch 72/300, Average Training Loss: 0.9158\n","Validation RMSLE: 0.0257\n","Epoch 73/300, Average Training Loss: 0.9085\n","Validation RMSLE: 0.0256\n","Epoch 74/300, Average Training Loss: 0.9087\n","Validation RMSLE: 0.0255\n","Epoch 75/300, Average Training Loss: 0.9045\n","Validation RMSLE: 0.0255\n","Epoch 76/300, Average Training Loss: 0.9057\n","Validation RMSLE: 0.0255\n","Epoch 77/300, Average Training Loss: 0.9061\n","Validation RMSLE: 0.0255\n","Epoch 78/300, Average Training Loss: 0.9076\n","Validation RMSLE: 0.0254\n","Epoch 79/300, Average Training Loss: 0.8908\n","Validation RMSLE: 0.0255\n","Epoch 80/300, Average Training Loss: 0.8757\n","Validation RMSLE: 0.0254\n","Epoch 81/300, Average Training Loss: 0.8743\n","Validation RMSLE: 0.0254\n","Epoch 82/300, Average Training Loss: 0.8853\n","Validation RMSLE: 0.0254\n","Epoch 83/300, Average Training Loss: 0.8928\n","Validation RMSLE: 0.0253\n","Epoch 84/300, Average Training Loss: 0.8929\n","Validation RMSLE: 0.0253\n","Epoch 85/300, Average Training Loss: 0.8963\n","Validation RMSLE: 0.0252\n","Epoch 86/300, Average Training Loss: 0.8900\n","Validation RMSLE: 0.0251\n","Epoch 87/300, Average Training Loss: 0.8821\n","Validation RMSLE: 0.0250\n","Epoch 88/300, Average Training Loss: 0.8790\n","Validation RMSLE: 0.0249\n","Epoch 89/300, Average Training Loss: 0.8775\n","Validation RMSLE: 0.0249\n","Epoch 90/300, Average Training Loss: 0.8888\n","Validation RMSLE: 0.0249\n","Epoch 91/300, Average Training Loss: 0.8835\n","Validation RMSLE: 0.0248\n","Epoch 92/300, Average Training Loss: 0.8788\n","Validation RMSLE: 0.0248\n","Epoch 93/300, Average Training Loss: 0.8800\n","Validation RMSLE: 0.0248\n","Epoch 94/300, Average Training Loss: 0.8727\n","Validation RMSLE: 0.0248\n","Epoch 95/300, Average Training Loss: 0.8676\n","Validation RMSLE: 0.0248\n","Epoch 96/300, Average Training Loss: 0.8583\n","Validation RMSLE: 0.0248\n","Epoch 97/300, Average Training Loss: 0.8761\n","Validation RMSLE: 0.0248\n","Epoch 98/300, Average Training Loss: 0.8765\n","Validation RMSLE: 0.0249\n","Epoch 99/300, Average Training Loss: 0.8732\n","Validation RMSLE: 0.0249\n","Epoch 100/300, Average Training Loss: 0.8654\n","Validation RMSLE: 0.0248\n","Epoch 101/300, Average Training Loss: 0.8713\n","Validation RMSLE: 0.0248\n","Epoch 102/300, Average Training Loss: 0.8671\n","Validation RMSLE: 0.0248\n","Epoch 103/300, Average Training Loss: 0.8600\n","Validation RMSLE: 0.0248\n","Epoch 104/300, Average Training Loss: 0.8643\n","Validation RMSLE: 0.0247\n","Epoch 105/300, Average Training Loss: 0.8648\n","Validation RMSLE: 0.0247\n","Epoch 106/300, Average Training Loss: 0.8573\n","Validation RMSLE: 0.0247\n","Epoch 107/300, Average Training Loss: 0.8470\n","Validation RMSLE: 0.0247\n","Epoch 108/300, Average Training Loss: 0.8591\n","Validation RMSLE: 0.0247\n","Epoch 109/300, Average Training Loss: 0.8585\n","Validation RMSLE: 0.0247\n","Epoch 110/300, Average Training Loss: 0.8651\n","Validation RMSLE: 0.0247\n","Epoch 111/300, Average Training Loss: 0.8534\n","Validation RMSLE: 0.0247\n","Epoch 112/300, Average Training Loss: 0.8515\n","Validation RMSLE: 0.0247\n","Epoch 113/300, Average Training Loss: 0.8506\n","Validation RMSLE: 0.0247\n","Epoch 114/300, Average Training Loss: 0.8483\n","Validation RMSLE: 0.0247\n","Epoch 115/300, Average Training Loss: 0.8515\n","Validation RMSLE: 0.0247\n","Epoch 116/300, Average Training Loss: 0.8412\n","Validation RMSLE: 0.0246\n","Epoch 117/300, Average Training Loss: 0.8554\n","Validation RMSLE: 0.0246\n","Epoch 118/300, Average Training Loss: 0.8666\n","Validation RMSLE: 0.0246\n","Epoch 119/300, Average Training Loss: 0.8745\n","Validation RMSLE: 0.0246\n","Epoch 120/300, Average Training Loss: 0.8748\n","Validation RMSLE: 0.0245\n","Epoch 121/300, Average Training Loss: 0.8639\n","Validation RMSLE: 0.0245\n","Epoch 122/300, Average Training Loss: 0.8624\n","Validation RMSLE: 0.0245\n","Epoch 123/300, Average Training Loss: 0.8423\n","Validation RMSLE: 0.0244\n","Epoch 124/300, Average Training Loss: 0.8513\n","Validation RMSLE: 0.0244\n","Epoch 125/300, Average Training Loss: 0.8678\n","Validation RMSLE: 0.0243\n","Epoch 126/300, Average Training Loss: 0.8627\n","Validation RMSLE: 0.0243\n","Epoch 127/300, Average Training Loss: 0.8639\n","Validation RMSLE: 0.0243\n","Epoch 128/300, Average Training Loss: 0.8438\n","Validation RMSLE: 0.0243\n","Epoch 129/300, Average Training Loss: 0.8568\n","Validation RMSLE: 0.0243\n","Epoch 130/300, Average Training Loss: 0.8518\n","Validation RMSLE: 0.0243\n","Epoch 131/300, Average Training Loss: 0.8550\n","Validation RMSLE: 0.0243\n","Epoch 132/300, Average Training Loss: 0.8504\n","Validation RMSLE: 0.0243\n","Epoch 133/300, Average Training Loss: 0.8362\n","Validation RMSLE: 0.0243\n","Epoch 134/300, Average Training Loss: 0.8379\n","Validation RMSLE: 0.0243\n","Epoch 135/300, Average Training Loss: 0.8386\n","Validation RMSLE: 0.0242\n","Epoch 136/300, Average Training Loss: 0.8537\n","Validation RMSLE: 0.0242\n","Epoch 137/300, Average Training Loss: 0.8420\n","Validation RMSLE: 0.0242\n","Epoch 138/300, Average Training Loss: 0.8370\n","Validation RMSLE: 0.0243\n","Epoch 139/300, Average Training Loss: 0.8431\n","Validation RMSLE: 0.0243\n","Epoch 140/300, Average Training Loss: 0.8471\n","Validation RMSLE: 0.0243\n","Epoch 141/300, Average Training Loss: 0.8466\n","Validation RMSLE: 0.0243\n","Epoch 142/300, Average Training Loss: 0.8301\n","Validation RMSLE: 0.0243\n","Epoch 143/300, Average Training Loss: 0.8497\n","Validation RMSLE: 0.0244\n","Epoch 144/300, Average Training Loss: 0.8721\n","Validation RMSLE: 0.0243\n","Epoch 145/300, Average Training Loss: 0.8784\n","Validation RMSLE: 0.0243\n","Epoch 146/300, Average Training Loss: 0.8538\n","Validation RMSLE: 0.0243\n","Epoch 147/300, Average Training Loss: 0.8470\n","Validation RMSLE: 0.0243\n","Epoch 148/300, Average Training Loss: 0.8457\n","Validation RMSLE: 0.0243\n","Epoch 149/300, Average Training Loss: 0.8476\n","Validation RMSLE: 0.0242\n","Epoch 150/300, Average Training Loss: 0.8392\n","Validation RMSLE: 0.0242\n","Epoch 151/300, Average Training Loss: 0.8469\n","Validation RMSLE: 0.0242\n","Epoch 152/300, Average Training Loss: 0.8425\n","Validation RMSLE: 0.0242\n","Epoch 153/300, Average Training Loss: 0.8438\n","Validation RMSLE: 0.0242\n","Epoch 154/300, Average Training Loss: 0.8414\n","Validation RMSLE: 0.0241\n","Epoch 155/300, Average Training Loss: 0.8518\n","Validation RMSLE: 0.0241\n","Epoch 156/300, Average Training Loss: 0.8370\n","Validation RMSLE: 0.0241\n","Epoch 157/300, Average Training Loss: 0.8350\n","Validation RMSLE: 0.0241\n","Epoch 158/300, Average Training Loss: 0.8390\n","Validation RMSLE: 0.0241\n","Epoch 159/300, Average Training Loss: 0.8655\n","Validation RMSLE: 0.0240\n","Epoch 160/300, Average Training Loss: 0.8551\n","Validation RMSLE: 0.0240\n","Epoch 161/300, Average Training Loss: 0.8611\n","Validation RMSLE: 0.0240\n","Epoch 162/300, Average Training Loss: 0.8385\n","Validation RMSLE: 0.0240\n","Epoch 163/300, Average Training Loss: 0.8545\n","Validation RMSLE: 0.0240\n","Epoch 164/300, Average Training Loss: 0.8510\n","Validation RMSLE: 0.0239\n","Epoch 165/300, Average Training Loss: 0.8693\n","Validation RMSLE: 0.0239\n","Epoch 166/300, Average Training Loss: 0.8650\n","Validation RMSLE: 0.0239\n","Epoch 167/300, Average Training Loss: 0.8577\n","Validation RMSLE: 0.0238\n","Epoch 168/300, Average Training Loss: 0.8553\n","Validation RMSLE: 0.0238\n","Epoch 169/300, Average Training Loss: 0.8537\n","Validation RMSLE: 0.0238\n","Epoch 170/300, Average Training Loss: 0.8490\n","Validation RMSLE: 0.0239\n","Epoch 171/300, Average Training Loss: 0.8400\n","Validation RMSLE: 0.0239\n","Epoch 172/300, Average Training Loss: 0.8361\n","Validation RMSLE: 0.0240\n","Epoch 173/300, Average Training Loss: 0.8357\n","Validation RMSLE: 0.0240\n","Epoch 174/300, Average Training Loss: 0.8258\n","Validation RMSLE: 0.0241\n","Epoch 175/300, Average Training Loss: 0.8238\n","Validation RMSLE: 0.0241\n","Epoch 176/300, Average Training Loss: 0.8362\n","Validation RMSLE: 0.0241\n","Epoch 177/300, Average Training Loss: 0.8564\n","Validation RMSLE: 0.0241\n","Epoch 178/300, Average Training Loss: 0.8618\n","Validation RMSLE: 0.0241\n","Epoch 179/300, Average Training Loss: 0.8548\n","Validation RMSLE: 0.0240\n","Epoch 180/300, Average Training Loss: 0.8368\n","Validation RMSLE: 0.0240\n","Epoch 181/300, Average Training Loss: 0.8308\n","Validation RMSLE: 0.0240\n","Epoch 182/300, Average Training Loss: 0.8278\n","Validation RMSLE: 0.0241\n","Epoch 183/300, Average Training Loss: 0.8417\n","Validation RMSLE: 0.0241\n","Epoch 184/300, Average Training Loss: 0.8370\n","Validation RMSLE: 0.0241\n","Epoch 185/300, Average Training Loss: 0.8284\n","Validation RMSLE: 0.0240\n","Epoch 186/300, Average Training Loss: 0.8264\n","Validation RMSLE: 0.0240\n","Epoch 187/300, Average Training Loss: 0.8210\n","Validation RMSLE: 0.0239\n","Epoch 188/300, Average Training Loss: 0.8299\n","Validation RMSLE: 0.0239\n","Epoch 189/300, Average Training Loss: 0.8235\n","Validation RMSLE: 0.0239\n","Epoch 190/300, Average Training Loss: 0.8420\n","Validation RMSLE: 0.0239\n","Epoch 191/300, Average Training Loss: 0.8472\n","Validation RMSLE: 0.0238\n","Epoch 192/300, Average Training Loss: 0.8439\n","Validation RMSLE: 0.0237\n","Epoch 193/300, Average Training Loss: 0.8281\n","Validation RMSLE: 0.0237\n","Epoch 194/300, Average Training Loss: 0.8202\n","Validation RMSLE: 0.0237\n","Epoch 195/300, Average Training Loss: 0.8104\n","Validation RMSLE: 0.0238\n","Epoch 196/300, Average Training Loss: 0.8116\n","Validation RMSLE: 0.0238\n","Epoch 197/300, Average Training Loss: 0.8096\n","Validation RMSLE: 0.0239\n","Epoch 198/300, Average Training Loss: 0.8296\n","Validation RMSLE: 0.0239\n","Epoch 199/300, Average Training Loss: 0.8286\n","Validation RMSLE: 0.0239\n","Epoch 200/300, Average Training Loss: 0.8274\n","Validation RMSLE: 0.0239\n","Epoch 201/300, Average Training Loss: 0.8195\n","Validation RMSLE: 0.0238\n","Epoch 202/300, Average Training Loss: 0.8340\n","Validation RMSLE: 0.0238\n","Epoch 203/300, Average Training Loss: 0.8480\n","Validation RMSLE: 0.0237\n","Epoch 204/300, Average Training Loss: 0.8500\n","Validation RMSLE: 0.0237\n","Epoch 205/300, Average Training Loss: 0.8383\n","Validation RMSLE: 0.0237\n","Epoch 206/300, Average Training Loss: 0.8332\n","Validation RMSLE: 0.0237\n","Epoch 207/300, Average Training Loss: 0.8339\n","Validation RMSLE: 0.0238\n","Epoch 208/300, Average Training Loss: 0.8344\n","Validation RMSLE: 0.0238\n","Epoch 209/300, Average Training Loss: 0.8242\n","Validation RMSLE: 0.0238\n","Epoch 210/300, Average Training Loss: 0.8370\n","Validation RMSLE: 0.0237\n","Epoch 211/300, Average Training Loss: 0.8402\n","Validation RMSLE: 0.0237\n","Epoch 212/300, Average Training Loss: 0.8411\n","Validation RMSLE: 0.0237\n","Epoch 213/300, Average Training Loss: 0.8198\n","Validation RMSLE: 0.0237\n","Epoch 214/300, Average Training Loss: 0.8185\n","Validation RMSLE: 0.0237\n","Epoch 215/300, Average Training Loss: 0.8165\n","Validation RMSLE: 0.0238\n","Epoch 216/300, Average Training Loss: 0.8297\n","Validation RMSLE: 0.0238\n","Epoch 217/300, Average Training Loss: 0.8432\n","Validation RMSLE: 0.0238\n","Epoch 218/300, Average Training Loss: 0.8558\n","Validation RMSLE: 0.0239\n","Epoch 219/300, Average Training Loss: 0.8483\n","Validation RMSLE: 0.0238\n","Epoch 220/300, Average Training Loss: 0.8374\n","Validation RMSLE: 0.0237\n","Epoch 221/300, Average Training Loss: 0.8382\n","Validation RMSLE: 0.0237\n","Epoch 222/300, Average Training Loss: 0.8290\n","Validation RMSLE: 0.0237\n","Epoch 223/300, Average Training Loss: 0.8332\n","Validation RMSLE: 0.0236\n","Epoch 224/300, Average Training Loss: 0.8252\n","Validation RMSLE: 0.0236\n","Epoch 225/300, Average Training Loss: 0.8349\n","Validation RMSLE: 0.0236\n","Epoch 226/300, Average Training Loss: 0.8380\n","Validation RMSLE: 0.0236\n","Epoch 227/300, Average Training Loss: 0.8369\n","Validation RMSLE: 0.0237\n","Epoch 228/300, Average Training Loss: 0.8399\n","Validation RMSLE: 0.0237\n","Epoch 229/300, Average Training Loss: 0.8209\n","Validation RMSLE: 0.0237\n","Epoch 230/300, Average Training Loss: 0.8219\n","Validation RMSLE: 0.0237\n","Epoch 231/300, Average Training Loss: 0.8178\n","Validation RMSLE: 0.0236\n","Epoch 232/300, Average Training Loss: 0.8313\n","Validation RMSLE: 0.0236\n","Epoch 233/300, Average Training Loss: 0.8326\n","Validation RMSLE: 0.0237\n","Epoch 234/300, Average Training Loss: 0.8293\n","Validation RMSLE: 0.0237\n","Epoch 235/300, Average Training Loss: 0.8329\n","Validation RMSLE: 0.0238\n","Epoch 236/300, Average Training Loss: 0.8231\n","Validation RMSLE: 0.0239\n","Epoch 237/300, Average Training Loss: 0.8221\n","Validation RMSLE: 0.0239\n","Epoch 238/300, Average Training Loss: 0.8044\n","Validation RMSLE: 0.0239\n","Epoch 239/300, Average Training Loss: 0.8140\n","Validation RMSLE: 0.0239\n","Epoch 240/300, Average Training Loss: 0.8167\n","Validation RMSLE: 0.0239\n","Epoch 241/300, Average Training Loss: 0.8245\n","Validation RMSLE: 0.0239\n","Epoch 242/300, Average Training Loss: 0.8153\n","Validation RMSLE: 0.0238\n","Epoch 243/300, Average Training Loss: 0.8131\n","Validation RMSLE: 0.0238\n","Epoch 244/300, Average Training Loss: 0.8050\n","Validation RMSLE: 0.0238\n","Epoch 245/300, Average Training Loss: 0.8234\n","Validation RMSLE: 0.0238\n","Epoch 246/300, Average Training Loss: 0.8184\n","Validation RMSLE: 0.0238\n","Epoch 247/300, Average Training Loss: 0.8275\n","Validation RMSLE: 0.0238\n","Epoch 248/300, Average Training Loss: 0.8094\n","Validation RMSLE: 0.0238\n","Epoch 249/300, Average Training Loss: 0.8210\n","Validation RMSLE: 0.0238\n","Epoch 250/300, Average Training Loss: 0.8237\n","Validation RMSLE: 0.0238\n","Epoch 251/300, Average Training Loss: 0.8412\n","Validation RMSLE: 0.0238\n","Epoch 252/300, Average Training Loss: 0.8422\n","Validation RMSLE: 0.0238\n","Epoch 253/300, Average Training Loss: 0.8389\n","Validation RMSLE: 0.0238\n","Epoch 254/300, Average Training Loss: 0.8452\n","Validation RMSLE: 0.0237\n","Epoch 255/300, Average Training Loss: 0.8414\n","Validation RMSLE: 0.0237\n","Epoch 256/300, Average Training Loss: 0.8479\n","Validation RMSLE: 0.0236\n","Epoch 257/300, Average Training Loss: 0.8294\n","Validation RMSLE: 0.0236\n","Epoch 258/300, Average Training Loss: 0.8095\n","Validation RMSLE: 0.0236\n","Epoch 259/300, Average Training Loss: 0.8002\n","Validation RMSLE: 0.0237\n","Epoch 260/300, Average Training Loss: 0.8054\n","Validation RMSLE: 0.0238\n","Epoch 261/300, Average Training Loss: 0.8208\n","Validation RMSLE: 0.0239\n","Epoch 262/300, Average Training Loss: 0.8258\n","Validation RMSLE: 0.0239\n","Epoch 263/300, Average Training Loss: 0.8349\n","Validation RMSLE: 0.0239\n","Epoch 264/300, Average Training Loss: 0.8385\n","Validation RMSLE: 0.0238\n","Epoch 265/300, Average Training Loss: 0.8474\n","Validation RMSLE: 0.0238\n","Epoch 266/300, Average Training Loss: 0.8376\n","Validation RMSLE: 0.0237\n","Epoch 267/300, Average Training Loss: 0.8313\n","Validation RMSLE: 0.0237\n","Epoch 268/300, Average Training Loss: 0.8209\n","Validation RMSLE: 0.0236\n","Epoch 269/300, Average Training Loss: 0.8195\n","Validation RMSLE: 0.0236\n","Epoch 270/300, Average Training Loss: 0.8446\n","Validation RMSLE: 0.0236\n","Epoch 271/300, Average Training Loss: 0.8406\n","Validation RMSLE: 0.0236\n","Epoch 272/300, Average Training Loss: 0.8412\n","Validation RMSLE: 0.0236\n","Epoch 273/300, Average Training Loss: 0.8270\n","Validation RMSLE: 0.0236\n","Epoch 274/300, Average Training Loss: 0.8358\n","Validation RMSLE: 0.0236\n","Epoch 275/300, Average Training Loss: 0.8285\n","Validation RMSLE: 0.0236\n","Epoch 276/300, Average Training Loss: 0.8233\n","Validation RMSLE: 0.0236\n","Epoch 277/300, Average Training Loss: 0.8182\n","Validation RMSLE: 0.0236\n","Epoch 278/300, Average Training Loss: 0.8313\n","Validation RMSLE: 0.0236\n","Epoch 279/300, Average Training Loss: 0.8337\n","Validation RMSLE: 0.0236\n","Epoch 280/300, Average Training Loss: 0.8221\n","Validation RMSLE: 0.0236\n","Epoch 281/300, Average Training Loss: 0.8119\n","Validation RMSLE: 0.0236\n","Epoch 282/300, Average Training Loss: 0.8133\n","Validation RMSLE: 0.0236\n","Epoch 283/300, Average Training Loss: 0.8277\n","Validation RMSLE: 0.0236\n","Epoch 284/300, Average Training Loss: 0.8266\n","Validation RMSLE: 0.0236\n","Epoch 285/300, Average Training Loss: 0.8214\n","Validation RMSLE: 0.0236\n","Epoch 286/300, Average Training Loss: 0.8093\n","Validation RMSLE: 0.0236\n","Epoch 287/300, Average Training Loss: 0.8180\n","Validation RMSLE: 0.0236\n","Epoch 288/300, Average Training Loss: 0.8317\n","Validation RMSLE: 0.0236\n","Epoch 289/300, Average Training Loss: 0.8452\n","Validation RMSLE: 0.0235\n","Epoch 290/300, Average Training Loss: 0.8480\n","Validation RMSLE: 0.0235\n","Epoch 291/300, Average Training Loss: 0.8330\n","Validation RMSLE: 0.0235\n","Epoch 292/300, Average Training Loss: 0.8211\n","Validation RMSLE: 0.0235\n","Epoch 293/300, Average Training Loss: 0.8174\n","Validation RMSLE: 0.0235\n","Epoch 294/300, Average Training Loss: 0.8202\n","Validation RMSLE: 0.0235\n","Epoch 295/300, Average Training Loss: 0.8409\n","Validation RMSLE: 0.0235\n","Epoch 296/300, Average Training Loss: 0.8258\n","Validation RMSLE: 0.0235\n","Epoch 297/300, Average Training Loss: 0.8296\n","Validation RMSLE: 0.0236\n","Epoch 298/300, Average Training Loss: 0.8148\n","Validation RMSLE: 0.0236\n","Epoch 299/300, Average Training Loss: 0.8247\n","Validation RMSLE: 0.0236\n","Epoch 300/300, Average Training Loss: 0.8173\n","Validation RMSLE: 0.0236\n","Training process has finished\n","fold 3, train rmsle: 0.020383, valid rmsle: 0.023636\n","(1044, 236) (1044, 1)\n","Epoch 1/300, Average Training Loss: 11.7515\n","Validation RMSLE: 0.2091\n","Epoch 2/300, Average Training Loss: 23.4369\n","Validation RMSLE: 0.2085\n","Epoch 3/300, Average Training Loss: 35.0623\n","Validation RMSLE: 0.2079\n","Epoch 4/300, Average Training Loss: 34.8397\n","Validation RMSLE: 0.2063\n","Epoch 5/300, Average Training Loss: 34.4631\n","Validation RMSLE: 0.2026\n","Epoch 6/300, Average Training Loss: 33.7450\n","Validation RMSLE: 0.1971\n","Epoch 7/300, Average Training Loss: 32.5535\n","Validation RMSLE: 0.1904\n","Epoch 8/300, Average Training Loss: 30.9091\n","Validation RMSLE: 0.1832\n","Epoch 9/300, Average Training Loss: 29.0022\n","Validation RMSLE: 0.1761\n","Epoch 10/300, Average Training Loss: 26.9849\n","Validation RMSLE: 0.1695\n","Epoch 11/300, Average Training Loss: 25.0719\n","Validation RMSLE: 0.1632\n","Epoch 12/300, Average Training Loss: 23.2935\n","Validation RMSLE: 0.1573\n","Epoch 13/300, Average Training Loss: 21.7095\n","Validation RMSLE: 0.1518\n","Epoch 14/300, Average Training Loss: 20.2301\n","Validation RMSLE: 0.1465\n","Epoch 15/300, Average Training Loss: 18.9080\n","Validation RMSLE: 0.1416\n","Epoch 16/300, Average Training Loss: 17.7091\n","Validation RMSLE: 0.1369\n","Epoch 17/300, Average Training Loss: 16.6346\n","Validation RMSLE: 0.1324\n","Epoch 18/300, Average Training Loss: 15.6044\n","Validation RMSLE: 0.1281\n","Epoch 19/300, Average Training Loss: 14.6679\n","Validation RMSLE: 0.1239\n","Epoch 20/300, Average Training Loss: 13.7857\n","Validation RMSLE: 0.1200\n","Epoch 21/300, Average Training Loss: 12.9962\n","Validation RMSLE: 0.1161\n","Epoch 22/300, Average Training Loss: 12.2400\n","Validation RMSLE: 0.1124\n","Epoch 23/300, Average Training Loss: 11.5619\n","Validation RMSLE: 0.1088\n","Epoch 24/300, Average Training Loss: 10.9207\n","Validation RMSLE: 0.1052\n","Epoch 25/300, Average Training Loss: 10.3054\n","Validation RMSLE: 0.1017\n","Epoch 26/300, Average Training Loss: 9.7053\n","Validation RMSLE: 0.0982\n","Epoch 27/300, Average Training Loss: 9.1168\n","Validation RMSLE: 0.0948\n","Epoch 28/300, Average Training Loss: 8.5880\n","Validation RMSLE: 0.0913\n","Epoch 29/300, Average Training Loss: 8.0699\n","Validation RMSLE: 0.0879\n","Epoch 30/300, Average Training Loss: 7.5660\n","Validation RMSLE: 0.0844\n","Epoch 31/300, Average Training Loss: 7.0433\n","Validation RMSLE: 0.0809\n","Epoch 32/300, Average Training Loss: 6.5924\n","Validation RMSLE: 0.0775\n","Epoch 33/300, Average Training Loss: 6.1699\n","Validation RMSLE: 0.0739\n","Epoch 34/300, Average Training Loss: 5.7744\n","Validation RMSLE: 0.0704\n","Epoch 35/300, Average Training Loss: 5.3444\n","Validation RMSLE: 0.0668\n","Epoch 36/300, Average Training Loss: 4.9275\n","Validation RMSLE: 0.0631\n","Epoch 37/300, Average Training Loss: 4.5466\n","Validation RMSLE: 0.0594\n","Epoch 38/300, Average Training Loss: 4.2105\n","Validation RMSLE: 0.0557\n","Epoch 39/300, Average Training Loss: 3.8701\n","Validation RMSLE: 0.0519\n","Epoch 40/300, Average Training Loss: 3.5489\n","Validation RMSLE: 0.0480\n","Epoch 41/300, Average Training Loss: 3.2121\n","Validation RMSLE: 0.0441\n","Epoch 42/300, Average Training Loss: 2.9211\n","Validation RMSLE: 0.0401\n","Epoch 43/300, Average Training Loss: 2.6334\n","Validation RMSLE: 0.0362\n","Epoch 44/300, Average Training Loss: 2.3784\n","Validation RMSLE: 0.0325\n","Epoch 45/300, Average Training Loss: 2.1504\n","Validation RMSLE: 0.0293\n","Epoch 46/300, Average Training Loss: 1.9622\n","Validation RMSLE: 0.0269\n","Epoch 47/300, Average Training Loss: 1.7784\n","Validation RMSLE: 0.0258\n","Epoch 48/300, Average Training Loss: 1.6185\n","Validation RMSLE: 0.0259\n","Epoch 49/300, Average Training Loss: 1.4808\n","Validation RMSLE: 0.0266\n","Epoch 50/300, Average Training Loss: 1.3849\n","Validation RMSLE: 0.0273\n","Epoch 51/300, Average Training Loss: 1.3133\n","Validation RMSLE: 0.0278\n","Epoch 52/300, Average Training Loss: 1.2382\n","Validation RMSLE: 0.0280\n","Epoch 53/300, Average Training Loss: 1.2006\n","Validation RMSLE: 0.0279\n","Epoch 54/300, Average Training Loss: 1.1552\n","Validation RMSLE: 0.0275\n","Epoch 55/300, Average Training Loss: 1.1294\n","Validation RMSLE: 0.0271\n","Epoch 56/300, Average Training Loss: 1.0871\n","Validation RMSLE: 0.0268\n","Epoch 57/300, Average Training Loss: 1.0642\n","Validation RMSLE: 0.0268\n","Epoch 58/300, Average Training Loss: 1.0262\n","Validation RMSLE: 0.0269\n","Epoch 59/300, Average Training Loss: 0.9992\n","Validation RMSLE: 0.0272\n","Epoch 60/300, Average Training Loss: 0.9752\n","Validation RMSLE: 0.0277\n","Epoch 61/300, Average Training Loss: 0.9632\n","Validation RMSLE: 0.0282\n","Epoch 62/300, Average Training Loss: 0.9555\n","Validation RMSLE: 0.0286\n","Epoch 63/300, Average Training Loss: 0.9352\n","Validation RMSLE: 0.0289\n","Epoch 64/300, Average Training Loss: 0.9263\n","Validation RMSLE: 0.0289\n","Epoch 65/300, Average Training Loss: 0.9153\n","Validation RMSLE: 0.0287\n","Epoch 66/300, Average Training Loss: 0.9241\n","Validation RMSLE: 0.0283\n","Epoch 67/300, Average Training Loss: 0.9251\n","Validation RMSLE: 0.0279\n","Epoch 68/300, Average Training Loss: 0.9246\n","Validation RMSLE: 0.0275\n","Epoch 69/300, Average Training Loss: 0.9158\n","Validation RMSLE: 0.0272\n","Epoch 70/300, Average Training Loss: 0.9082\n","Validation RMSLE: 0.0270\n","Epoch 71/300, Average Training Loss: 0.9007\n","Validation RMSLE: 0.0269\n","Epoch 72/300, Average Training Loss: 0.8877\n","Validation RMSLE: 0.0269\n","Epoch 73/300, Average Training Loss: 0.8799\n","Validation RMSLE: 0.0269\n","Epoch 74/300, Average Training Loss: 0.8878\n","Validation RMSLE: 0.0269\n","Epoch 75/300, Average Training Loss: 0.8850\n","Validation RMSLE: 0.0270\n","Epoch 76/300, Average Training Loss: 0.8888\n","Validation RMSLE: 0.0270\n","Epoch 77/300, Average Training Loss: 0.8866\n","Validation RMSLE: 0.0269\n","Epoch 78/300, Average Training Loss: 0.8884\n","Validation RMSLE: 0.0268\n","Epoch 79/300, Average Training Loss: 0.8852\n","Validation RMSLE: 0.0268\n","Epoch 80/300, Average Training Loss: 0.8742\n","Validation RMSLE: 0.0268\n","Epoch 81/300, Average Training Loss: 0.8739\n","Validation RMSLE: 0.0268\n","Epoch 82/300, Average Training Loss: 0.8699\n","Validation RMSLE: 0.0265\n","Epoch 83/300, Average Training Loss: 0.8810\n","Validation RMSLE: 0.0264\n","Epoch 84/300, Average Training Loss: 0.8798\n","Validation RMSLE: 0.0263\n","Epoch 85/300, Average Training Loss: 0.8768\n","Validation RMSLE: 0.0263\n","Epoch 86/300, Average Training Loss: 0.8585\n","Validation RMSLE: 0.0262\n","Epoch 87/300, Average Training Loss: 0.8528\n","Validation RMSLE: 0.0262\n","Epoch 88/300, Average Training Loss: 0.8528\n","Validation RMSLE: 0.0262\n","Epoch 89/300, Average Training Loss: 0.8564\n","Validation RMSLE: 0.0262\n","Epoch 90/300, Average Training Loss: 0.8655\n","Validation RMSLE: 0.0261\n","Epoch 91/300, Average Training Loss: 0.8776\n","Validation RMSLE: 0.0261\n","Epoch 92/300, Average Training Loss: 0.8754\n","Validation RMSLE: 0.0262\n","Epoch 93/300, Average Training Loss: 0.8682\n","Validation RMSLE: 0.0262\n","Epoch 94/300, Average Training Loss: 0.8480\n","Validation RMSLE: 0.0262\n","Epoch 95/300, Average Training Loss: 0.8422\n","Validation RMSLE: 0.0260\n","Epoch 96/300, Average Training Loss: 0.8468\n","Validation RMSLE: 0.0258\n","Epoch 97/300, Average Training Loss: 0.8500\n","Validation RMSLE: 0.0256\n","Epoch 98/300, Average Training Loss: 0.8414\n","Validation RMSLE: 0.0254\n","Epoch 99/300, Average Training Loss: 0.8315\n","Validation RMSLE: 0.0254\n","Epoch 100/300, Average Training Loss: 0.8278\n","Validation RMSLE: 0.0256\n","Epoch 101/300, Average Training Loss: 0.8462\n","Validation RMSLE: 0.0258\n","Epoch 102/300, Average Training Loss: 0.8489\n","Validation RMSLE: 0.0258\n","Epoch 103/300, Average Training Loss: 0.8521\n","Validation RMSLE: 0.0257\n","Epoch 104/300, Average Training Loss: 0.8494\n","Validation RMSLE: 0.0254\n","Epoch 105/300, Average Training Loss: 0.8469\n","Validation RMSLE: 0.0252\n","Epoch 106/300, Average Training Loss: 0.8412\n","Validation RMSLE: 0.0252\n","Epoch 107/300, Average Training Loss: 0.8236\n","Validation RMSLE: 0.0253\n","Epoch 108/300, Average Training Loss: 0.8187\n","Validation RMSLE: 0.0255\n","Epoch 109/300, Average Training Loss: 0.8168\n","Validation RMSLE: 0.0254\n","Epoch 110/300, Average Training Loss: 0.8204\n","Validation RMSLE: 0.0254\n","Epoch 111/300, Average Training Loss: 0.8208\n","Validation RMSLE: 0.0256\n","Epoch 112/300, Average Training Loss: 0.8169\n","Validation RMSLE: 0.0257\n","Epoch 113/300, Average Training Loss: 0.8193\n","Validation RMSLE: 0.0257\n","Epoch 114/300, Average Training Loss: 0.8179\n","Validation RMSLE: 0.0256\n","Epoch 115/300, Average Training Loss: 0.8133\n","Validation RMSLE: 0.0255\n","Epoch 116/300, Average Training Loss: 0.8151\n","Validation RMSLE: 0.0255\n","Epoch 117/300, Average Training Loss: 0.8032\n","Validation RMSLE: 0.0258\n","Epoch 118/300, Average Training Loss: 0.8053\n","Validation RMSLE: 0.0261\n","Epoch 119/300, Average Training Loss: 0.8083\n","Validation RMSLE: 0.0261\n","Epoch 120/300, Average Training Loss: 0.8253\n","Validation RMSLE: 0.0259\n","Epoch 121/300, Average Training Loss: 0.8310\n","Validation RMSLE: 0.0255\n","Epoch 122/300, Average Training Loss: 0.8122\n","Validation RMSLE: 0.0252\n","Epoch 123/300, Average Training Loss: 0.7977\n","Validation RMSLE: 0.0249\n","Epoch 124/300, Average Training Loss: 0.7910\n","Validation RMSLE: 0.0249\n","Epoch 125/300, Average Training Loss: 0.8007\n","Validation RMSLE: 0.0251\n","Epoch 126/300, Average Training Loss: 0.8081\n","Validation RMSLE: 0.0252\n","Epoch 127/300, Average Training Loss: 0.8157\n","Validation RMSLE: 0.0254\n","Epoch 128/300, Average Training Loss: 0.8145\n","Validation RMSLE: 0.0255\n","Epoch 129/300, Average Training Loss: 0.8218\n","Validation RMSLE: 0.0254\n","Epoch 130/300, Average Training Loss: 0.8167\n","Validation RMSLE: 0.0254\n","Epoch 131/300, Average Training Loss: 0.8327\n","Validation RMSLE: 0.0253\n","Epoch 132/300, Average Training Loss: 0.8294\n","Validation RMSLE: 0.0251\n","Epoch 133/300, Average Training Loss: 0.8292\n","Validation RMSLE: 0.0251\n","Epoch 134/300, Average Training Loss: 0.8249\n","Validation RMSLE: 0.0252\n","Epoch 135/300, Average Training Loss: 0.8243\n","Validation RMSLE: 0.0254\n","Epoch 136/300, Average Training Loss: 0.8168\n","Validation RMSLE: 0.0255\n","Epoch 137/300, Average Training Loss: 0.8037\n","Validation RMSLE: 0.0255\n","Epoch 138/300, Average Training Loss: 0.7913\n","Validation RMSLE: 0.0255\n","Epoch 139/300, Average Training Loss: 0.7876\n","Validation RMSLE: 0.0256\n","Epoch 140/300, Average Training Loss: 0.7868\n","Validation RMSLE: 0.0257\n","Epoch 141/300, Average Training Loss: 0.7972\n","Validation RMSLE: 0.0257\n","Epoch 142/300, Average Training Loss: 0.8135\n","Validation RMSLE: 0.0257\n","Epoch 143/300, Average Training Loss: 0.8129\n","Validation RMSLE: 0.0256\n","Epoch 144/300, Average Training Loss: 0.8192\n","Validation RMSLE: 0.0254\n","Epoch 145/300, Average Training Loss: 0.8164\n","Validation RMSLE: 0.0252\n","Epoch 146/300, Average Training Loss: 0.8148\n","Validation RMSLE: 0.0253\n","Epoch 147/300, Average Training Loss: 0.8122\n","Validation RMSLE: 0.0255\n","Epoch 148/300, Average Training Loss: 0.8120\n","Validation RMSLE: 0.0256\n","Epoch 149/300, Average Training Loss: 0.8278\n","Validation RMSLE: 0.0254\n","Epoch 150/300, Average Training Loss: 0.8141\n","Validation RMSLE: 0.0254\n","Epoch 151/300, Average Training Loss: 0.8211\n","Validation RMSLE: 0.0253\n","Epoch 152/300, Average Training Loss: 0.8059\n","Validation RMSLE: 0.0251\n","Epoch 153/300, Average Training Loss: 0.8180\n","Validation RMSLE: 0.0251\n","Epoch 154/300, Average Training Loss: 0.8101\n","Validation RMSLE: 0.0253\n","Epoch 155/300, Average Training Loss: 0.8168\n","Validation RMSLE: 0.0254\n","Epoch 156/300, Average Training Loss: 0.8143\n","Validation RMSLE: 0.0256\n","Epoch 157/300, Average Training Loss: 0.8183\n","Validation RMSLE: 0.0255\n","Epoch 158/300, Average Training Loss: 0.8216\n","Validation RMSLE: 0.0253\n","Epoch 159/300, Average Training Loss: 0.8039\n","Validation RMSLE: 0.0250\n","Epoch 160/300, Average Training Loss: 0.7987\n","Validation RMSLE: 0.0251\n","Epoch 161/300, Average Training Loss: 0.7946\n","Validation RMSLE: 0.0252\n","Epoch 162/300, Average Training Loss: 0.8034\n","Validation RMSLE: 0.0251\n","Epoch 163/300, Average Training Loss: 0.7952\n","Validation RMSLE: 0.0251\n","Epoch 164/300, Average Training Loss: 0.7957\n","Validation RMSLE: 0.0251\n","Epoch 165/300, Average Training Loss: 0.8100\n","Validation RMSLE: 0.0250\n","Epoch 166/300, Average Training Loss: 0.8160\n","Validation RMSLE: 0.0250\n","Epoch 167/300, Average Training Loss: 0.8057\n","Validation RMSLE: 0.0251\n","Epoch 168/300, Average Training Loss: 0.7971\n","Validation RMSLE: 0.0254\n","Epoch 169/300, Average Training Loss: 0.7997\n","Validation RMSLE: 0.0255\n","Epoch 170/300, Average Training Loss: 0.8032\n","Validation RMSLE: 0.0254\n","Epoch 171/300, Average Training Loss: 0.8009\n","Validation RMSLE: 0.0254\n","Epoch 172/300, Average Training Loss: 0.7951\n","Validation RMSLE: 0.0254\n","Epoch 173/300, Average Training Loss: 0.7880\n","Validation RMSLE: 0.0253\n","Epoch 174/300, Average Training Loss: 0.7900\n","Validation RMSLE: 0.0252\n","Epoch 175/300, Average Training Loss: 0.7922\n","Validation RMSLE: 0.0252\n","Epoch 176/300, Average Training Loss: 0.8000\n","Validation RMSLE: 0.0252\n","Epoch 177/300, Average Training Loss: 0.7967\n","Validation RMSLE: 0.0257\n","Epoch 178/300, Average Training Loss: 0.7959\n","Validation RMSLE: 0.0261\n","Epoch 179/300, Average Training Loss: 0.8065\n","Validation RMSLE: 0.0260\n","Epoch 180/300, Average Training Loss: 0.8094\n","Validation RMSLE: 0.0255\n","Epoch 181/300, Average Training Loss: 0.8138\n","Validation RMSLE: 0.0251\n","Epoch 182/300, Average Training Loss: 0.7933\n","Validation RMSLE: 0.0249\n","Epoch 183/300, Average Training Loss: 0.7966\n","Validation RMSLE: 0.0247\n","Epoch 184/300, Average Training Loss: 0.7989\n","Validation RMSLE: 0.0249\n","Epoch 185/300, Average Training Loss: 0.8011\n","Validation RMSLE: 0.0252\n","Epoch 186/300, Average Training Loss: 0.8010\n","Validation RMSLE: 0.0254\n","Epoch 187/300, Average Training Loss: 0.7985\n","Validation RMSLE: 0.0255\n","Epoch 188/300, Average Training Loss: 0.8114\n","Validation RMSLE: 0.0254\n","Epoch 189/300, Average Training Loss: 0.8129\n","Validation RMSLE: 0.0250\n","Epoch 190/300, Average Training Loss: 0.8068\n","Validation RMSLE: 0.0249\n","Epoch 191/300, Average Training Loss: 0.7946\n","Validation RMSLE: 0.0248\n","Epoch 192/300, Average Training Loss: 0.7806\n","Validation RMSLE: 0.0251\n","Epoch 193/300, Average Training Loss: 0.7816\n","Validation RMSLE: 0.0254\n","Epoch 194/300, Average Training Loss: 0.7771\n","Validation RMSLE: 0.0258\n","Epoch 195/300, Average Training Loss: 0.7908\n","Validation RMSLE: 0.0260\n","Epoch 196/300, Average Training Loss: 0.7933\n","Validation RMSLE: 0.0256\n","Epoch 197/300, Average Training Loss: 0.8056\n","Validation RMSLE: 0.0251\n","Epoch 198/300, Average Training Loss: 0.7977\n","Validation RMSLE: 0.0251\n","Epoch 199/300, Average Training Loss: 0.8038\n","Validation RMSLE: 0.0252\n","Epoch 200/300, Average Training Loss: 0.8100\n","Validation RMSLE: 0.0254\n","Epoch 201/300, Average Training Loss: 0.8219\n","Validation RMSLE: 0.0255\n","Epoch 202/300, Average Training Loss: 0.8165\n","Validation RMSLE: 0.0255\n","Epoch 203/300, Average Training Loss: 0.8086\n","Validation RMSLE: 0.0256\n","Epoch 204/300, Average Training Loss: 0.8053\n","Validation RMSLE: 0.0255\n","Epoch 205/300, Average Training Loss: 0.8058\n","Validation RMSLE: 0.0254\n","Epoch 206/300, Average Training Loss: 0.7967\n","Validation RMSLE: 0.0253\n","Epoch 207/300, Average Training Loss: 0.7903\n","Validation RMSLE: 0.0253\n","Epoch 208/300, Average Training Loss: 0.7884\n","Validation RMSLE: 0.0253\n","Epoch 209/300, Average Training Loss: 0.7902\n","Validation RMSLE: 0.0252\n","Epoch 210/300, Average Training Loss: 0.7829\n","Validation RMSLE: 0.0252\n","Epoch 211/300, Average Training Loss: 0.7836\n","Validation RMSLE: 0.0253\n","Epoch 212/300, Average Training Loss: 0.7876\n","Validation RMSLE: 0.0254\n","Epoch 213/300, Average Training Loss: 0.8026\n","Validation RMSLE: 0.0254\n","Epoch 214/300, Average Training Loss: 0.7953\n","Validation RMSLE: 0.0252\n","Epoch 215/300, Average Training Loss: 0.7926\n","Validation RMSLE: 0.0253\n","Epoch 216/300, Average Training Loss: 0.7771\n","Validation RMSLE: 0.0254\n","Epoch 217/300, Average Training Loss: 0.7800\n","Validation RMSLE: 0.0253\n","Epoch 218/300, Average Training Loss: 0.7808\n","Validation RMSLE: 0.0251\n","Epoch 219/300, Average Training Loss: 0.7841\n","Validation RMSLE: 0.0249\n","Epoch 220/300, Average Training Loss: 0.7847\n","Validation RMSLE: 0.0249\n","Epoch 221/300, Average Training Loss: 0.7940\n","Validation RMSLE: 0.0249\n","Epoch 222/300, Average Training Loss: 0.7880\n","Validation RMSLE: 0.0250\n","Epoch 223/300, Average Training Loss: 0.7916\n","Validation RMSLE: 0.0251\n","Epoch 224/300, Average Training Loss: 0.7934\n","Validation RMSLE: 0.0252\n","Epoch 225/300, Average Training Loss: 0.8102\n","Validation RMSLE: 0.0252\n","Epoch 226/300, Average Training Loss: 0.8079\n","Validation RMSLE: 0.0252\n","Epoch 227/300, Average Training Loss: 0.8008\n","Validation RMSLE: 0.0251\n","Epoch 228/300, Average Training Loss: 0.7718\n","Validation RMSLE: 0.0253\n","Epoch 229/300, Average Training Loss: 0.7707\n","Validation RMSLE: 0.0256\n","Epoch 230/300, Average Training Loss: 0.7715\n","Validation RMSLE: 0.0259\n","Epoch 231/300, Average Training Loss: 0.7884\n","Validation RMSLE: 0.0257\n","Epoch 232/300, Average Training Loss: 0.7890\n","Validation RMSLE: 0.0255\n","Epoch 233/300, Average Training Loss: 0.7856\n","Validation RMSLE: 0.0252\n","Epoch 234/300, Average Training Loss: 0.7913\n","Validation RMSLE: 0.0251\n","Epoch 235/300, Average Training Loss: 0.7944\n","Validation RMSLE: 0.0252\n","Epoch 236/300, Average Training Loss: 0.7914\n","Validation RMSLE: 0.0254\n","Epoch 237/300, Average Training Loss: 0.7979\n","Validation RMSLE: 0.0256\n","Epoch 238/300, Average Training Loss: 0.7998\n","Validation RMSLE: 0.0256\n","Epoch 239/300, Average Training Loss: 0.8023\n","Validation RMSLE: 0.0254\n","Epoch 240/300, Average Training Loss: 0.8004\n","Validation RMSLE: 0.0253\n","Epoch 241/300, Average Training Loss: 0.7921\n","Validation RMSLE: 0.0251\n","Epoch 242/300, Average Training Loss: 0.7889\n","Validation RMSLE: 0.0250\n","Epoch 243/300, Average Training Loss: 0.7755\n","Validation RMSLE: 0.0252\n","Epoch 244/300, Average Training Loss: 0.7755\n","Validation RMSLE: 0.0254\n","Epoch 245/300, Average Training Loss: 0.7727\n","Validation RMSLE: 0.0254\n","Epoch 246/300, Average Training Loss: 0.7746\n","Validation RMSLE: 0.0257\n","Epoch 247/300, Average Training Loss: 0.7784\n","Validation RMSLE: 0.0257\n","Epoch 248/300, Average Training Loss: 0.7831\n","Validation RMSLE: 0.0258\n","Epoch 249/300, Average Training Loss: 0.7764\n","Validation RMSLE: 0.0259\n","Epoch 250/300, Average Training Loss: 0.7757\n","Validation RMSLE: 0.0260\n","Epoch 251/300, Average Training Loss: 0.7789\n","Validation RMSLE: 0.0260\n","Epoch 252/300, Average Training Loss: 0.7864\n","Validation RMSLE: 0.0258\n","Epoch 253/300, Average Training Loss: 0.7850\n","Validation RMSLE: 0.0254\n","Epoch 254/300, Average Training Loss: 0.7865\n","Validation RMSLE: 0.0252\n","Epoch 255/300, Average Training Loss: 0.7792\n","Validation RMSLE: 0.0249\n","Epoch 256/300, Average Training Loss: 0.7803\n","Validation RMSLE: 0.0247\n","Epoch 257/300, Average Training Loss: 0.7697\n","Validation RMSLE: 0.0249\n","Epoch 258/300, Average Training Loss: 0.7794\n","Validation RMSLE: 0.0254\n","Epoch 259/300, Average Training Loss: 0.7886\n","Validation RMSLE: 0.0258\n","Epoch 260/300, Average Training Loss: 0.8002\n","Validation RMSLE: 0.0261\n","Epoch 261/300, Average Training Loss: 0.8072\n","Validation RMSLE: 0.0260\n","Epoch 262/300, Average Training Loss: 0.7946\n","Validation RMSLE: 0.0256\n","Epoch 263/300, Average Training Loss: 0.7911\n","Validation RMSLE: 0.0250\n","Epoch 264/300, Average Training Loss: 0.7845\n","Validation RMSLE: 0.0247\n","Epoch 265/300, Average Training Loss: 0.7872\n","Validation RMSLE: 0.0248\n","Epoch 266/300, Average Training Loss: 0.7900\n","Validation RMSLE: 0.0251\n","Epoch 267/300, Average Training Loss: 0.7892\n","Validation RMSLE: 0.0253\n","Epoch 268/300, Average Training Loss: 0.7755\n","Validation RMSLE: 0.0257\n","Epoch 269/300, Average Training Loss: 0.7731\n","Validation RMSLE: 0.0259\n","Epoch 270/300, Average Training Loss: 0.7655\n","Validation RMSLE: 0.0257\n","Epoch 271/300, Average Training Loss: 0.7737\n","Validation RMSLE: 0.0253\n","Epoch 272/300, Average Training Loss: 0.7707\n","Validation RMSLE: 0.0251\n","Epoch 273/300, Average Training Loss: 0.7681\n","Validation RMSLE: 0.0250\n","Epoch 274/300, Average Training Loss: 0.7699\n","Validation RMSLE: 0.0249\n","Epoch 275/300, Average Training Loss: 0.7623\n","Validation RMSLE: 0.0250\n","Epoch 276/300, Average Training Loss: 0.7694\n","Validation RMSLE: 0.0252\n","Epoch 277/300, Average Training Loss: 0.7756\n","Validation RMSLE: 0.0255\n","Epoch 278/300, Average Training Loss: 0.7809\n","Validation RMSLE: 0.0257\n","Epoch 279/300, Average Training Loss: 0.7749\n","Validation RMSLE: 0.0258\n","Epoch 280/300, Average Training Loss: 0.7719\n","Validation RMSLE: 0.0256\n","Epoch 281/300, Average Training Loss: 0.7722\n","Validation RMSLE: 0.0256\n","Epoch 282/300, Average Training Loss: 0.7745\n","Validation RMSLE: 0.0255\n","Epoch 283/300, Average Training Loss: 0.7726\n","Validation RMSLE: 0.0255\n","Epoch 284/300, Average Training Loss: 0.7679\n","Validation RMSLE: 0.0254\n","Epoch 285/300, Average Training Loss: 0.7727\n","Validation RMSLE: 0.0253\n","Epoch 286/300, Average Training Loss: 0.7772\n","Validation RMSLE: 0.0252\n","Epoch 287/300, Average Training Loss: 0.7857\n","Validation RMSLE: 0.0252\n","Epoch 288/300, Average Training Loss: 0.7673\n","Validation RMSLE: 0.0254\n","Epoch 289/300, Average Training Loss: 0.7565\n","Validation RMSLE: 0.0257\n","Epoch 290/300, Average Training Loss: 0.7549\n","Validation RMSLE: 0.0259\n","Epoch 291/300, Average Training Loss: 0.7694\n","Validation RMSLE: 0.0260\n","Epoch 292/300, Average Training Loss: 0.7636\n","Validation RMSLE: 0.0260\n","Epoch 293/300, Average Training Loss: 0.7496\n","Validation RMSLE: 0.0258\n","Epoch 294/300, Average Training Loss: 0.7581\n","Validation RMSLE: 0.0253\n","Epoch 295/300, Average Training Loss: 0.7652\n","Validation RMSLE: 0.0251\n","Epoch 296/300, Average Training Loss: 0.7741\n","Validation RMSLE: 0.0252\n","Epoch 297/300, Average Training Loss: 0.7761\n","Validation RMSLE: 0.0253\n","Epoch 298/300, Average Training Loss: 0.7794\n","Validation RMSLE: 0.0253\n","Epoch 299/300, Average Training Loss: 0.7802\n","Validation RMSLE: 0.0255\n","Epoch 300/300, Average Training Loss: 0.7697\n","Validation RMSLE: 0.0257\n","Training process has finished\n","fold 4, train rmsle: 0.019235, valid rmsle: 0.025704\n","5-fold validation: avg train rmse: nan, avg valid rmse: nan\n"]}],"source":["k, num_epochs, lr, weight_decay, batch_size = 5, 300, 0.0025, 0, 360\n","train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\n","                          weight_decay, batch_size)\n","print('%d-fold validation: avg train rmse: %f, avg valid rmse: %f' % (k, train_l, valid_l))"]},{"cell_type":"markdown","metadata":{"id":"IJ0OGSrGvhPj"},"source":["You will notice that sometimes the number of training errors for a set of hyper-parameters can be very low, while the number of errors for the $k$-fold cross validation may be higher. This is most likely a consequence of overfitting. Therefore, when we reduce the amount of training errors, we need to check whether the amount of errors in the k-fold cross-validation have also been reduced accordingly."]},{"cell_type":"markdown","metadata":{"id":"PZekvL9aNTkN"},"source":["\n","##  **4. Predict and Submit**\n","\n","Once you have figured out what a good choice of hyperparameters should be, you might as well use all the data to train on it (rather than just $1-1/k$ of the data that is used in the crossvalidation slices). The model that we obtain in this way can then be applied to the test set. Saving the estimates in a CSV file will simplify uploading the results to Kaggle.\n","\n","So your task here is that once you have decided what are the best hyperparameters, train on the entire Kaggle training set, test it on the Kaggle test set, save the result as a .csv file and submit it to Kaggle. The steps are quite simple:\n","* Log in to the Kaggle website and visit the House Price Prediction Competition page.\n","* Click the “Submit Predictions” or “Late Submission” button on the right.\n","* Click the “Upload Submission File” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n","* Click the “Make Submission” button at the bottom of the page to view your results.\n","\n","You need to include a screenshot of your results in the assignment submission."]},{"cell_type":"markdown","metadata":{},"source":["## Prediction"]},{"cell_type":"code","execution_count":1303,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/300, Average Training Loss: 11.6494\n","Epoch 2/300, Average Training Loss: 23.2180\n","Epoch 3/300, Average Training Loss: 34.6319\n","Epoch 4/300, Average Training Loss: 45.7534\n","Epoch 5/300, Average Training Loss: 44.7251\n","Epoch 6/300, Average Training Loss: 43.0382\n","Epoch 7/300, Average Training Loss: 40.6835\n","Epoch 8/300, Average Training Loss: 37.7568\n","Epoch 9/300, Average Training Loss: 34.5301\n","Epoch 10/300, Average Training Loss: 31.3882\n","Epoch 11/300, Average Training Loss: 28.4578\n","Epoch 12/300, Average Training Loss: 25.8474\n","Epoch 13/300, Average Training Loss: 23.5930\n","Epoch 14/300, Average Training Loss: 21.5632\n","Epoch 15/300, Average Training Loss: 19.7764\n","Epoch 16/300, Average Training Loss: 18.1824\n","Epoch 17/300, Average Training Loss: 16.7054\n","Epoch 18/300, Average Training Loss: 15.3770\n","Epoch 19/300, Average Training Loss: 14.1348\n","Epoch 20/300, Average Training Loss: 13.0009\n","Epoch 21/300, Average Training Loss: 11.9613\n","Epoch 22/300, Average Training Loss: 10.9651\n","Epoch 23/300, Average Training Loss: 10.0431\n","Epoch 24/300, Average Training Loss: 9.1681\n","Epoch 25/300, Average Training Loss: 8.3372\n","Epoch 26/300, Average Training Loss: 7.5640\n","Epoch 27/300, Average Training Loss: 6.8739\n","Epoch 28/300, Average Training Loss: 6.2416\n","Epoch 29/300, Average Training Loss: 5.6337\n","Epoch 30/300, Average Training Loss: 5.0382\n","Epoch 31/300, Average Training Loss: 4.4707\n","Epoch 32/300, Average Training Loss: 3.9357\n","Epoch 33/300, Average Training Loss: 3.4704\n","Epoch 34/300, Average Training Loss: 3.0961\n","Epoch 35/300, Average Training Loss: 2.8145\n","Epoch 36/300, Average Training Loss: 2.5378\n","Epoch 37/300, Average Training Loss: 2.3261\n","Epoch 38/300, Average Training Loss: 2.1599\n","Epoch 39/300, Average Training Loss: 1.9683\n","Epoch 40/300, Average Training Loss: 1.9867\n","Epoch 41/300, Average Training Loss: 1.9003\n","Epoch 42/300, Average Training Loss: 1.8335\n","Epoch 43/300, Average Training Loss: 1.7805\n","Epoch 44/300, Average Training Loss: 1.6827\n","Epoch 45/300, Average Training Loss: 1.7224\n","Epoch 46/300, Average Training Loss: 1.6790\n","Epoch 47/300, Average Training Loss: 1.6251\n","Epoch 48/300, Average Training Loss: 1.6076\n","Epoch 49/300, Average Training Loss: 1.4947\n","Epoch 50/300, Average Training Loss: 1.4422\n","Epoch 51/300, Average Training Loss: 1.4337\n","Epoch 52/300, Average Training Loss: 1.3173\n","Epoch 53/300, Average Training Loss: 1.2904\n","Epoch 54/300, Average Training Loss: 1.3958\n","Epoch 55/300, Average Training Loss: 1.4618\n","Epoch 56/300, Average Training Loss: 1.4502\n","Epoch 57/300, Average Training Loss: 1.4598\n","Epoch 58/300, Average Training Loss: 1.3664\n","Epoch 59/300, Average Training Loss: 1.2973\n","Epoch 60/300, Average Training Loss: 1.3003\n","Epoch 61/300, Average Training Loss: 1.3026\n","Epoch 62/300, Average Training Loss: 1.3055\n","Epoch 63/300, Average Training Loss: 1.2860\n","Epoch 64/300, Average Training Loss: 1.2825\n","Epoch 65/300, Average Training Loss: 1.2662\n","Epoch 66/300, Average Training Loss: 1.4909\n","Epoch 67/300, Average Training Loss: 1.4856\n","Epoch 68/300, Average Training Loss: 1.4945\n","Epoch 69/300, Average Training Loss: 1.5018\n","Epoch 70/300, Average Training Loss: 1.2709\n","Epoch 71/300, Average Training Loss: 1.2615\n","Epoch 72/300, Average Training Loss: 1.2446\n","Epoch 73/300, Average Training Loss: 1.2306\n","Epoch 74/300, Average Training Loss: 1.2215\n","Epoch 75/300, Average Training Loss: 1.2198\n","Epoch 76/300, Average Training Loss: 1.2344\n","Epoch 77/300, Average Training Loss: 1.2354\n","Epoch 78/300, Average Training Loss: 1.3280\n","Epoch 79/300, Average Training Loss: 1.3427\n","Epoch 80/300, Average Training Loss: 1.3267\n","Epoch 81/300, Average Training Loss: 1.3217\n","Epoch 82/300, Average Training Loss: 1.2255\n","Epoch 83/300, Average Training Loss: 1.2200\n","Epoch 84/300, Average Training Loss: 1.2399\n","Epoch 85/300, Average Training Loss: 1.2415\n","Epoch 86/300, Average Training Loss: 1.3433\n","Epoch 87/300, Average Training Loss: 1.3454\n","Epoch 88/300, Average Training Loss: 1.3311\n","Epoch 89/300, Average Training Loss: 1.3360\n","Epoch 90/300, Average Training Loss: 1.2447\n","Epoch 91/300, Average Training Loss: 1.3259\n","Epoch 92/300, Average Training Loss: 1.3249\n","Epoch 93/300, Average Training Loss: 1.3204\n","Epoch 94/300, Average Training Loss: 1.3018\n","Epoch 95/300, Average Training Loss: 1.2097\n","Epoch 96/300, Average Training Loss: 1.1958\n","Epoch 97/300, Average Training Loss: 1.1924\n","Epoch 98/300, Average Training Loss: 1.2931\n","Epoch 99/300, Average Training Loss: 1.2997\n","Epoch 100/300, Average Training Loss: 1.3092\n","Epoch 101/300, Average Training Loss: 1.3015\n","Epoch 102/300, Average Training Loss: 1.2050\n","Epoch 103/300, Average Training Loss: 1.3043\n","Epoch 104/300, Average Training Loss: 1.2972\n","Epoch 105/300, Average Training Loss: 1.2948\n","Epoch 106/300, Average Training Loss: 1.3066\n","Epoch 107/300, Average Training Loss: 1.2164\n","Epoch 108/300, Average Training Loss: 1.2065\n","Epoch 109/300, Average Training Loss: 1.2165\n","Epoch 110/300, Average Training Loss: 1.2009\n","Epoch 111/300, Average Training Loss: 1.1941\n","Epoch 112/300, Average Training Loss: 1.2143\n","Epoch 113/300, Average Training Loss: 1.2167\n","Epoch 114/300, Average Training Loss: 1.3429\n","Epoch 115/300, Average Training Loss: 1.3457\n","Epoch 116/300, Average Training Loss: 1.3611\n","Epoch 117/300, Average Training Loss: 1.3467\n","Epoch 118/300, Average Training Loss: 1.2248\n","Epoch 119/300, Average Training Loss: 1.2292\n","Epoch 120/300, Average Training Loss: 1.1989\n","Epoch 121/300, Average Training Loss: 1.2048\n","Epoch 122/300, Average Training Loss: 1.2005\n","Epoch 123/300, Average Training Loss: 1.1958\n","Epoch 124/300, Average Training Loss: 1.1900\n","Epoch 125/300, Average Training Loss: 1.1856\n","Epoch 126/300, Average Training Loss: 1.1870\n","Epoch 127/300, Average Training Loss: 1.1903\n","Epoch 128/300, Average Training Loss: 1.2187\n","Epoch 129/300, Average Training Loss: 1.2363\n","Epoch 130/300, Average Training Loss: 1.2325\n","Epoch 131/300, Average Training Loss: 1.2345\n","Epoch 132/300, Average Training Loss: 1.2190\n","Epoch 133/300, Average Training Loss: 1.1948\n","Epoch 134/300, Average Training Loss: 1.3357\n","Epoch 135/300, Average Training Loss: 1.3334\n","Epoch 136/300, Average Training Loss: 1.3285\n","Epoch 137/300, Average Training Loss: 1.3365\n","Epoch 138/300, Average Training Loss: 1.1890\n","Epoch 139/300, Average Training Loss: 1.1879\n","Epoch 140/300, Average Training Loss: 1.1914\n","Epoch 141/300, Average Training Loss: 1.1943\n","Epoch 142/300, Average Training Loss: 1.2023\n","Epoch 143/300, Average Training Loss: 1.3310\n","Epoch 144/300, Average Training Loss: 1.3245\n","Epoch 145/300, Average Training Loss: 1.4667\n","Epoch 146/300, Average Training Loss: 1.4582\n","Epoch 147/300, Average Training Loss: 1.3226\n","Epoch 148/300, Average Training Loss: 1.3277\n","Epoch 149/300, Average Training Loss: 1.1910\n","Epoch 150/300, Average Training Loss: 1.1822\n","Epoch 151/300, Average Training Loss: 1.1891\n","Epoch 152/300, Average Training Loss: 1.3098\n","Epoch 153/300, Average Training Loss: 1.2962\n","Epoch 154/300, Average Training Loss: 1.2974\n","Epoch 155/300, Average Training Loss: 1.2952\n","Epoch 156/300, Average Training Loss: 1.1569\n","Epoch 157/300, Average Training Loss: 1.2583\n","Epoch 158/300, Average Training Loss: 1.2586\n","Epoch 159/300, Average Training Loss: 1.4674\n","Epoch 160/300, Average Training Loss: 1.4967\n","Epoch 161/300, Average Training Loss: 1.4150\n","Epoch 162/300, Average Training Loss: 1.4248\n","Epoch 163/300, Average Training Loss: 1.2072\n","Epoch 164/300, Average Training Loss: 1.1987\n","Epoch 165/300, Average Training Loss: 1.1921\n","Epoch 166/300, Average Training Loss: 1.2014\n","Epoch 167/300, Average Training Loss: 1.1951\n","Epoch 168/300, Average Training Loss: 1.1858\n","Epoch 169/300, Average Training Loss: 1.1794\n","Epoch 170/300, Average Training Loss: 1.1798\n","Epoch 171/300, Average Training Loss: 1.1807\n","Epoch 172/300, Average Training Loss: 1.1920\n","Epoch 173/300, Average Training Loss: 1.1950\n","Epoch 174/300, Average Training Loss: 1.1782\n","Epoch 175/300, Average Training Loss: 1.1708\n","Epoch 176/300, Average Training Loss: 1.1554\n","Epoch 177/300, Average Training Loss: 1.1452\n","Epoch 178/300, Average Training Loss: 1.1579\n","Epoch 179/300, Average Training Loss: 1.1514\n","Epoch 180/300, Average Training Loss: 1.1634\n","Epoch 181/300, Average Training Loss: 1.1604\n","Epoch 182/300, Average Training Loss: 1.1346\n","Epoch 183/300, Average Training Loss: 1.1501\n","Epoch 184/300, Average Training Loss: 1.1512\n","Epoch 185/300, Average Training Loss: 1.1662\n","Epoch 186/300, Average Training Loss: 1.1818\n","Epoch 187/300, Average Training Loss: 1.1836\n","Epoch 188/300, Average Training Loss: 1.1894\n","Epoch 189/300, Average Training Loss: 1.1718\n","Epoch 190/300, Average Training Loss: 1.1908\n","Epoch 191/300, Average Training Loss: 1.1946\n","Epoch 192/300, Average Training Loss: 1.1827\n","Epoch 193/300, Average Training Loss: 1.2965\n","Epoch 194/300, Average Training Loss: 1.2751\n","Epoch 195/300, Average Training Loss: 1.2746\n","Epoch 196/300, Average Training Loss: 1.2691\n","Epoch 197/300, Average Training Loss: 1.1497\n","Epoch 198/300, Average Training Loss: 1.1594\n","Epoch 199/300, Average Training Loss: 1.1317\n","Epoch 200/300, Average Training Loss: 1.1482\n","Epoch 201/300, Average Training Loss: 1.1620\n","Epoch 202/300, Average Training Loss: 1.2485\n","Epoch 203/300, Average Training Loss: 1.2678\n","Epoch 204/300, Average Training Loss: 1.2566\n","Epoch 205/300, Average Training Loss: 1.2623\n","Epoch 206/300, Average Training Loss: 1.1805\n","Epoch 207/300, Average Training Loss: 1.1802\n","Epoch 208/300, Average Training Loss: 1.2699\n","Epoch 209/300, Average Training Loss: 1.2552\n","Epoch 210/300, Average Training Loss: 1.2544\n","Epoch 211/300, Average Training Loss: 1.2553\n","Epoch 212/300, Average Training Loss: 1.1706\n","Epoch 213/300, Average Training Loss: 1.3040\n","Epoch 214/300, Average Training Loss: 1.2961\n","Epoch 215/300, Average Training Loss: 1.2907\n","Epoch 216/300, Average Training Loss: 1.2810\n","Epoch 217/300, Average Training Loss: 1.1507\n","Epoch 218/300, Average Training Loss: 1.1436\n","Epoch 219/300, Average Training Loss: 1.1400\n","Epoch 220/300, Average Training Loss: 1.1438\n","Epoch 221/300, Average Training Loss: 1.1407\n","Epoch 222/300, Average Training Loss: 1.1317\n","Epoch 223/300, Average Training Loss: 1.1317\n","Epoch 224/300, Average Training Loss: 1.1352\n","Epoch 225/300, Average Training Loss: 1.1378\n","Epoch 226/300, Average Training Loss: 1.1570\n","Epoch 227/300, Average Training Loss: 1.1583\n","Epoch 228/300, Average Training Loss: 1.1742\n","Epoch 229/300, Average Training Loss: 1.1860\n","Epoch 230/300, Average Training Loss: 1.1754\n","Epoch 231/300, Average Training Loss: 1.1871\n","Epoch 232/300, Average Training Loss: 1.1795\n","Epoch 233/300, Average Training Loss: 1.1876\n","Epoch 234/300, Average Training Loss: 1.2028\n","Epoch 235/300, Average Training Loss: 1.1964\n","Epoch 236/300, Average Training Loss: 1.1882\n","Epoch 237/300, Average Training Loss: 1.1744\n","Epoch 238/300, Average Training Loss: 1.1716\n","Epoch 239/300, Average Training Loss: 1.1760\n","Epoch 240/300, Average Training Loss: 1.1688\n","Epoch 241/300, Average Training Loss: 1.1431\n","Epoch 242/300, Average Training Loss: 1.2727\n","Epoch 243/300, Average Training Loss: 1.3456\n","Epoch 244/300, Average Training Loss: 1.3469\n","Epoch 245/300, Average Training Loss: 1.3802\n","Epoch 246/300, Average Training Loss: 1.2573\n","Epoch 247/300, Average Training Loss: 1.1849\n","Epoch 248/300, Average Training Loss: 1.1975\n","Epoch 249/300, Average Training Loss: 1.1985\n","Epoch 250/300, Average Training Loss: 1.1874\n","Epoch 251/300, Average Training Loss: 1.1862\n","Epoch 252/300, Average Training Loss: 1.1855\n","Epoch 253/300, Average Training Loss: 1.1747\n","Epoch 254/300, Average Training Loss: 1.1725\n","Epoch 255/300, Average Training Loss: 1.2723\n","Epoch 256/300, Average Training Loss: 1.2612\n","Epoch 257/300, Average Training Loss: 1.2779\n","Epoch 258/300, Average Training Loss: 1.3728\n","Epoch 259/300, Average Training Loss: 1.4070\n","Epoch 260/300, Average Training Loss: 1.4054\n","Epoch 261/300, Average Training Loss: 1.4065\n","Epoch 262/300, Average Training Loss: 1.3019\n","Epoch 263/300, Average Training Loss: 1.1713\n","Epoch 264/300, Average Training Loss: 1.1686\n","Epoch 265/300, Average Training Loss: 1.1486\n","Epoch 266/300, Average Training Loss: 1.1450\n","Epoch 267/300, Average Training Loss: 1.1370\n","Epoch 268/300, Average Training Loss: 1.1392\n","Epoch 269/300, Average Training Loss: 1.2521\n","Epoch 270/300, Average Training Loss: 1.2586\n","Epoch 271/300, Average Training Loss: 1.2644\n","Epoch 272/300, Average Training Loss: 1.2604\n","Epoch 273/300, Average Training Loss: 1.1453\n","Epoch 274/300, Average Training Loss: 1.1485\n","Epoch 275/300, Average Training Loss: 1.1339\n","Epoch 276/300, Average Training Loss: 1.1471\n","Epoch 277/300, Average Training Loss: 1.1501\n","Epoch 278/300, Average Training Loss: 1.2861\n","Epoch 279/300, Average Training Loss: 1.2964\n","Epoch 280/300, Average Training Loss: 1.2903\n","Epoch 281/300, Average Training Loss: 1.2903\n","Epoch 282/300, Average Training Loss: 1.1478\n","Epoch 283/300, Average Training Loss: 1.1518\n","Epoch 284/300, Average Training Loss: 1.2454\n","Epoch 285/300, Average Training Loss: 1.2447\n","Epoch 286/300, Average Training Loss: 1.2648\n","Epoch 287/300, Average Training Loss: 1.2598\n","Epoch 288/300, Average Training Loss: 1.2548\n","Epoch 289/300, Average Training Loss: 1.2559\n","Epoch 290/300, Average Training Loss: 1.2490\n","Epoch 291/300, Average Training Loss: 1.2639\n","Epoch 292/300, Average Training Loss: 1.1808\n","Epoch 293/300, Average Training Loss: 1.1769\n","Epoch 294/300, Average Training Loss: 1.1764\n","Epoch 295/300, Average Training Loss: 1.2655\n","Epoch 296/300, Average Training Loss: 1.2661\n","Epoch 297/300, Average Training Loss: 1.2587\n","Epoch 298/300, Average Training Loss: 1.2437\n","Epoch 299/300, Average Training Loss: 1.1555\n","Epoch 300/300, Average Training Loss: 1.1536\n","Training process has finished\n"]}],"source":["class HousePriceTestDataset(Dataset):\n","    def __init__(self, x_tensor):\n","        if(type(x_tensor) == np.ndarray):\n","            self.x = x_tensor\n","        else:\n","            self.x = x_tensor.values\n","        \n","    def __getitem__(self, index):\n","        return self.x[index]\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","test_dataset = HousePriceTestDataset(test_features)\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","net = MLP(inputSize=test_features.shape[1], outputSize=1, lr=lr, weight_decay=weight_decay)\n","trainer = Trainer(n_epochs=num_epochs)\n","trainloader = torch.utils.data.DataLoader(HousePriceDataset(train_features, train_labels), batch_size=batch_size, shuffle=True)\n","trainer.fit(net, trainloader)\n","net.eval()\n","predictions = []\n","\n","with torch.no_grad():\n","    for inputs in testloader:\n","        outputs = net(inputs)\n","        predictions.extend(outputs.numpy().reshape(-1))"]},{"cell_type":"code","execution_count":1304,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction for example 1: 118094.5234375\n","Prediction for example 2: 117956.671875\n","Prediction for example 3: 177952.71875\n","Prediction for example 4: 199377.40625\n","Prediction for example 5: 202254.5\n","Prediction for example 6: 173401.71875\n","Prediction for example 7: 181612.84375\n","Prediction for example 8: 161796.0\n","Prediction for example 9: 184800.625\n","Prediction for example 10: 122129.28125\n","Prediction for example 11: 190846.609375\n","Prediction for example 12: 96144.9296875\n","Prediction for example 13: 94633.703125\n","Prediction for example 14: 146038.734375\n","Prediction for example 15: 108684.875\n","Prediction for example 16: 332216.96875\n","Prediction for example 17: 251030.640625\n","Prediction for example 18: 276004.34375\n","Prediction for example 19: 281700.875\n","Prediction for example 20: 398302.1875\n","Prediction for example 21: 293191.28125\n","Prediction for example 22: 207887.75\n","Prediction for example 23: 179827.3125\n","Prediction for example 24: 156810.8125\n","Prediction for example 25: 187469.0625\n","Prediction for example 26: 199539.78125\n","Prediction for example 27: 314122.40625\n","Prediction for example 28: 234801.15625\n","Prediction for example 29: 201979.296875\n","Prediction for example 30: 236590.359375\n","Prediction for example 31: 194685.515625\n","Prediction for example 32: 89234.0\n","Prediction for example 33: 187103.90625\n","Prediction for example 34: 285696.09375\n","Prediction for example 35: 277397.875\n","Prediction for example 36: 222551.28125\n","Prediction for example 37: 181243.3125\n","Prediction for example 38: 160293.953125\n","Prediction for example 39: 162010.96875\n","Prediction for example 40: 156928.09375\n","Prediction for example 41: 173228.765625\n","Prediction for example 42: 149312.859375\n","Prediction for example 43: 282250.09375\n","Prediction for example 44: 242515.34375\n","Prediction for example 45: 228255.46875\n","Prediction for example 46: 196807.90625\n","Prediction for example 47: 247054.8125\n","Prediction for example 48: 200504.453125\n","Prediction for example 49: 153602.84375\n","Prediction for example 50: 144182.34375\n","Prediction for example 51: 150333.84375\n","Prediction for example 52: 183903.0625\n","Prediction for example 53: 131374.453125\n","Prediction for example 54: 158209.09375\n","Prediction for example 55: 216795.0\n","Prediction for example 56: 146266.90625\n","Prediction for example 57: 164624.171875\n","Prediction for example 58: 129296.734375\n","Prediction for example 59: 215961.625\n","Prediction for example 60: 127813.34375\n","Prediction for example 61: 129972.7578125\n","Prediction for example 62: 185917.875\n","Prediction for example 63: 106376.2890625\n","Prediction for example 64: 115941.0\n","Prediction for example 65: 116356.46875\n","Prediction for example 66: 111761.65625\n","Prediction for example 67: 110599.078125\n","Prediction for example 68: 142716.0\n","Prediction for example 69: 147228.5\n","Prediction for example 70: 198291.125\n","Prediction for example 71: 118163.765625\n","Prediction for example 72: 100235.9296875\n","Prediction for example 73: 146093.828125\n","Prediction for example 74: 121048.5234375\n","Prediction for example 75: 150147.40625\n","Prediction for example 76: 102384.15625\n","Prediction for example 77: 53523.765625\n","Prediction for example 78: 161959.59375\n","Prediction for example 79: 206408.78125\n","Prediction for example 80: 88915.6796875\n","Prediction for example 81: 132102.859375\n","Prediction for example 82: 139777.15625\n","Prediction for example 83: 183862.90625\n","Prediction for example 84: 85930.4375\n","Prediction for example 85: 112924.25\n","Prediction for example 86: 138726.71875\n","Prediction for example 87: 138316.046875\n","Prediction for example 88: 127830.515625\n","Prediction for example 89: 119106.453125\n","Prediction for example 90: 130402.21875\n","Prediction for example 91: 110829.4765625\n","Prediction for example 92: 147040.328125\n","Prediction for example 93: 155308.109375\n","Prediction for example 94: 116774.0859375\n","Prediction for example 95: 170884.484375\n","Prediction for example 96: 72101.8125\n","Prediction for example 97: 109167.671875\n","Prediction for example 98: 102536.953125\n","Prediction for example 99: 79200.9140625\n","Prediction for example 100: 122145.1328125\n","Prediction for example 101: 124688.078125\n","Prediction for example 102: 129557.265625\n","Prediction for example 103: 118641.015625\n","Prediction for example 104: 149676.953125\n","Prediction for example 105: 144705.9375\n","Prediction for example 106: 234992.640625\n","Prediction for example 107: 74417.546875\n","Prediction for example 108: 239983.015625\n","Prediction for example 109: 119436.1328125\n","Prediction for example 110: 139331.1875\n","Prediction for example 111: 123510.21875\n","Prediction for example 112: 139540.0625\n","Prediction for example 113: 232925.71875\n","Prediction for example 114: 110354.8125\n","Prediction for example 115: 230925.984375\n","Prediction for example 116: 254361.296875\n","Prediction for example 117: 179704.609375\n","Prediction for example 118: 145655.875\n","Prediction for example 119: 134674.6875\n","Prediction for example 120: 194717.1875\n","Prediction for example 121: 161966.0625\n","Prediction for example 122: 118170.75\n","Prediction for example 123: 287127.9375\n","Prediction for example 124: 227776.59375\n","Prediction for example 125: 137824.46875\n","Prediction for example 126: 70861.078125\n","Prediction for example 127: 104247.953125\n","Prediction for example 128: 144066.875\n","Prediction for example 129: 94129.078125\n","Prediction for example 130: 131145.734375\n","Prediction for example 131: 91737.3203125\n","Prediction for example 132: 116082.9453125\n","Prediction for example 133: 130209.0859375\n","Prediction for example 134: 143774.0625\n","Prediction for example 135: 106027.2265625\n","Prediction for example 136: 222323.796875\n","Prediction for example 137: 185875.953125\n","Prediction for example 138: 208475.421875\n","Prediction for example 139: 166322.46875\n","Prediction for example 140: 162325.5625\n","Prediction for example 141: 73770.671875\n","Prediction for example 142: 108714.8125\n","Prediction for example 143: 67375.578125\n","Prediction for example 144: 251946.96875\n","Prediction for example 145: 241804.71875\n","Prediction for example 146: 173734.21875\n","Prediction for example 147: 179989.859375\n","Prediction for example 148: 219998.1875\n","Prediction for example 149: 196554.375\n","Prediction for example 150: 152854.34375\n","Prediction for example 151: 138251.875\n","Prediction for example 152: 170830.3125\n","Prediction for example 153: 176241.734375\n","Prediction for example 154: 120931.375\n","Prediction for example 155: 92923.578125\n","Prediction for example 156: 77788.9765625\n","Prediction for example 157: 92110.0\n","Prediction for example 158: 120345.8359375\n","Prediction for example 159: 143347.6875\n","Prediction for example 160: 170040.0\n","Prediction for example 161: 136689.90625\n","Prediction for example 162: 140490.84375\n","Prediction for example 163: 262007.421875\n","Prediction for example 164: 223281.578125\n","Prediction for example 165: 117610.921875\n","Prediction for example 166: 171206.765625\n","Prediction for example 167: 191697.0\n","Prediction for example 168: 254098.703125\n","Prediction for example 169: 171385.6875\n","Prediction for example 170: 316435.34375\n","Prediction for example 171: 212222.953125\n","Prediction for example 172: 236792.125\n","Prediction for example 173: 171895.640625\n","Prediction for example 174: 187620.859375\n","Prediction for example 175: 175937.21875\n","Prediction for example 176: 157381.078125\n","Prediction for example 177: 204738.9375\n","Prediction for example 178: 198330.3125\n","Prediction for example 179: 193880.390625\n","Prediction for example 180: 245440.390625\n","Prediction for example 181: 186824.46875\n","Prediction for example 182: 245444.84375\n","Prediction for example 183: 228117.125\n","Prediction for example 184: 230575.4375\n","Prediction for example 185: 238400.5625\n","Prediction for example 186: 161536.84375\n","Prediction for example 187: 156528.03125\n","Prediction for example 188: 119799.375\n","Prediction for example 189: 136125.25\n","Prediction for example 190: 116667.015625\n","Prediction for example 191: 119694.109375\n","Prediction for example 192: 97863.015625\n","Prediction for example 193: 103365.5\n","Prediction for example 194: 142277.3125\n","Prediction for example 195: 131379.953125\n","Prediction for example 196: 136378.21875\n","Prediction for example 197: 146705.75\n","Prediction for example 198: 147618.078125\n","Prediction for example 199: 120102.90625\n","Prediction for example 200: 167550.078125\n","Prediction for example 201: 375945.5625\n","Prediction for example 202: 342069.03125\n","Prediction for example 203: 327615.21875\n","Prediction for example 204: 403631.28125\n","Prediction for example 205: 291412.46875\n","Prediction for example 206: 293888.875\n","Prediction for example 207: 314032.09375\n","Prediction for example 208: 298972.90625\n","Prediction for example 209: 278388.03125\n","Prediction for example 210: 314879.875\n","Prediction for example 211: 268389.28125\n","Prediction for example 212: 363170.21875\n","Prediction for example 213: 279558.125\n","Prediction for example 214: 244795.609375\n","Prediction for example 215: 201371.78125\n","Prediction for example 216: 200981.0\n","Prediction for example 217: 217831.4375\n","Prediction for example 218: 388358.34375\n","Prediction for example 219: 329881.875\n","Prediction for example 220: 304521.65625\n","Prediction for example 221: 247115.734375\n","Prediction for example 222: 281777.1875\n","Prediction for example 223: 198084.75\n","Prediction for example 224: 184802.421875\n","Prediction for example 225: 178473.15625\n","Prediction for example 226: 164109.25\n","Prediction for example 227: 173507.4375\n","Prediction for example 228: 196208.484375\n","Prediction for example 229: 199320.96875\n","Prediction for example 230: 200549.71875\n","Prediction for example 231: 190299.65625\n","Prediction for example 232: 242232.59375\n","Prediction for example 233: 168919.09375\n","Prediction for example 234: 181945.0625\n","Prediction for example 235: 163750.953125\n","Prediction for example 236: 266221.78125\n","Prediction for example 237: 172211.46875\n","Prediction for example 238: 318055.65625\n","Prediction for example 239: 295400.90625\n","Prediction for example 240: 258042.03125\n","Prediction for example 241: 270173.15625\n","Prediction for example 242: 241767.78125\n","Prediction for example 243: 243048.75\n","Prediction for example 244: 260288.25\n","Prediction for example 245: 225335.9375\n","Prediction for example 246: 369633.0625\n","Prediction for example 247: 213900.96875\n","Prediction for example 248: 208503.90625\n","Prediction for example 249: 253424.0625\n","Prediction for example 250: 218229.578125\n","Prediction for example 251: 251897.90625\n","Prediction for example 252: 260246.4375\n","Prediction for example 253: 271275.1875\n","Prediction for example 254: 222915.375\n","Prediction for example 255: 208073.4375\n","Prediction for example 256: 188100.0625\n","Prediction for example 257: 177018.34375\n","Prediction for example 258: 133113.859375\n","Prediction for example 259: 204884.40625\n","Prediction for example 260: 234891.0\n","Prediction for example 261: 167789.59375\n","Prediction for example 262: 119015.0390625\n","Prediction for example 263: 154059.359375\n","Prediction for example 264: 214478.171875\n","Prediction for example 265: 236584.890625\n","Prediction for example 266: 193653.046875\n","Prediction for example 267: 157031.4375\n","Prediction for example 268: 176866.671875\n","Prediction for example 269: 158317.328125\n","Prediction for example 270: 156163.203125\n","Prediction for example 271: 116510.53125\n","Prediction for example 272: 121862.375\n","Prediction for example 273: 111776.6328125\n","Prediction for example 274: 108411.0234375\n","Prediction for example 275: 123969.671875\n","Prediction for example 276: 101747.515625\n","Prediction for example 277: 304963.34375\n","Prediction for example 278: 252962.328125\n","Prediction for example 279: 253003.875\n","Prediction for example 280: 227838.03125\n","Prediction for example 281: 194312.53125\n","Prediction for example 282: 171110.71875\n","Prediction for example 283: 178704.171875\n","Prediction for example 284: 316715.03125\n","Prediction for example 285: 217028.53125\n","Prediction for example 286: 197084.921875\n","Prediction for example 287: 226971.9375\n","Prediction for example 288: 235498.59375\n","Prediction for example 289: 143202.84375\n","Prediction for example 290: 123465.6328125\n","Prediction for example 291: 259860.09375\n","Prediction for example 292: 117641.25\n","Prediction for example 293: 145866.375\n","Prediction for example 294: 191737.5625\n","Prediction for example 295: 160661.90625\n","Prediction for example 296: 140404.125\n","Prediction for example 297: 118791.875\n","Prediction for example 298: 147853.6875\n","Prediction for example 299: 167079.6875\n","Prediction for example 300: 171569.046875\n","Prediction for example 301: 147364.046875\n","Prediction for example 302: 179605.84375\n","Prediction for example 303: 183026.171875\n","Prediction for example 304: 108520.1171875\n","Prediction for example 305: 164033.609375\n","Prediction for example 306: 209338.59375\n","Prediction for example 307: 245268.453125\n","Prediction for example 308: 141736.4375\n","Prediction for example 309: 174281.125\n","Prediction for example 310: 160606.890625\n","Prediction for example 311: 107800.8046875\n","Prediction for example 312: 124223.34375\n","Prediction for example 313: 117989.21875\n","Prediction for example 314: 150326.65625\n","Prediction for example 315: 140688.921875\n","Prediction for example 316: 132088.203125\n","Prediction for example 317: 106316.640625\n","Prediction for example 318: 146580.15625\n","Prediction for example 319: 121692.4453125\n","Prediction for example 320: 177588.59375\n","Prediction for example 321: 121919.59375\n","Prediction for example 322: 79912.921875\n","Prediction for example 323: 149190.15625\n","Prediction for example 324: 110576.265625\n","Prediction for example 325: 128273.3125\n","Prediction for example 326: 156059.71875\n","Prediction for example 327: 179230.9375\n","Prediction for example 328: 52216.9609375\n","Prediction for example 329: 99687.390625\n","Prediction for example 330: 76285.546875\n","Prediction for example 331: 192601.9375\n","Prediction for example 332: 167846.578125\n","Prediction for example 333: 133123.1875\n","Prediction for example 334: 160976.59375\n","Prediction for example 335: 130580.203125\n","Prediction for example 336: 146199.625\n","Prediction for example 337: 110148.15625\n","Prediction for example 338: 115733.109375\n","Prediction for example 339: 108542.40625\n","Prediction for example 340: 126141.015625\n","Prediction for example 341: 121319.1015625\n","Prediction for example 342: 130834.671875\n","Prediction for example 343: 149857.234375\n","Prediction for example 344: 127919.53125\n","Prediction for example 345: 139933.0\n","Prediction for example 346: 127088.21875\n","Prediction for example 347: 133155.3125\n","Prediction for example 348: 123097.2109375\n","Prediction for example 349: 126838.3515625\n","Prediction for example 350: 134704.59375\n","Prediction for example 351: 83271.328125\n","Prediction for example 352: 92434.96875\n","Prediction for example 353: 119792.609375\n","Prediction for example 354: 87284.6953125\n","Prediction for example 355: 58853.359375\n","Prediction for example 356: 102186.25\n","Prediction for example 357: 103283.578125\n","Prediction for example 358: 155493.84375\n","Prediction for example 359: 130328.078125\n","Prediction for example 360: 64380.0703125\n","Prediction for example 361: 100331.3046875\n","Prediction for example 362: 162456.71875\n","Prediction for example 363: 49641.41015625\n","Prediction for example 364: 136707.28125\n","Prediction for example 365: 146476.0625\n","Prediction for example 366: 95110.9765625\n","Prediction for example 367: 101700.984375\n","Prediction for example 368: 137906.71875\n","Prediction for example 369: 134056.5\n","Prediction for example 370: 151461.875\n","Prediction for example 371: 148284.8125\n","Prediction for example 372: 74946.5078125\n","Prediction for example 373: 144895.5625\n","Prediction for example 374: 117651.046875\n","Prediction for example 375: 97287.7890625\n","Prediction for example 376: 124874.296875\n","Prediction for example 377: 91632.03125\n","Prediction for example 378: 122760.8671875\n","Prediction for example 379: 100080.2734375\n","Prediction for example 380: 122401.59375\n","Prediction for example 381: 127357.2265625\n","Prediction for example 382: 90010.734375\n","Prediction for example 383: 125595.703125\n","Prediction for example 384: 128961.921875\n","Prediction for example 385: 140759.453125\n","Prediction for example 386: 141353.75\n","Prediction for example 387: 177976.90625\n","Prediction for example 388: 52898.265625\n","Prediction for example 389: 116351.078125\n","Prediction for example 390: 113685.0625\n","Prediction for example 391: 148365.078125\n","Prediction for example 392: 120050.6640625\n","Prediction for example 393: 121089.15625\n","Prediction for example 394: 167194.78125\n","Prediction for example 395: 153387.546875\n","Prediction for example 396: 229898.78125\n","Prediction for example 397: 147252.90625\n","Prediction for example 398: 130029.0625\n","Prediction for example 399: 115103.1875\n","Prediction for example 400: 142336.875\n","Prediction for example 401: 114258.203125\n","Prediction for example 402: 285739.84375\n","Prediction for example 403: 275532.90625\n","Prediction for example 404: 275558.65625\n","Prediction for example 405: 306749.40625\n","Prediction for example 406: 293354.125\n","Prediction for example 407: 227013.1875\n","Prediction for example 408: 270727.46875\n","Prediction for example 409: 207285.59375\n","Prediction for example 410: 220076.0\n","Prediction for example 411: 228675.84375\n","Prediction for example 412: 181302.40625\n","Prediction for example 413: 246471.65625\n","Prediction for example 414: 153506.546875\n","Prediction for example 415: 197435.015625\n","Prediction for example 416: 200690.9375\n","Prediction for example 417: 213220.65625\n","Prediction for example 418: 202225.640625\n","Prediction for example 419: 122124.09375\n","Prediction for example 420: 134239.125\n","Prediction for example 421: 247720.578125\n","Prediction for example 422: 242527.375\n","Prediction for example 423: 191174.625\n","Prediction for example 424: 208372.71875\n","Prediction for example 425: 221915.0\n","Prediction for example 426: 258575.8125\n","Prediction for example 427: 218816.59375\n","Prediction for example 428: 261530.53125\n","Prediction for example 429: 177639.453125\n","Prediction for example 430: 109678.59375\n","Prediction for example 431: 116549.71875\n","Prediction for example 432: 90526.015625\n","Prediction for example 433: 120326.59375\n","Prediction for example 434: 118379.0859375\n","Prediction for example 435: 137554.5625\n","Prediction for example 436: 129391.0\n","Prediction for example 437: 125148.3671875\n","Prediction for example 438: 112940.8828125\n","Prediction for example 439: 169192.53125\n","Prediction for example 440: 161807.8125\n","Prediction for example 441: 181034.453125\n","Prediction for example 442: 159856.84375\n","Prediction for example 443: 220632.84375\n","Prediction for example 444: 126920.953125\n","Prediction for example 445: 201393.8125\n","Prediction for example 446: 134366.390625\n","Prediction for example 447: 223669.140625\n","Prediction for example 448: 116341.640625\n","Prediction for example 449: 131673.34375\n","Prediction for example 450: 119763.4375\n","Prediction for example 451: 210393.09375\n","Prediction for example 452: 307793.78125\n","Prediction for example 453: 148406.5\n","Prediction for example 454: 76987.65625\n","Prediction for example 455: 293316.53125\n","Prediction for example 456: 47580.890625\n","Prediction for example 457: 246799.640625\n","Prediction for example 458: 144107.25\n","Prediction for example 459: 179851.09375\n","Prediction for example 460: 155466.546875\n","Prediction for example 461: 331382.90625\n","Prediction for example 462: 294312.09375\n","Prediction for example 463: 237470.859375\n","Prediction for example 464: 229063.359375\n","Prediction for example 465: 215332.5\n","Prediction for example 466: 332947.25\n","Prediction for example 467: 134779.84375\n","Prediction for example 468: 170231.671875\n","Prediction for example 469: 125138.21875\n","Prediction for example 470: 130725.3359375\n","Prediction for example 471: 141476.34375\n","Prediction for example 472: 133616.109375\n","Prediction for example 473: 201593.578125\n","Prediction for example 474: 183524.84375\n","Prediction for example 475: 166702.09375\n","Prediction for example 476: 181551.015625\n","Prediction for example 477: 181525.1875\n","Prediction for example 478: 176051.9375\n","Prediction for example 479: 232382.171875\n","Prediction for example 480: 201630.375\n","Prediction for example 481: 163924.65625\n","Prediction for example 482: 179096.5625\n","Prediction for example 483: 212878.65625\n","Prediction for example 484: 304839.46875\n","Prediction for example 485: 331014.1875\n","Prediction for example 486: 135630.46875\n","Prediction for example 487: 286522.84375\n","Prediction for example 488: 166917.28125\n","Prediction for example 489: 247021.125\n","Prediction for example 490: 186035.71875\n","Prediction for example 491: 251612.1875\n","Prediction for example 492: 217957.125\n","Prediction for example 493: 180164.953125\n","Prediction for example 494: 183744.8125\n","Prediction for example 495: 132414.359375\n","Prediction for example 496: 311128.09375\n","Prediction for example 497: 156031.890625\n","Prediction for example 498: 283755.875\n","Prediction for example 499: 137119.625\n","Prediction for example 500: 103306.6484375\n","Prediction for example 501: 121190.3046875\n","Prediction for example 502: 101087.734375\n","Prediction for example 503: 116019.7734375\n","Prediction for example 504: 101877.5859375\n","Prediction for example 505: 140872.5\n","Prediction for example 506: 130978.3125\n","Prediction for example 507: 275062.84375\n","Prediction for example 508: 349949.84375\n","Prediction for example 509: 327861.90625\n","Prediction for example 510: 345250.46875\n","Prediction for example 511: 389623.6875\n","Prediction for example 512: 329308.03125\n","Prediction for example 513: 266678.0\n","Prediction for example 514: 304254.6875\n","Prediction for example 515: 405702.375\n","Prediction for example 516: 265578.40625\n","Prediction for example 517: 309259.0\n","Prediction for example 518: 309607.59375\n","Prediction for example 519: 308148.78125\n","Prediction for example 520: 192902.234375\n","Prediction for example 521: 312923.21875\n","Prediction for example 522: 219013.0625\n","Prediction for example 523: 205799.421875\n","Prediction for example 524: 179178.078125\n","Prediction for example 525: 217312.09375\n","Prediction for example 526: 213491.484375\n","Prediction for example 527: 187578.15625\n","Prediction for example 528: 183758.8125\n","Prediction for example 529: 199073.65625\n","Prediction for example 530: 212975.359375\n","Prediction for example 531: 235562.6875\n","Prediction for example 532: 210630.796875\n","Prediction for example 533: 177854.515625\n","Prediction for example 534: 229306.484375\n","Prediction for example 535: 186017.609375\n","Prediction for example 536: 250677.03125\n","Prediction for example 537: 291402.9375\n","Prediction for example 538: 291041.65625\n","Prediction for example 539: 274234.34375\n","Prediction for example 540: 299261.96875\n","Prediction for example 541: 266347.71875\n","Prediction for example 542: 237172.0\n","Prediction for example 543: 248955.328125\n","Prediction for example 544: 271470.25\n","Prediction for example 545: 215166.21875\n","Prediction for example 546: 221696.234375\n","Prediction for example 547: 243431.5625\n","Prediction for example 548: 221296.03125\n","Prediction for example 549: 196393.75\n","Prediction for example 550: 194452.703125\n","Prediction for example 551: 134720.71875\n","Prediction for example 552: 173714.171875\n","Prediction for example 553: 179346.3125\n","Prediction for example 554: 191835.90625\n","Prediction for example 555: 196226.15625\n","Prediction for example 556: 194365.21875\n","Prediction for example 557: 194775.375\n","Prediction for example 558: 113458.21875\n","Prediction for example 559: 133489.828125\n","Prediction for example 560: 104552.5859375\n","Prediction for example 561: 94014.6015625\n","Prediction for example 562: 199548.703125\n","Prediction for example 563: 132774.53125\n","Prediction for example 564: 294500.03125\n","Prediction for example 565: 310802.96875\n","Prediction for example 566: 176278.828125\n","Prediction for example 567: 163279.6875\n","Prediction for example 568: 159159.15625\n","Prediction for example 569: 157748.0\n","Prediction for example 570: 253990.484375\n","Prediction for example 571: 220936.15625\n","Prediction for example 572: 238567.15625\n","Prediction for example 573: 247243.140625\n","Prediction for example 574: 171710.8125\n","Prediction for example 575: 217562.6875\n","Prediction for example 576: 198203.28125\n","Prediction for example 577: 201385.109375\n","Prediction for example 578: 305795.21875\n","Prediction for example 579: 214254.3125\n","Prediction for example 580: 268527.46875\n","Prediction for example 581: 298910.90625\n","Prediction for example 582: 213241.1875\n","Prediction for example 583: 181072.765625\n","Prediction for example 584: 181871.03125\n","Prediction for example 585: 218832.6875\n","Prediction for example 586: 145622.0625\n","Prediction for example 587: 138986.84375\n","Prediction for example 588: 150745.9375\n","Prediction for example 589: 146727.03125\n","Prediction for example 590: 166132.5\n","Prediction for example 591: 114519.859375\n","Prediction for example 592: 112361.53125\n","Prediction for example 593: 150898.78125\n","Prediction for example 594: 98452.3125\n","Prediction for example 595: 154295.4375\n","Prediction for example 596: 155194.484375\n","Prediction for example 597: 111980.390625\n","Prediction for example 598: 221439.8125\n","Prediction for example 599: 131884.921875\n","Prediction for example 600: 190219.84375\n","Prediction for example 601: 179864.25\n","Prediction for example 602: 125804.625\n","Prediction for example 603: 117049.421875\n","Prediction for example 604: 137567.296875\n","Prediction for example 605: 123892.03125\n","Prediction for example 606: 166076.34375\n","Prediction for example 607: 112588.078125\n","Prediction for example 608: 147245.703125\n","Prediction for example 609: 94009.46875\n","Prediction for example 610: 113883.203125\n","Prediction for example 611: 96210.796875\n","Prediction for example 612: 154301.6875\n","Prediction for example 613: 134384.765625\n","Prediction for example 614: 187019.609375\n","Prediction for example 615: 137026.359375\n","Prediction for example 616: 124439.7890625\n","Prediction for example 617: 158005.609375\n","Prediction for example 618: 119862.34375\n","Prediction for example 619: 128722.4296875\n","Prediction for example 620: 119201.2890625\n","Prediction for example 621: 125372.4921875\n","Prediction for example 622: 115670.4296875\n","Prediction for example 623: 138171.5\n","Prediction for example 624: 107175.9609375\n","Prediction for example 625: 108365.4375\n","Prediction for example 626: 104877.140625\n","Prediction for example 627: 120224.671875\n","Prediction for example 628: 94223.828125\n","Prediction for example 629: 81510.34375\n","Prediction for example 630: 129212.1796875\n","Prediction for example 631: 101150.9296875\n","Prediction for example 632: 116992.578125\n","Prediction for example 633: 125200.75\n","Prediction for example 634: 110430.296875\n","Prediction for example 635: 149000.40625\n","Prediction for example 636: 85367.046875\n","Prediction for example 637: 94572.8046875\n","Prediction for example 638: 146287.875\n","Prediction for example 639: 57256.453125\n","Prediction for example 640: 65021.8046875\n","Prediction for example 641: 115679.703125\n","Prediction for example 642: 124457.953125\n","Prediction for example 643: 108263.953125\n","Prediction for example 644: 134607.875\n","Prediction for example 645: 138900.09375\n","Prediction for example 646: 53153.234375\n","Prediction for example 647: 205634.4375\n","Prediction for example 648: 107585.125\n","Prediction for example 649: 110223.8671875\n","Prediction for example 650: 134078.375\n","Prediction for example 651: 135503.90625\n","Prediction for example 652: 127971.8828125\n","Prediction for example 653: 116914.375\n","Prediction for example 654: 129557.5625\n","Prediction for example 655: 163872.28125\n","Prediction for example 656: 126670.875\n","Prediction for example 657: 148743.390625\n","Prediction for example 658: 126347.8046875\n","Prediction for example 659: 115000.28125\n","Prediction for example 660: 101357.4140625\n","Prediction for example 661: 100221.171875\n","Prediction for example 662: 119085.515625\n","Prediction for example 663: 90701.59375\n","Prediction for example 664: 160175.5625\n","Prediction for example 665: 124158.515625\n","Prediction for example 666: 159678.1875\n","Prediction for example 667: 166809.90625\n","Prediction for example 668: 131391.171875\n","Prediction for example 669: 73528.6171875\n","Prediction for example 670: 116536.5859375\n","Prediction for example 671: 127355.609375\n","Prediction for example 672: 118692.296875\n","Prediction for example 673: 122689.703125\n","Prediction for example 674: 108885.90625\n","Prediction for example 675: 95141.5625\n","Prediction for example 676: 79430.1015625\n","Prediction for example 677: 99634.65625\n","Prediction for example 678: 125448.671875\n","Prediction for example 679: 143218.375\n","Prediction for example 680: 141060.109375\n","Prediction for example 681: 159031.15625\n","Prediction for example 682: 112786.46875\n","Prediction for example 683: 142288.953125\n","Prediction for example 684: 115583.7109375\n","Prediction for example 685: 133673.296875\n","Prediction for example 686: 164571.71875\n","Prediction for example 687: 159219.609375\n","Prediction for example 688: 130709.125\n","Prediction for example 689: 154585.78125\n","Prediction for example 690: 215846.046875\n","Prediction for example 691: 119787.015625\n","Prediction for example 692: 183336.609375\n","Prediction for example 693: 191292.359375\n","Prediction for example 694: 105847.6875\n","Prediction for example 695: 147927.25\n","Prediction for example 696: 258084.03125\n","Prediction for example 697: 236027.828125\n","Prediction for example 698: 236176.9375\n","Prediction for example 699: 218136.234375\n","Prediction for example 700: 197715.5625\n","Prediction for example 701: 236282.78125\n","Prediction for example 702: 347506.28125\n","Prediction for example 703: 320562.78125\n","Prediction for example 704: 234608.015625\n","Prediction for example 705: 215506.75\n","Prediction for example 706: 161315.796875\n","Prediction for example 707: 212790.890625\n","Prediction for example 708: 199989.34375\n","Prediction for example 709: 194394.0\n","Prediction for example 710: 204172.8125\n","Prediction for example 711: 159170.578125\n","Prediction for example 712: 131247.921875\n","Prediction for example 713: 185863.6875\n","Prediction for example 714: 232074.34375\n","Prediction for example 715: 275649.84375\n","Prediction for example 716: 271348.5\n","Prediction for example 717: 243125.671875\n","Prediction for example 718: 215930.1875\n","Prediction for example 719: 139827.28125\n","Prediction for example 720: 207804.75\n","Prediction for example 721: 190586.4375\n","Prediction for example 722: 220457.0\n","Prediction for example 723: 189285.671875\n","Prediction for example 724: 118930.0390625\n","Prediction for example 725: 117915.359375\n","Prediction for example 726: 151052.3125\n","Prediction for example 727: 155861.8125\n","Prediction for example 728: 149237.71875\n","Prediction for example 729: 330801.3125\n","Prediction for example 730: 88979.921875\n","Prediction for example 731: 88625.3359375\n","Prediction for example 732: 96181.421875\n","Prediction for example 733: 113126.734375\n","Prediction for example 734: 106081.453125\n","Prediction for example 735: 94996.8125\n","Prediction for example 736: 100586.484375\n","Prediction for example 737: 119566.90625\n","Prediction for example 738: 172365.28125\n","Prediction for example 739: 185207.34375\n","Prediction for example 740: 143867.515625\n","Prediction for example 741: 144400.828125\n","Prediction for example 742: 202170.671875\n","Prediction for example 743: 143935.609375\n","Prediction for example 744: 188447.390625\n","Prediction for example 745: 114901.171875\n","Prediction for example 746: 154043.9375\n","Prediction for example 747: 216829.59375\n","Prediction for example 748: 253903.109375\n","Prediction for example 749: 254246.84375\n","Prediction for example 750: 122164.1875\n","Prediction for example 751: 113950.4375\n","Prediction for example 752: 120699.0\n","Prediction for example 753: 101777.7109375\n","Prediction for example 754: 131004.765625\n","Prediction for example 755: 101644.96875\n","Prediction for example 756: 139210.71875\n","Prediction for example 757: 50935.171875\n","Prediction for example 758: 96781.75\n","Prediction for example 759: 95287.484375\n","Prediction for example 760: 63472.57421875\n","Prediction for example 761: 295064.84375\n","Prediction for example 762: 266471.0625\n","Prediction for example 763: 265555.59375\n","Prediction for example 764: 218651.828125\n","Prediction for example 765: 124478.1640625\n","Prediction for example 766: 199650.21875\n","Prediction for example 767: 202367.0\n","Prediction for example 768: 250495.25\n","Prediction for example 769: 259285.484375\n","Prediction for example 770: 151301.484375\n","Prediction for example 771: 225536.5625\n","Prediction for example 772: 190638.546875\n","Prediction for example 773: 189497.09375\n","Prediction for example 774: 237790.953125\n","Prediction for example 775: 224336.6875\n","Prediction for example 776: 262886.90625\n","Prediction for example 777: 292065.25\n","Prediction for example 778: 186639.921875\n","Prediction for example 779: 102145.96875\n","Prediction for example 780: 147643.875\n","Prediction for example 781: 156003.5\n","Prediction for example 782: 117814.375\n","Prediction for example 783: 131437.75\n","Prediction for example 784: 94696.265625\n","Prediction for example 785: 95217.421875\n","Prediction for example 786: 142285.34375\n","Prediction for example 787: 120591.359375\n","Prediction for example 788: 125269.765625\n","Prediction for example 789: 121569.9453125\n","Prediction for example 790: 130030.234375\n","Prediction for example 791: 115939.3359375\n","Prediction for example 792: 183323.953125\n","Prediction for example 793: 157815.828125\n","Prediction for example 794: 181021.65625\n","Prediction for example 795: 188519.828125\n","Prediction for example 796: 173602.15625\n","Prediction for example 797: 209803.609375\n","Prediction for example 798: 159411.0\n","Prediction for example 799: 183346.28125\n","Prediction for example 800: 136220.671875\n","Prediction for example 801: 187832.34375\n","Prediction for example 802: 213819.390625\n","Prediction for example 803: 319803.8125\n","Prediction for example 804: 419748.8125\n","Prediction for example 805: 166097.09375\n","Prediction for example 806: 266228.78125\n","Prediction for example 807: 318540.53125\n","Prediction for example 808: 366798.28125\n","Prediction for example 809: 142496.171875\n","Prediction for example 810: 197514.78125\n","Prediction for example 811: 209945.3125\n","Prediction for example 812: 195823.9375\n","Prediction for example 813: 155153.1875\n","Prediction for example 814: 182254.046875\n","Prediction for example 815: 174839.40625\n","Prediction for example 816: 187079.90625\n","Prediction for example 817: 193874.671875\n","Prediction for example 818: 155687.65625\n","Prediction for example 819: 133429.140625\n","Prediction for example 820: 117633.90625\n","Prediction for example 821: 171288.6875\n","Prediction for example 822: 183135.5625\n","Prediction for example 823: 111952.875\n","Prediction for example 824: 115159.859375\n","Prediction for example 825: 136845.796875\n","Prediction for example 826: 125285.25\n","Prediction for example 827: 319787.09375\n","Prediction for example 828: 270836.09375\n","Prediction for example 829: 326094.65625\n","Prediction for example 830: 378950.6875\n","Prediction for example 831: 312522.78125\n","Prediction for example 832: 363569.1875\n","Prediction for example 833: 387085.34375\n","Prediction for example 834: 335574.21875\n","Prediction for example 835: 397509.25\n","Prediction for example 836: 292035.0625\n","Prediction for example 837: 317024.375\n","Prediction for example 838: 311300.0625\n","Prediction for example 839: 332888.375\n","Prediction for example 840: 297417.46875\n","Prediction for example 841: 290989.34375\n","Prediction for example 842: 252435.265625\n","Prediction for example 843: 245308.3125\n","Prediction for example 844: 261540.40625\n","Prediction for example 845: 210440.015625\n","Prediction for example 846: 199721.078125\n","Prediction for example 847: 207763.03125\n","Prediction for example 848: 221537.46875\n","Prediction for example 849: 271342.4375\n","Prediction for example 850: 217696.578125\n","Prediction for example 851: 211982.28125\n","Prediction for example 852: 188435.375\n","Prediction for example 853: 180395.78125\n","Prediction for example 854: 181766.390625\n","Prediction for example 855: 195432.671875\n","Prediction for example 856: 210821.65625\n","Prediction for example 857: 181327.84375\n","Prediction for example 858: 186691.921875\n","Prediction for example 859: 190647.65625\n","Prediction for example 860: 193922.640625\n","Prediction for example 861: 225895.125\n","Prediction for example 862: 193295.9375\n","Prediction for example 863: 182762.84375\n","Prediction for example 864: 177559.78125\n","Prediction for example 865: 222196.53125\n","Prediction for example 866: 182593.390625\n","Prediction for example 867: 206037.859375\n","Prediction for example 868: 220684.75\n","Prediction for example 869: 193013.09375\n","Prediction for example 870: 190644.1875\n","Prediction for example 871: 320628.28125\n","Prediction for example 872: 327260.65625\n","Prediction for example 873: 304868.15625\n","Prediction for example 874: 256373.734375\n","Prediction for example 875: 267365.6875\n","Prediction for example 876: 291946.03125\n","Prediction for example 877: 210304.5625\n","Prediction for example 878: 260519.515625\n","Prediction for example 879: 206927.96875\n","Prediction for example 880: 346050.875\n","Prediction for example 881: 229754.609375\n","Prediction for example 882: 227928.421875\n","Prediction for example 883: 230078.65625\n","Prediction for example 884: 213574.875\n","Prediction for example 885: 235886.28125\n","Prediction for example 886: 217479.71875\n","Prediction for example 887: 211197.953125\n","Prediction for example 888: 238491.171875\n","Prediction for example 889: 191648.65625\n","Prediction for example 890: 290462.78125\n","Prediction for example 891: 272807.78125\n","Prediction for example 892: 245043.8125\n","Prediction for example 893: 252447.8125\n","Prediction for example 894: 144961.421875\n","Prediction for example 895: 147843.703125\n","Prediction for example 896: 154058.8125\n","Prediction for example 897: 196158.84375\n","Prediction for example 898: 199586.75\n","Prediction for example 899: 138335.09375\n","Prediction for example 900: 115137.875\n","Prediction for example 901: 145652.84375\n","Prediction for example 902: 270548.25\n","Prediction for example 903: 124188.875\n","Prediction for example 904: 165252.6875\n","Prediction for example 905: 213135.1875\n","Prediction for example 906: 195338.359375\n","Prediction for example 907: 221667.03125\n","Prediction for example 908: 218062.34375\n","Prediction for example 909: 219274.5625\n","Prediction for example 910: 163313.515625\n","Prediction for example 911: 162854.46875\n","Prediction for example 912: 196410.890625\n","Prediction for example 913: 278381.90625\n","Prediction for example 914: 293090.53125\n","Prediction for example 915: 231674.9375\n","Prediction for example 916: 285748.84375\n","Prediction for example 917: 313952.21875\n","Prediction for example 918: 150925.4375\n","Prediction for example 919: 214070.421875\n","Prediction for example 920: 154916.59375\n","Prediction for example 921: 183928.46875\n","Prediction for example 922: 218197.71875\n","Prediction for example 923: 209398.53125\n","Prediction for example 924: 244009.640625\n","Prediction for example 925: 170173.421875\n","Prediction for example 926: 121560.6953125\n","Prediction for example 927: 133960.875\n","Prediction for example 928: 118123.6328125\n","Prediction for example 929: 124423.1875\n","Prediction for example 930: 147997.75\n","Prediction for example 931: 150296.15625\n","Prediction for example 932: 116972.125\n","Prediction for example 933: 169517.265625\n","Prediction for example 934: 135416.9375\n","Prediction for example 935: 209406.4375\n","Prediction for example 936: 140844.265625\n","Prediction for example 937: 226142.546875\n","Prediction for example 938: 142439.59375\n","Prediction for example 939: 64947.625\n","Prediction for example 940: 59771.89453125\n","Prediction for example 941: 121757.4375\n","Prediction for example 942: 138302.6875\n","Prediction for example 943: 145794.90625\n","Prediction for example 944: 153142.9375\n","Prediction for example 945: 162343.625\n","Prediction for example 946: 146264.953125\n","Prediction for example 947: 124345.25\n","Prediction for example 948: 154884.453125\n","Prediction for example 949: 122816.265625\n","Prediction for example 950: 182024.0\n","Prediction for example 951: 112901.0546875\n","Prediction for example 952: 150189.296875\n","Prediction for example 953: 124127.5625\n","Prediction for example 954: 169807.15625\n","Prediction for example 955: 129392.546875\n","Prediction for example 956: 123087.28125\n","Prediction for example 957: 131002.6171875\n","Prediction for example 958: 136532.28125\n","Prediction for example 959: 119572.890625\n","Prediction for example 960: 130803.015625\n","Prediction for example 961: 152289.140625\n","Prediction for example 962: 92038.1796875\n","Prediction for example 963: 116043.7734375\n","Prediction for example 964: 165013.78125\n","Prediction for example 965: 227559.96875\n","Prediction for example 966: 130932.4140625\n","Prediction for example 967: 126554.828125\n","Prediction for example 968: 182550.75\n","Prediction for example 969: 106381.40625\n","Prediction for example 970: 130617.765625\n","Prediction for example 971: 109952.875\n","Prediction for example 972: 146864.46875\n","Prediction for example 973: 150175.5625\n","Prediction for example 974: 127056.2578125\n","Prediction for example 975: 158799.53125\n","Prediction for example 976: 108955.109375\n","Prediction for example 977: 107764.3359375\n","Prediction for example 978: 119515.921875\n","Prediction for example 979: 102616.34375\n","Prediction for example 980: 119959.2734375\n","Prediction for example 981: 90487.3125\n","Prediction for example 982: 102536.3125\n","Prediction for example 983: 134441.609375\n","Prediction for example 984: 130461.234375\n","Prediction for example 985: 98639.5390625\n","Prediction for example 986: 144030.125\n","Prediction for example 987: 197061.953125\n","Prediction for example 988: 123862.765625\n","Prediction for example 989: 119170.484375\n","Prediction for example 990: 148578.5\n","Prediction for example 991: 126766.296875\n","Prediction for example 992: 201836.28125\n","Prediction for example 993: 103739.96875\n","Prediction for example 994: 125467.953125\n","Prediction for example 995: 125267.171875\n","Prediction for example 996: 127924.2421875\n","Prediction for example 997: 133179.515625\n","Prediction for example 998: 134726.109375\n","Prediction for example 999: 102223.7578125\n","Prediction for example 1000: 146863.671875\n","Prediction for example 1001: 124148.3671875\n","Prediction for example 1002: 139296.875\n","Prediction for example 1003: 122237.5703125\n","Prediction for example 1004: 178948.75\n","Prediction for example 1005: 140564.75\n","Prediction for example 1006: 113936.53125\n","Prediction for example 1007: 133202.59375\n","Prediction for example 1008: 81891.546875\n","Prediction for example 1009: 77963.046875\n","Prediction for example 1010: 203633.09375\n","Prediction for example 1011: 234628.09375\n","Prediction for example 1012: 171048.15625\n","Prediction for example 1013: 112283.6015625\n","Prediction for example 1014: 55965.17578125\n","Prediction for example 1015: 209434.3125\n","Prediction for example 1016: 118318.453125\n","Prediction for example 1017: 114430.515625\n","Prediction for example 1018: 154323.25\n","Prediction for example 1019: 100708.546875\n","Prediction for example 1020: 154115.90625\n","Prediction for example 1021: 115545.28125\n","Prediction for example 1022: 123792.9609375\n","Prediction for example 1023: 109362.0\n","Prediction for example 1024: 119296.0\n","Prediction for example 1025: 122516.53125\n","Prediction for example 1026: 143145.25\n","Prediction for example 1027: 188742.78125\n","Prediction for example 1028: 167480.78125\n","Prediction for example 1029: 156227.3125\n","Prediction for example 1030: 136080.3125\n","Prediction for example 1031: 103639.140625\n","Prediction for example 1032: 200728.6875\n","Prediction for example 1033: 157575.671875\n","Prediction for example 1034: 136891.796875\n","Prediction for example 1035: 92652.046875\n","Prediction for example 1036: 254654.0\n","Prediction for example 1037: 150794.65625\n","Prediction for example 1038: 111920.609375\n","Prediction for example 1039: 99268.453125\n","Prediction for example 1040: 122788.21875\n","Prediction for example 1041: 132455.578125\n","Prediction for example 1042: 146537.4375\n","Prediction for example 1043: 92523.09375\n","Prediction for example 1044: 204887.125\n","Prediction for example 1045: 223677.0625\n","Prediction for example 1046: 251482.90625\n","Prediction for example 1047: 286998.09375\n","Prediction for example 1048: 253701.3125\n","Prediction for example 1049: 218242.859375\n","Prediction for example 1050: 218230.375\n","Prediction for example 1051: 182918.59375\n","Prediction for example 1052: 205610.15625\n","Prediction for example 1053: 230581.984375\n","Prediction for example 1054: 256256.015625\n","Prediction for example 1055: 149591.421875\n","Prediction for example 1056: 177709.140625\n","Prediction for example 1057: 146597.90625\n","Prediction for example 1058: 157110.9375\n","Prediction for example 1059: 228236.75\n","Prediction for example 1060: 219690.96875\n","Prediction for example 1061: 196642.640625\n","Prediction for example 1062: 226897.484375\n","Prediction for example 1063: 109803.53125\n","Prediction for example 1064: 132884.46875\n","Prediction for example 1065: 148714.3125\n","Prediction for example 1066: 139393.28125\n","Prediction for example 1067: 121343.453125\n","Prediction for example 1068: 128801.21875\n","Prediction for example 1069: 141822.328125\n","Prediction for example 1070: 134225.1875\n","Prediction for example 1071: 251093.9375\n","Prediction for example 1072: 228740.484375\n","Prediction for example 1073: 203264.53125\n","Prediction for example 1074: 224302.890625\n","Prediction for example 1075: 276968.59375\n","Prediction for example 1076: 232340.46875\n","Prediction for example 1077: 233002.015625\n","Prediction for example 1078: 198180.71875\n","Prediction for example 1079: 187575.65625\n","Prediction for example 1080: 195331.8125\n","Prediction for example 1081: 188116.0625\n","Prediction for example 1082: 169432.15625\n","Prediction for example 1083: 123548.7890625\n","Prediction for example 1084: 108520.1875\n","Prediction for example 1085: 134819.21875\n","Prediction for example 1086: 123666.421875\n","Prediction for example 1087: 145044.984375\n","Prediction for example 1088: 155790.21875\n","Prediction for example 1089: 154160.484375\n","Prediction for example 1090: 558383.9375\n","Prediction for example 1091: 133153.09375\n","Prediction for example 1092: 127591.9765625\n","Prediction for example 1093: 79806.0625\n","Prediction for example 1094: 96232.3984375\n","Prediction for example 1095: 119582.78125\n","Prediction for example 1096: 103390.328125\n","Prediction for example 1097: 96591.0234375\n","Prediction for example 1098: 179988.3125\n","Prediction for example 1099: 137293.234375\n","Prediction for example 1100: 156779.4375\n","Prediction for example 1101: 145511.1875\n","Prediction for example 1102: 142596.34375\n","Prediction for example 1103: 148042.09375\n","Prediction for example 1104: 205692.46875\n","Prediction for example 1105: 148731.203125\n","Prediction for example 1106: 160832.5\n","Prediction for example 1107: 139392.421875\n","Prediction for example 1108: 223004.484375\n","Prediction for example 1109: 234977.03125\n","Prediction for example 1110: 116097.0625\n","Prediction for example 1111: 196436.0625\n","Prediction for example 1112: 137309.75\n","Prediction for example 1113: 222310.53125\n","Prediction for example 1114: 288806.96875\n","Prediction for example 1115: 128929.6171875\n","Prediction for example 1116: 124208.53125\n","Prediction for example 1117: 153220.171875\n","Prediction for example 1118: 83887.28125\n","Prediction for example 1119: 66071.875\n","Prediction for example 1120: 114088.21875\n","Prediction for example 1121: 127112.984375\n","Prediction for example 1122: 141860.546875\n","Prediction for example 1123: 269065.8125\n","Prediction for example 1124: 177168.609375\n","Prediction for example 1125: 197991.21875\n","Prediction for example 1126: 235473.90625\n","Prediction for example 1127: 206415.84375\n","Prediction for example 1128: 133971.0\n","Prediction for example 1129: 152851.625\n","Prediction for example 1130: 215822.671875\n","Prediction for example 1131: 245526.3125\n","Prediction for example 1132: 234002.578125\n","Prediction for example 1133: 262263.375\n","Prediction for example 1134: 186880.71875\n","Prediction for example 1135: 201387.0\n","Prediction for example 1136: 277988.46875\n","Prediction for example 1137: 197832.875\n","Prediction for example 1138: 270254.90625\n","Prediction for example 1139: 324067.125\n","Prediction for example 1140: 191214.71875\n","Prediction for example 1141: 135773.578125\n","Prediction for example 1142: 84070.015625\n","Prediction for example 1143: 87807.34375\n","Prediction for example 1144: 92097.0625\n","Prediction for example 1145: 76752.2890625\n","Prediction for example 1146: 145688.453125\n","Prediction for example 1147: 224677.03125\n","Prediction for example 1148: 217538.6875\n","Prediction for example 1149: 189875.28125\n","Prediction for example 1150: 113098.328125\n","Prediction for example 1151: 123187.4921875\n","Prediction for example 1152: 165360.625\n","Prediction for example 1153: 134703.1875\n","Prediction for example 1154: 119398.3125\n","Prediction for example 1155: 154957.453125\n","Prediction for example 1156: 162697.96875\n","Prediction for example 1157: 195791.375\n","Prediction for example 1158: 211408.90625\n","Prediction for example 1159: 204581.71875\n","Prediction for example 1160: 191383.15625\n","Prediction for example 1161: 174760.59375\n","Prediction for example 1162: 185821.046875\n","Prediction for example 1163: 242068.21875\n","Prediction for example 1164: 299660.84375\n","Prediction for example 1165: 288972.21875\n","Prediction for example 1166: 177260.90625\n","Prediction for example 1167: 161322.78125\n","Prediction for example 1168: 377839.59375\n","Prediction for example 1169: 402578.96875\n","Prediction for example 1170: 319270.5\n","Prediction for example 1171: 388832.40625\n","Prediction for example 1172: 352970.78125\n","Prediction for example 1173: 264892.90625\n","Prediction for example 1174: 360858.59375\n","Prediction for example 1175: 156170.9375\n","Prediction for example 1176: 179428.875\n","Prediction for example 1177: 166945.75\n","Prediction for example 1178: 270423.96875\n","Prediction for example 1179: 187300.84375\n","Prediction for example 1180: 149860.578125\n","Prediction for example 1181: 109935.4765625\n","Prediction for example 1182: 184560.734375\n","Prediction for example 1183: 106546.328125\n","Prediction for example 1184: 124272.0703125\n","Prediction for example 1185: 113494.8359375\n","Prediction for example 1186: 99463.015625\n","Prediction for example 1187: 110994.359375\n","Prediction for example 1188: 152243.3125\n","Prediction for example 1189: 157611.28125\n","Prediction for example 1190: 134486.234375\n","Prediction for example 1191: 137645.90625\n","Prediction for example 1192: 353482.03125\n","Prediction for example 1193: 251568.375\n","Prediction for example 1194: 259381.53125\n","Prediction for example 1195: 352378.875\n","Prediction for example 1196: 310337.875\n","Prediction for example 1197: 325770.03125\n","Prediction for example 1198: 300646.53125\n","Prediction for example 1199: 284453.9375\n","Prediction for example 1200: 328396.1875\n","Prediction for example 1201: 313253.03125\n","Prediction for example 1202: 317637.96875\n","Prediction for example 1203: 286280.84375\n","Prediction for example 1204: 265178.90625\n","Prediction for example 1205: 305438.96875\n","Prediction for example 1206: 271448.03125\n","Prediction for example 1207: 190984.171875\n","Prediction for example 1208: 193875.890625\n","Prediction for example 1209: 190627.609375\n","Prediction for example 1210: 267962.34375\n","Prediction for example 1211: 198313.3125\n","Prediction for example 1212: 198786.203125\n","Prediction for example 1213: 197910.125\n","Prediction for example 1214: 201678.0\n","Prediction for example 1215: 169341.25\n","Prediction for example 1216: 197842.625\n","Prediction for example 1217: 201107.59375\n","Prediction for example 1218: 253295.15625\n","Prediction for example 1219: 268685.84375\n","Prediction for example 1220: 273071.375\n","Prediction for example 1221: 340698.0\n","Prediction for example 1222: 307068.0625\n","Prediction for example 1223: 407928.9375\n","Prediction for example 1224: 292553.8125\n","Prediction for example 1225: 316716.59375\n","Prediction for example 1226: 262511.875\n","Prediction for example 1227: 299439.96875\n","Prediction for example 1228: 221739.359375\n","Prediction for example 1229: 211251.3125\n","Prediction for example 1230: 363059.15625\n","Prediction for example 1231: 198323.296875\n","Prediction for example 1232: 137970.78125\n","Prediction for example 1233: 210727.671875\n","Prediction for example 1234: 142444.546875\n","Prediction for example 1235: 198776.0\n","Prediction for example 1236: 190943.0625\n","Prediction for example 1237: 192722.578125\n","Prediction for example 1238: 194440.9375\n","Prediction for example 1239: 170977.03125\n","Prediction for example 1240: 147912.203125\n","Prediction for example 1241: 148925.34375\n","Prediction for example 1242: 105250.796875\n","Prediction for example 1243: 127876.3984375\n","Prediction for example 1244: 145149.484375\n","Prediction for example 1245: 112276.265625\n","Prediction for example 1246: 90185.34375\n","Prediction for example 1247: 118765.6875\n","Prediction for example 1248: 131461.890625\n","Prediction for example 1249: 109772.8984375\n","Prediction for example 1250: 125740.203125\n","Prediction for example 1251: 306889.9375\n","Prediction for example 1252: 348788.625\n","Prediction for example 1253: 179455.375\n","Prediction for example 1254: 155134.203125\n","Prediction for example 1255: 167452.21875\n","Prediction for example 1256: 152342.109375\n","Prediction for example 1257: 211632.625\n","Prediction for example 1258: 232389.8125\n","Prediction for example 1259: 152214.40625\n","Prediction for example 1260: 190202.765625\n","Prediction for example 1261: 129229.6796875\n","Prediction for example 1262: 170129.859375\n","Prediction for example 1263: 148007.015625\n","Prediction for example 1264: 121338.609375\n","Prediction for example 1265: 127888.671875\n","Prediction for example 1266: 151050.375\n","Prediction for example 1267: 190284.859375\n","Prediction for example 1268: 190308.6875\n","Prediction for example 1269: 150625.828125\n","Prediction for example 1270: 145794.78125\n","Prediction for example 1271: 141199.78125\n","Prediction for example 1272: 134200.828125\n","Prediction for example 1273: 162363.828125\n","Prediction for example 1274: 147827.5\n","Prediction for example 1275: 137000.3125\n","Prediction for example 1276: 138246.640625\n","Prediction for example 1277: 119492.46875\n","Prediction for example 1278: 135341.484375\n","Prediction for example 1279: 164451.125\n","Prediction for example 1280: 135597.859375\n","Prediction for example 1281: 137991.15625\n","Prediction for example 1282: 155703.5\n","Prediction for example 1283: 165470.4375\n","Prediction for example 1284: 169274.640625\n","Prediction for example 1285: 145084.03125\n","Prediction for example 1286: 133545.40625\n","Prediction for example 1287: 155830.46875\n","Prediction for example 1288: 135316.09375\n","Prediction for example 1289: 138383.359375\n","Prediction for example 1290: 125560.4609375\n","Prediction for example 1291: 140452.65625\n","Prediction for example 1292: 211253.75\n","Prediction for example 1293: 161556.71875\n","Prediction for example 1294: 235964.5\n","Prediction for example 1295: 138654.609375\n","Prediction for example 1296: 102494.796875\n","Prediction for example 1297: 75694.21875\n","Prediction for example 1298: 97578.0625\n","Prediction for example 1299: 169883.34375\n","Prediction for example 1300: 114327.96875\n","Prediction for example 1301: 142369.78125\n","Prediction for example 1302: 143480.09375\n","Prediction for example 1303: 208932.28125\n","Prediction for example 1304: 160062.5\n","Prediction for example 1305: 280920.90625\n","Prediction for example 1306: 125616.59375\n","Prediction for example 1307: 94908.3828125\n","Prediction for example 1308: 124005.9296875\n","Prediction for example 1309: 128559.734375\n","Prediction for example 1310: 141390.546875\n","Prediction for example 1311: 107717.2734375\n","Prediction for example 1312: 113082.6796875\n","Prediction for example 1313: 188423.5625\n","Prediction for example 1314: 136359.921875\n","Prediction for example 1315: 122619.3203125\n","Prediction for example 1316: 146024.25\n","Prediction for example 1317: 142673.5625\n","Prediction for example 1318: 119020.8125\n","Prediction for example 1319: 125158.6171875\n","Prediction for example 1320: 93781.75\n","Prediction for example 1321: 88774.453125\n","Prediction for example 1322: 94823.390625\n","Prediction for example 1323: 99771.3671875\n","Prediction for example 1324: 121404.296875\n","Prediction for example 1325: 142027.484375\n","Prediction for example 1326: 63325.328125\n","Prediction for example 1327: 123859.3046875\n","Prediction for example 1328: 79623.96875\n","Prediction for example 1329: 177374.65625\n","Prediction for example 1330: 93149.125\n","Prediction for example 1331: 96987.4453125\n","Prediction for example 1332: 68596.2890625\n","Prediction for example 1333: 147196.765625\n","Prediction for example 1334: 78093.296875\n","Prediction for example 1335: 119505.671875\n","Prediction for example 1336: 99392.3125\n","Prediction for example 1337: 204822.84375\n","Prediction for example 1338: 110173.28125\n","Prediction for example 1339: 104994.75\n","Prediction for example 1340: 78023.5078125\n","Prediction for example 1341: 96227.25\n","Prediction for example 1342: 123053.0\n","Prediction for example 1343: 164209.296875\n","Prediction for example 1344: 132626.125\n","Prediction for example 1345: 113829.3515625\n","Prediction for example 1346: 82726.875\n","Prediction for example 1347: 162681.3125\n","Prediction for example 1348: 146573.53125\n","Prediction for example 1349: 123885.671875\n","Prediction for example 1350: 119335.296875\n","Prediction for example 1351: 162125.671875\n","Prediction for example 1352: 171214.46875\n","Prediction for example 1353: 155031.75\n","Prediction for example 1354: 153713.15625\n","Prediction for example 1355: 94032.84375\n","Prediction for example 1356: 242293.078125\n","Prediction for example 1357: 181973.390625\n","Prediction for example 1358: 129639.5\n","Prediction for example 1359: 182088.703125\n","Prediction for example 1360: 143303.71875\n","Prediction for example 1361: 107374.875\n","Prediction for example 1362: 214971.40625\n","Prediction for example 1363: 321742.65625\n","Prediction for example 1364: 181323.796875\n","Prediction for example 1365: 161927.53125\n","Prediction for example 1366: 132322.25\n","Prediction for example 1367: 132418.5\n","Prediction for example 1368: 241321.4375\n","Prediction for example 1369: 218479.90625\n","Prediction for example 1370: 239380.34375\n","Prediction for example 1371: 179695.796875\n","Prediction for example 1372: 253953.96875\n","Prediction for example 1373: 275526.59375\n","Prediction for example 1374: 226633.65625\n","Prediction for example 1375: 216659.875\n","Prediction for example 1376: 195048.25\n","Prediction for example 1377: 157186.015625\n","Prediction for example 1378: 147836.75\n","Prediction for example 1379: 193476.625\n","Prediction for example 1380: 202826.40625\n","Prediction for example 1381: 205922.375\n","Prediction for example 1382: 233903.96875\n","Prediction for example 1383: 146228.84375\n","Prediction for example 1384: 179578.8125\n","Prediction for example 1385: 128830.875\n","Prediction for example 1386: 209928.265625\n","Prediction for example 1387: 208911.484375\n","Prediction for example 1388: 212585.53125\n","Prediction for example 1389: 205764.6875\n","Prediction for example 1390: 265674.6875\n","Prediction for example 1391: 221593.921875\n","Prediction for example 1392: 234431.578125\n","Prediction for example 1393: 235955.5\n","Prediction for example 1394: 143166.4375\n","Prediction for example 1395: 207098.15625\n","Prediction for example 1396: 213973.34375\n","Prediction for example 1397: 192405.875\n","Prediction for example 1398: 213001.4375\n","Prediction for example 1399: 105859.515625\n","Prediction for example 1400: 128470.2578125\n","Prediction for example 1401: 123543.875\n","Prediction for example 1402: 196294.40625\n","Prediction for example 1403: 128616.7421875\n","Prediction for example 1404: 240567.96875\n","Prediction for example 1405: 137548.15625\n","Prediction for example 1406: 152611.3125\n","Prediction for example 1407: 99254.265625\n","Prediction for example 1408: 85887.53125\n","Prediction for example 1409: 102601.109375\n","Prediction for example 1410: 128058.40625\n","Prediction for example 1411: 89234.453125\n","Prediction for example 1412: 55874.25\n","Prediction for example 1413: 109181.53125\n","Prediction for example 1414: 136884.78125\n","Prediction for example 1415: 116974.5390625\n","Prediction for example 1416: 176092.1875\n","Prediction for example 1417: 156452.9375\n","Prediction for example 1418: 153666.515625\n","Prediction for example 1419: 142751.421875\n","Prediction for example 1420: 106619.2734375\n","Prediction for example 1421: 156998.96875\n","Prediction for example 1422: 175664.03125\n","Prediction for example 1423: 191012.25\n","Prediction for example 1424: 217897.53125\n","Prediction for example 1425: 193297.5\n","Prediction for example 1426: 235906.25\n","Prediction for example 1427: 106594.96875\n","Prediction for example 1428: 145497.625\n","Prediction for example 1429: 62949.9140625\n","Prediction for example 1430: 87346.28125\n","Prediction for example 1431: 132902.4375\n","Prediction for example 1432: 57396.6953125\n","Prediction for example 1433: 99615.109375\n","Prediction for example 1434: 61101.0234375\n","Prediction for example 1435: 301396.90625\n","Prediction for example 1436: 263442.0\n","Prediction for example 1437: 200142.5\n","Prediction for example 1438: 152378.4375\n","Prediction for example 1439: 210412.0625\n","Prediction for example 1440: 154706.5\n","Prediction for example 1441: 232580.6875\n","Prediction for example 1442: 191911.75\n","Prediction for example 1443: 301858.90625\n","Prediction for example 1444: 326465.90625\n","Prediction for example 1445: 88806.796875\n","Prediction for example 1446: 186141.3125\n","Prediction for example 1447: 119185.40625\n","Prediction for example 1448: 128957.296875\n","Prediction for example 1449: 156728.8125\n","Prediction for example 1450: 83600.6796875\n","Prediction for example 1451: 89102.765625\n","Prediction for example 1452: 157176.09375\n","Prediction for example 1453: 83766.8671875\n","Prediction for example 1454: 79109.296875\n","Prediction for example 1455: 89206.53125\n","Prediction for example 1456: 89107.2109375\n","Prediction for example 1457: 181692.4375\n","Prediction for example 1458: 111777.28125\n","Prediction for example 1459: 229619.34375\n"]}],"source":["for i, pred in enumerate(predictions):\n","    print(f\"Prediction for example {i + 1}: {float(pred)}\")\n","\n","# Or if you want to save the predictions to a file, for example, a CSV file\n","import pandas as pd\n","\n","# Assuming 'predictions' and 'X_test' are both lists or arrays\n","\n","output_df = pd.DataFrame({'Id':[x for x in range(1461, 2920)], 'SalePrice': predictions})\n","output_df.to_csv('predictions.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"s3ps-8K_vhPj"},"source":["## **5. What you need to do in this assignment**\n","\n","In this assignment, you need to come up with the best neural network that can predict the house prices. This includes:\n","- The choice of the network: convolutional network, MLP, a combination, ...\n","- Setting the hyperparameters including the number of layers and the number of neurons in each layer.\n","- Train it on the Kaggle train set and test it on the Kaggle test set.\n","\n","You may want to consider the following **hints**:\n","1. Can you improve your model by minimizing the log-price directly? What happens if you try to predict the log price rather than the price?\n","1. Is it always a good idea to replace missing values by their mean? Hint - can you construct a situation where the values are not missing at random?\n","1. Find a better representation to deal with missing values. Hint - What happens if you add an indicator variable?\n","1. Improve the score on Kaggle by tuning the hyperparameters through k-fold cross validation.\n","1. Improve the score by improving the model (layers, regularization, dropout).\n","1. What happens if we do not standardize the continuous numerical features like we have done in this section?\n","\n","**5。您在此作業中需要做什麼**\n","\n","在這項任務中，您需要建立一個能夠預測房價的最佳神經網路。這包括\n","- 選擇網路：卷積網路、MLP、組合網路...\n","- 設定超參數，包括層數和每層的神經元數。\n","- 在 Kaggle 訓練集上進行訓練，並在 Kaggle 測試集上進行測試。\n","\n","您可能需要考慮以下提示：\n","1. 能否透過直接最小化對數價格來改進模型？如果嘗試預測價格對數而不是價格，會發生什麼情況？\n","1. 用平均值替換缺失值是否總是個好主意？提示--你能建立一種數值不是隨機缺失的情況嗎？\n","1. 找到更好的表示方法來處理缺失值。提示--如果新增一個指標變數會怎樣？\n","1. 透過 k 倍交叉驗證調整超參數，提高在 Kaggle 上的得分。\n","1. 透過改進模型（層、正規化、剔除）來提高得分。\n","1. 如果我們不對連續數值特徵進行標準化處理，會發生什麼情況？\n","\n","**What to submit**\n","- The colab notebook that implements the solution. You can update directly this notebook, as as many sections as you want. When I run your notebook, it  should load the Kaggle data. Make sure you provide enough explanation of your work.\n","- A screenshot of your score on Kaggle.\n","\n","**需要提交什麼**\n","- 實作該解決方案的 Colab 筆記本。\n","- 您可以直接更新此筆記本，可以更新任多個部分。\n","- 當我運行你的筆記本時，它應該加載 Kaggle 資料。確保你對你的工作提供了足夠的解釋。\n","- 您在 Kaggle 上的得分的螢幕截圖。"]},{"attachments":{"image.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABcAAAADFCAYAAABzRlH2AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAAGtDSURBVHhe7d0LXFRl/j/wDxcdxcLVFssN1gqqXeiyYLnBrxL+plKp83MN2lajTcVWs7z9apG2Jeq3yNZPsY1sk3CLsE3phriGmguVwa4lVApbyXSDyqQ0pkRGgfk/z5lzhjPDDMzAgAqft6/jnHPmzLk+58xzvjzzPX5WAf3sJCySiIiIiIiIiIiIiHzIz89P7Tt1+SwAzqA2EREREREREREREblysoLlvQqAu/uow/jT4K8AREREREREREREROQDutiwu6B3fwbDexQAd/6IMixWWlvtHsySiIiIiIiIiIiIiAYQLdCtRIut1k6B7/4IhHsdANdPLvvlSmrjnGfl5ayJiIiIiIiIiIiI6DTnLtCtxZL17ztP62seB8D1k2krqY2Tr/Z+5X9BNz0RERERERERERERDSJa0Fv53xbo1oLdWmxZH/zW9/uSRwFw/STO/UpnG3AYp+noIyIiIiIiIiIiIqKBTB/G1ge9RY/ynsM4wV2/r3gVAFf+VwPcSqcbbm9vV4at8lUMyw3SPuegDzaCiIiIiIiIiIiIiE4CFzFgJZAtxivBbn9/JfDtL1/leDlOnUY/LCnDPtZtAFx7W/+qdLK/vV0JfCvBbzGura0NluPHESA2xjad8hEiIiIiIiIiIiIiGiRscW0/+AcEwF+8BohXZdjf3xYIV4Pi9iC42q9/9ZUuA+DaW/pXrV8GvdvU4Hdrayv27NmD7dt34tVXS9Hc3KxMI+3Y+RouDB+nDhERERERERERERHRYKA1mD4hOvj54VDjEYSee7bSgFoGwqW+DoLbltINbYFKAFy8Kq2+Rb98PXGiFc8/vwn33fdHvPjiSw7BbyIiIiIiIiIiIiIanGTL76DhwzFy5Eh7jFkfW5axZq3BtS+D3npuA+DOrb614LeS9kT0t7e14cSJE/jXv/+F/Py/obGxUZmeiIiIiIiIiIiIiEgv+MwzlVcZU5axZRljVp4nKcbpY9CS9uoL3bYA17f+Fv8pr3IFZfqT1hOteG3Hawx+ExEREREREREREVG3ZExZSa3d1uYQc5b6ohV4lwFwuWB7pxuW0fm21lYcO9aMV0u32yYmIiIiIiIiIiIiIurCiRPHldiy0gJc68R4e7/ofMllANy+EBetv5WuXX0Apuhnzm8iIiIiIiIiIiIi8sTxEyeU2LKS/kSNN2uxZ4U+Ju0DXbYAl4vSVkLrlNbfMgAu06C0ttomJCIiIiIiIiIiIiLqhowpK7FltYG1c/zZ10lQunwIpuwUfn4dw7ITw+3tMhDumyg8EREREREREREREQ18MqYsY8tKS2815qzEnXUtv5VhH+n2IZiSPuquBcHla7u1XR1LRERERERERERERNQ1GVPWx5g1vm75rfEoAK6tiP5VrpB+BYmIiIiIiIiIiIiIuuIcW3Z+9bVuA+D2FVD+79BXK0REREREREREREREA5dzbFkb6ouYc6cAuH0has4VZwx8ExERDU4yR9uJ1lYca7Gg+VgLvj/aDPMPR/HUF2/g5+/+EbH7s3FD7aNYWFeIv31dgZqjX4hKDOsNRERERERE5JrbWLMuH3hveZQCxRmD4ERERIOD/M4/fuIEjh47hh+am5XgtwyCt7a12esDMsj9Q7sFDcePYF/Ll9hq3oc/flGMxA//givfz8IjDdtx6LhZmZaIiIiIiIhI6q8Yc48C4HYMhBMREQ1YluPH8UPzMbRYjqOtzfHB1/Jv8f7+/ggMDMDYoT9CxNAxOCtghKhYOP6C7OtWM/5y6J+4cn8W7jAV4svj36nvEBERERER0aDUzzFlP6tTqL2jNZcygPZ221M528Sr7G9rbVVafllaLGgyN+H666cp07uzY+druDB8nDpEREREp7rW1ja0WCxod6qUyGB3YICtk8FvV1qtbdjz/Sd463sT3jB/hHePNajv2AzzH4K7QhKw6CcJCPTr3d/hiYiIiIiI6PRywPQZ2lpbMDJ4JAzDDBgSGIgA0cl7zADR+fn52e43xavWvEqO6w3eeRIREZFdy/HjaG5pcQh+G4YOxRkjghA0bBiGDhliD363tbXhxIkT9k7+oTzQLwBxwRG459ypKPn5XXgj8h4sGhOvtA6XWtpP4JGvd2DGf3LReOJ7ZRwRERERERFRX2EAnIiIiJRfe8kHWx4/fkIdA+Uv8WeOCIJh6BD4+/mJ947DbDbjm2++wVdffYVDhw4p/Vr39ddf4+DBgzh8+DCOHj2qBMjPH/ZjrAy9HhWXpmHuj/9LCZBL+1q+wLQPHsOHzQeVYSIiIiIiIqK+wAA4ERHRIKcEv1ssyoMtJfnzsuHDDEon+48dO6YEuL/99lslsC1be7sj52WxWJRAuQyQHzlyRJk+yH8oMn86A69efBd+MmSkMu2XJ5ow68BfUXesURkmIiIiIiIi8jUGwImIiAa5Yy0WpbW2JNObjBg+TGn9LQPXsjX3d99912XQuystLS1K8Pz7723pTn4WNBb/+NkSXDLsJ8pwU9sxzKl7Co3HmQ6FiIiIiIiIfI8BcCIiokFM5vzWWn5rwW/5qrX6lq25feGHH35QWpDLQPuPh4zASz9bhPFBP1Xe++LEd0oQvLn9uDJMRERERERE5CsMgBMREQ1Sra1t9pzfMtVJkJryRKY5ka2+uyPTncgHX8qgtnztjswhLoPgra2tGO4/BE9fOBc/HTpaea+25SAy60uUfiIiIiIiIiJfYQCciIhokGrRte4eZhhqb/kt83d3RQa+ZSBbPghTPvSysbERn3/+ORoaGpR+GeB2RwbLZV5wGTD/UcBw5F+QgkA/W3Xk79++jQ+bv1L6iYiIiIiIiHyBAXAiIqJByHL8ONqtVqVf5vvWcn531/JbBrc//vhjBAQEKN2PfvQjGAwGJXAu84V/8skn2L9/v5L72x05j6amJqVf5gS/MyRe6beKfys/f1npJyIiIiIiIvIFBsCJiIgGGdmC+/iJjlbasvW3pD2o0h35ufr6evz0pz/Feeedh4svvhhnnHGGEuyWrz/5yU8wduxYpYW3bBHeFfkZmWpFWnruZEQMDVH6327+DHu//1TpJyIiIiIiIuotBsB7wlyLTfenYJrRiLxqdZyTxsp8LE2Ow88uuBDnXxKHWfcXoaaLX5Q37l6HhZPH43xPpm+sRsH9yZhwiZhWTH/55DuRu7tRfZOAauSJYyOPT6cuZTlWv1CNRtvz3lxqfKcIWQuN9v17/vhEzF6RjzJT1ykBXGnclYlZ2rJXlMDlUWquRJZ9HZejuEGO7GIbdJ1W/hq3pavjUpD7jqsH1jWiNF37XDpKPSkuZhPK1i/HbK1cXnAJJky7E1ku959+ffNRpY510LgDado06TvUfdH95zq2zYi0bSznRL5worVVCWZLhqFDlbzfsgW3uwdeymll9+GHHyopT4aKz8jW3p999hnKyspQWVmJ5uZmpQW5THEyZswYvP322/ZluCMfjCmnkSlQlo+9Th0LvPity6sIERERCZaPSsT9SiIuV+ro4zF54TpUHFTf7M7BSuSKex239wRe3WtaUFeS7XgfK+6bKjytsjeUYGkX9wFy/vW7vLhPpgFBiY3IcmG/Z3TSbEJx9p2YPN6TMkrd6u6aIHR7TIhOAwyAe6lxdw5mxRmRtrESNftqXQZS619IxTWzs1E2bAYy8wqw9u6rYHk5HdNmZaOqWZ1IR5k+ZR3qI5OQ+egaZP5anT7hThQ7V2QaijA3IRkZ24bh+pVrsDEvCymRDchNScC0x6pFFYGkRnFs5PHp1O0uQe69okI3QexbJdCs09aA4sXjMSE5HXnbxbHVjtUREypezsbcyXGYllMJcxfBc2ch8XNgDFSX/fIa5LkITtc8k4k8df0sifNhDLWNd7sNus5e/pob1XGVWL0yx2U5azqofa4RTd1sg+WdHEyLS8Tc7BJU2AP/FjTW7kCem/3Xsb5uvhLbmnBIm+agLfWB1O3n7NtWi0MutouIvCcD4JohQwKVV601tisyqP3FF18ogW8ZLH/++efx7rvvKjm/JdniW+b+lg+5DAwMVNKbyEC5DJJ3ReYBl4Fz6frRl2JUQJDSX/zde2izdv9QTSIiosHG8k42piUuR0F9JBY/WoANDydhzJ4czL4uFZuc72/02syoeSEdk+NSsFrc67i8J/DqXtOCqmwjJi/JR8XYGUh/eA2y756KYeXZmJ3QzbooGlGcuRLFbu8DbPO/NlXeJ89BtryvXinmLxvHuLpPptOfbGh4byImpOSgVJYL3T2jXXM1soyJWFrYgKg7bGX05rH/wmpRRue+0G2hIz1PrgmeHBOi0wQD4B4zoyx9vDjxCzHsjgLsKZinjndyZAdWP1COMXPlNGm4eVIsjAvWYGtpFhK+ysfSvzhVHGrXYe69lYh7uBRbH01DyvTpSPmDOr1lB37/ZKVuegsq8jNRNnoeNr5ZgMzZ0xE3KQkrHi3G1oxI1OSswyusCDiJhvGuRVisdHMQN0odLY7T0iWFqFcH5b4te0B8kW7Tgr3BiJk5T/lcytRwMSRZUPNYCm4vNClDHgkIR8pDyxChDDQgb1UR6vRfKkdKkPuIOr+gOUi/LdLW70C/DY5djC1jgCNTPn7vXM68YSrEb5LXoUYNNhvOiUeKXN7c6Yg5x2AbKfff7a7/oENEp7b2diva2mzB5cDAAPj7+SmBa9l625lsnS0D2x999JEyzaWXXoqrrrpKaSkuW4wPGzYMkZGRmDx5Mi666CJccMEFOOussxAUFKSkQtm9e3e3rcDlfKRAvwDcffYkLPlxAv4WlgKruo5ERESkaqtF3sp81MVnYWfxGqROj0XCTWni3nAzUseWI+2PRW5aZ5qQZ4zDtAdqMP5hce+YZXv2hiMv7zX3iXvb9SZELNgs7nszkHrTdNy8IKNjXRzutTozb1uF3++KhHGmq/sf4bMi/EnMPyFL3icvs91Xz5bzz0OKvE9+2s1Psen09FE+psUZkVEbg+ytxch2VUSFmmdWIs8Uj+zSYqxdYCuj6QUVeHFBKMruzcQmxkM85ME1wcNjQnS6YADcYxaYMVWc+GXYeFcsQoapo53Ub3sKxc1TsOLOWKihQptQUXm4IxT167egQheZrNqyDnXRaci8SW32qxHTpz+8CKmjLTikjgJqUVlkgWHmVMTZGsnZRYgLfxzKYeIfPZ3EIGXZMqxQOrVCFq6+VZ2P4lpbr2zxnLFRPTCh07H2jb14cXWa8rnMJ0qxpzQDCeo+r8rM9u6LNXIe0merpaE6G7nbO1pUVz25BqXqUMIDi+zLcKTfBscucZw6iZO69cuxurInIfBGbPpTpv0niBELCrBHVDIz5fL+sAYvvlmGDbPVsmrKx59eZoEjOt20tXf8FS4wIEB5dfXASpkP/IMPPlBaccv83rJV99dff63k9h4xYoTS6js4OFgJeMtc4Oeee67SQlyOkw/GHDdunDKNTJPSFRl414Lvt4fEYsHoa/AzwzloZQCciIjIgaWyCLmmUHEfkIQw21e4TVA0Ft89BSgvxKsu2+qYYRm9CBtfK0b2TZEYY/vxlxPv7jWrXluH+qA54j4h2vG+V6zLiow5MOjutTo5sgN/vLcEYfc8gBWxY9SRjhqrXhf3JPEwTnK6Tw6Kx9Rfi33xUR1TMQwkZgtG3lGAN4uzcHOkq1ZegqUSmx83IUzcm97sUCwMiLnjTiSKMvrsdi8aqw1qHlwTPDomDajaXYm6I6K/rRE1u8tRtqsSVfr0sWIal+OJ+hkD4B4LgfEheeLb2gK71oiq8mogNh4xWktjnairZ4hLcxGq7BWBapQVWhCXFI8wdYxexA3LsOIu/XuiMrH/AD4QlYxOLBbwxygeEBUy46+0b0txsf5QVpss2PsPrYWCASkPrrKnIdEYLpqD9Hu0/e7tF6sBCXdmIEHpt6A4pxA1Mv50cAty16u1yHBxrGe6+VLpkQbkpWajzNsW2p9tx+ZytT90GdbeE4tgfeU6IERsyyJ1W0TFV5R3VjyJTi+trZ0D4LJ1t57Mzf3JJ5/g5z//OSZMmIAAMZ1s4S0D4REREbjyyiuV4LbBYFDek68yKC4D4JJMkyJbh4eEhODf//53t63AteXLecnPSq1tzr/BJCIiGtxq3i4SdxNTEeOi0XTwVYmijl6L8vdd1c6jsfhvixB3jjrokjf3mo04sM8CTIjEhQ7RbxtDRCTi7PdaziwoW7McxWPFvcaCSLhpV4aQmXn45OM8GDvdIlnk6tBAE73I1tBQf+/prPZf2CTubxOvcHECjLoKiZPEObK7lvenHvHgmuDJMTlSidyUFGzeVYKFE+IwLSUVc1NTMGtynJKSxixTq45PcBg/LYepe+nkYADcG12d+IoG1FWKl0sjXAa0MTZUVAQsqDGpl+SDdUqaiaiIUJhri5Axq6cP97CgqigfNUFzEH+pOorcsjR1fCWGjZJ/0DChapd2CU5CfKyLWpwQcfUMRKn9Xn+xnpOEFfeoTc9NOVj9sgllj2eiTBlhQMp98xDltnyJCucRM8zOndn110bUpHhbypXmQixcVe7Vl4u5psre+jvslnjX6yS2ZcPHB0SFVHR509E5bO9mfQ+Lyqo6BRGdPFowWsaZ/f1t1QDn9Ccyf7dMZyID3l999ZUSDK+oqMCOHTuwfft2vPnmm2htbUVdXZ3SwrumpkZpGS4/d+jQIWV+Mih+9tlnK+NksLwrcl4af39bAFzmByciIiJNI+prRW06PhwXuqqjh4Qq9wBlH7r5hWa397LuuLrXDEHYBeKlrlH3a2WdBhMqxEvFp53XxVIpf3kbisUPd3X/04WDW/DsRiBmUqyL+xA6bXlQFho/rRWlMR7h56sjHIgyKW+3d9V0mXqHdLrb516cnwWrihD11F5bjOBAGbIniWuRzB2+sh6p/1BjBwcq8MTsENQ8xtS9dHIwAO5jlq5a3J4ToeaCVh21pTcxv5aNWdOyURe5DGsL8pA9r+uHZjqT6Tt+v74RCQ8tQpzr2C1pPitCbqEWho0WX55yh1lgttfNDBjpbh+GhmK82os2LV1AIzbNtT192rmb+4JjiDzqtjSkqD8nLHvAiIVqyhXDpFVYHN/VgSvE3PHjcblzt3SLyyD8mKlp+PNdtmC7ZeNi/N6e17x7lubv1T4gIqSnVUo36zstU6kIE9HJ1aYGlv38bFUA+YBLZzLHd319PV566SUUFBTgm2++UcZ/+eWXeOONN5Rgt2y1LR+C+emnnyrvy/Qo3377rRL8PvPMM5Xg+scff4xRo0bZP++Ofh20oDwRERE5aunyx1GhCO+DHL3u7jUvuTIJhoZ12LzL6V6jrQGbnixUGr50aqltqUbuH/OBuRlYHN2DG1c57/szURa+CPc5pxClga+bHweGRTBJ9ckScac8p9VsCQGhuDktDVHiKhB3x0oYtbStASFI/O08MZ6pe+nk4F3myXSkATXiZdP6ciRsrsDGh+bAeHU8bl62Blv/YXto5m8e0T8EszPLR4VY+FtRiVjwLJ7waQqNgaIIvzcaMU12k8fj/IR0e1oQw+xFuNlNDm2XAg3oQTWtQ1A8Fj8Ub5tHs9YaOhrpaa5aUfdGMGLuXoMVSgzcguJ7V/Ap6URkp7UA76qltczpff7552PSpElYsGABZsyYgV/84hf46U9/iuHDh8NsNuOLL75AVVWVkuKktrYW3333nZIv3GQy4cCBA0oOcTntkCFDOqVYcaZfBz/xj4iIiE6+ru41g69bhMxJQEFqAmY/VoIqUwPqdhch65ZEZCDWnjJRr2bDSuR+NQeZy52el+WJtkaU/e98pFXGIvtvyxDTqxszIvKluMu0B62pRhggs/tHnOcU6VDHE50MDID3p0ZRKVB7FUHBtlQps9Ow4gqnb/BxSVg8xwDLM69jr7u/dDYUYeGvMlERm4UN9zg9fIRUZtTtq0WN7OwPXDAg6tePY+cDajBajtE96MVty4qvbH+wUARo2epC8N9r9+K9vZ27tdM7h7VDZqQhPVodEMIWrMTNTt8Vnc3BBhfzf2/tDPeB84BILP5rBmJkf3M5fn9/Pkxd/SVFo/uZk6XVkw+44mZ9t2YgTp2CiE6+rgLNgYGBSg5vmZNbtsiWncznfc011yAlJQWzZs3CFVdcgZEjRyrBbdnCe/fu3SgvL1e6PXv2KMHwY8eOKXnBnVOsEBERka81oGGP2usL3d1rylaefy3F2tnh2J+zHLMmJ2Byipg+IgNbH12k/PI5IULXSttUiD8+0gDjw8vcPPi/KxZUPXIr5j4DpD6d6/QARCKb+nqZj5aIyDUGwH1K/dmZqcF1fujGeiUAHnGuGrYcHWKrGFwe5TJ4HRYRK/43od7VzBrLkXF7OirOW4TnHnV6CjjpRMN41yIsVroMrC3YjJ3/3outWVN0+ywcUZPUXhRie6XrwK+5utyewiMsJtwefDYEByN4lIvO1UENCMf1t3T8NCvxOk/+cGHASFfzd7kAnfA5WJtlW5ZlVz7yPKgPhFwcbc9fX1FU7jp/mlmUPa1VffoOF2XdzfqO7r4Fvas/PrT0OBBPRF3p7sGUrsjW3Oeccw5iYmJw44034rbbbsO4ceOU9CfagzLlwzBlv2wFLlOkNDU1ddsCXE99BiYRERE5CMGFF4mXPQ1u6uiNqG8G4pxbXPaEp/eaAaEwPrQZ7x3Yb2vw8p/94j4rCRFHbQ2/7Pe9Mm3knzJRFTIDMcE1qNhd2dF9IJOC1mOvMmxC5+SNFtQ9sxi/Wd+IxNVPId254RgNGiHnyZZjlWj4yjbsyIxDn4r7xtiO+3QiIj0GwH0qBJdcGQrsKkWFi+d91VfLgOIURP1MHXFOFGLE5BV1rhMgHfpKVhtCMWaUbdiuuRpZv0lFAebhuc3LEOP1X9AHkxikLFuGFUonU8xEIyLEudIUjISkOWpw1oKCzJzOuddFJXD1IzvUgXAkT3Tx5OlTUFhSBrIneVFJjJyB+dFqf3U2Ml5wLpsWVK3LRoHaqt4cFtrLCkaowx8fNm9zCqe3mVBWpEXuDYgKZ3WGqLf81AhzuxoAl8Fqb8kW4fJzo0ePRkREBC6//HJce+21SsoT2TI8KipKeYimbEUu84HrH3Lpij7vd08C80RERINBxGXTxb3gFlTsU0foWN6vRKmoW8dG9bJ5tIf3muaPdAHrAIOtwYt622H+VynKMB0xUbZh2TrdVC5eGouQkZKC2bpu6YZa8cYOZCnDRY6/mBbqX1iMaZmViHu4GE/MZNPvQe2iGBhlis/dssw4sdSgcpe4/70qyt6gi4hIjwFwH4u6YR5ixBd47vNOF+XmSjz7ZDUM041IsAe0I5FwSzgsheuwyTnO2FCE1U82wHBTLMbr45fywR9LbkWeTDPxXBqD3z5iiJ2HTC1QbMrHrGtSkPVCudIqoXj9ckxLEJVA9RhFLFiF1H6Nf2stIjp3dS7+0OJA/jQxI8OLnxmK6VfOUx/WalGe3DxtSb6oZIjl7RJlckkyfrPepLyLoOlI/3Vvd0QIEmZOt//xoXhJImZnF6FMLq8kH0uNRmRUK28C4YuQeJnaT0Q9puX+hj0XuL89KN4TY8eOVYLhR44cUYLX8gGasuX3WWedpbQIl7nBZT7wrsiUK5r2dgbAiYiIXAmelISUoAbkPrXDsaW0uEd8ZUMhLOFJSNBXzw+aur9f0PPmXrOuCLNTjFhd7vRrzeZq5P5lB8IW3IoE9Zl48le5K5zTI6rd1gz5q2ctheIyWwpHlbkyG3PvrURcRjGe4EMvKXgikm8zoP7Jp1DqVK7rX96AguZwJE86PRqqEVH/YwDc18Yl4b4F4ah7JBmzckpQIVvK7ipEWnIq8g7HI/OeKbDXA4So21YhdWw50oy2gKtsVVtRsg6zjekog5j+bv30jSj+XSLSdgXD+NuJMHzoGAxVuo86/2iMPCADxY8+i8WRahD8SCXy7k21tUrILkGN2iI8+IY1JyHfutYionO3WY1Fdyk0CX/WHr7pAcMVadiwWit3FtSUZGOpXF5qOnJLasUYISgSi59ehUTnXyf0QPANq/CcOGdszKhYn465cnlLslFcq1aog+KR/bdFiGKqH6Je89e1ANcePinzdPfU2WefrcynubkZR48eVVKetLS04Msvv1QelCkD4zIo3hWZWkWjrZM9UE9EREQ2hljMfyAeKLkTs+4tRNk74l7znRLk3m5EWnk4UlfN66gvHyzC3LhETL4mHaUe3SJ6d68ZPHU5suVDMBcnI21jOarEfWyVbMCSLAPo87D2bsd7JoNzekS1GzNCTqWlUOz4hKU6B7Nm56PuinlIDj/kokFQNeq7rl7QgGNA3LwMJKAEC5PTUbCrGjX7qlH8WAqmpZefhIZqRHQ6YQDc5wyIuWczXkyLRX3+csyWeZJTM/GqIQnZL7l4YEdQNNJfLEb2DS3YdG+qkld59pIc7I9aho2v5TlN34CaXfJbXlRO7rcFZzt1L3kSESWXxLFYUVyGjWnTEeMU2DWcE4/UR4vx5qPTT8t86yEzV2PtdM/D9mEzH8ebW9cgdVKIU+A8GDEz00TZLO784NYeE+dMWin2FCxDwjnO8xTLm52FrRXO5wIR9VRAQEdr6zY12Gww9Px8lp+VqU9kypPg4GAlmC5bdJvNZiX1iQxudzd/7X25Pr1JzUJERDTQhd2Uh52PzkHwrkzMTRb3msnLkVsXgxUbn3XMjx0UYkulOVa8evQ17+W9ptKASNzHzjTgVTH9LHEfO0vcx1aMXYSNPvilstlUa0uH8s46LHS1LinrXKYdpQEuNAkb/rEGKcHbkZGajGnGZCx9sg7jlxXguX5vqEZEpxM/q1OyTW1Q+V9tHSbHKTelomsTN7MnRGdpsaDJ3ITrr5+mTO/Ojp2v4cLwcerQINNmgdksKhGGYAR7UgHQpg8S0/PKffI1myEPx6A/Hlq5FNUJfauMPmMR+11tcS8fMGpgDIzIp+R3+Q/Nx5T+IYGBGD7MgLa2Nhw6JB9C1TM7d+5UXmW+74MHDyotwH/0ox/h22+/VYLhMk/4+PHjlWmcyTzho0bZ/upoOX5CdLYHZsr1kutHRERErlmOmGGR+bfd3axoD5jv6/o072PpZFDv13nPSHT6OWD6DG2tLRgZPBIG9b4vQHTKs6bUFJ3Kc6LEq/a74N6k7ZTYArwvaQ8D8fSv304PD6GTTFbgeDw6ymV/BL8l+QcjZXmsyBD1BVmR0NKLyD9oyz9yy9bWMhDdU3Ie5557rhL8lmSLbtnye8yYMTh27FiXrbmDgjq+JFvbtDt1x7zgRERE1JmSVqSrmxX59dsf9Wnex9LJoN6v856RiDzBADgREdEgo29ZLYPg0hlnnKG8eku2KB8xYoQS6Ja5vmUecBnUlkHww4cPK9PI91yR02jpT1pb25SW6JJcP2YAJyIiIiIiIl9gAJyIiGiQGap76KRMOyLJFts9CYLLh1+eddZZGDduHMLDw5XW3jIFSn19vfIqg+HuHrIpU6Zojrfa1kNi6hMiIiIiIiLyFQbAiYiIBhmZP23oEFuQWaYv0YLgMiDtLljtzjfffKO04pbzPH78uPIgTJm+ZOTIkUoecJlaxVUKFDmdDLpLshW6bAEuBYppAwP5W1YiIiIiIiLyDQbAiYiIBiF9oFs+eFJLPyKD1l3l7NZTgucWC06cOKGkQZH5xWXgWwbEZWC8Vc0xrs1bI1uFy+klfQBeGjq0o3U6ERERERERUW8xAE5ERDQI+fv5YZihIwh+zHIcViuU4Pfo0aM9egilDHDLVtwyBYoMhMt++fnhw4cr42Qw/NChQzCbzeonbMFvOV7TIpYr84hLQ4YEKi3AiYiIiIiIiHyFAXAiIqJBSuYC1/JtyyD0sZYWpV8Gv2UAW6Yv6coPP/ygtPa+4IILlAdeavm+ZXoTGRCXrcnlAzC1FuVyvHPwW3sIZ4C/P4arD8QkIiIiIiIi8hUGwImIiAYx2QpcBp+l1rY2NLe0wCr6ZTqTUaNGKUFr2e9MBsxlahM5jQyUn3322Rg7dix++tOf4osvvkBISAjeffddZfjiiy/Gj3/8Y3vaE0kGv4+fsKU+kfnDhzH4TURERERERH2AAXAiIqJBTAafhw8bprxK8mGUzceOoa3NlpZEBq3HjBnj8NBKSaY/+fbbb5UW30eOHFFae8vpZDBcvsoW5KGhobjuuuvwi1/8wv5ZmfP7WIvFHvyWhg8zICCAVRIiIiIiIiLyPd5tEhERDXL+/n4YMXy4vaW3DH4fPXbMoYW2DITLVtyyk8Fwmedb5gqXwzJligxwy+lk0PuXv/wlrrnmGtx+++248MILlXlIMt3J0WMt9rQncvqg4cOY95uIiIiIiIj6DAPgREREpAbBhyEwsCMYLdOU/NB8zB6wlmSgW2sVftlllymBcJnXWwbFZUBcDsuguGwZrlFalbe0KC2/tQdeyrQrQcMY/CYiIiIiIqK+xQA4ERERKZQW2cOGwaALXtsejmnBD0eb0XL8uJL32xNt4nMWMb1s8S2D3zIIrhkyJBAjgoYz7QkRERERERH1Od55EhERkQPD0CE4I2g4huhag7dbrTh+/IQS0P7+aLOSIuWYxaK0EpedDHbL4WbxvgyWH20+JsadcAiYy9beMuXJcD7wkoiIiIiIiPoJA+BERETUicwHLh+OKVtqD1Xze2vkgyxlnvATJ1qVPOGyk8FuOdza1qYEy/WGBAYqLcuZ75uIiIiIiIj6GwPgRERE5JbM1T3MMBRnjghC0DCDEgzXHpbpjswnLtOcDBfTn3nGCOVVn1uciIiIiIiIqL8wAE5EREQeCQwMVILhMj1K8Bkj3HZnBAUpaU5ky++OduNERERERERE/Y8BcCIiIiIiIiIiIiIakBgAJyIiIiIiIiIiIqIBiQFwIiIiIiIiIiIiIhqQGAAnIiIiIiIiIiIiogGJAXAiIiIiIiIiIiIiGpAYACciIiIiIiIiIiKiAYkBcCIiIiIiIiIiIiIakBgAJyIiIiIiIiIiIqIBiQFwIiIiIiIiIiIiIhqQGAAnIiIiIiIiIiIiogHJzyqo/QptUPlf9Le3tyvj2sSr7G9rbcUJ0VlaLGgyN+H666cp07vzyccH1D7fMv9wVO0jIiIiIiIiIiIiIl8LPmOE2uc7H3z4IUYGj4RhmAFDAgMRIDp/f38EiM7Pz0/pFz3wU6eX43rjtA2AExEREREREREREdHppb8D4EyBQkREREREREREREQDEgPgRERERERERERERDQgMQBORERERERERERERAMSA+BERERERERERERENCAxAE5EREREREREREREAxID4EREREREREREREQ0IDEATkREREREREREREQDEgPgRERERERERERERDQgMQBORERERERERERERAMSA+BERERERERERERENCAxAE5EREREREREREREAxID4EREREREREREREQ0IDEATkREREREREREREQDEgPgRERERERERERERDQgMQBORERERERERERERAMSA+BERERERERERERENCAxAE5EREREREREREREAxID4EREREREREREREQ0IDEATkREREREREREREQDEgPgRERERERERERERDQgMQBORERERERERERERAMSA+BERERERERERERENCAxAE5EREREREREREREAxID4EREREREREREREQ0IDEATkREREREREREREQDkp9VUPsV2qDyv+hvb29XxrWJV9nf1tqKE6KztFjQZG7C9ddPU6Z355OPD6h9NDg1oizzTqyuisLipzOQOEod7cS8PRO/WVeD8Xc/jsxJIerYk6w6H9Me2KIO6IwMR8KMW5EyMxohAeq4U0YjStPnI7dGHXThkjueQvYNp8g+dqCt+ww8WDwPMepYIiIiIiIiIiIaOD748EOMDB4JwzADhgQGIkB0/v7+CBCdn5+f0i964KdOL8f1BluAUx8LQcKcGTDsK0TW45WwqGMdNJdj9YpC1JyVhMWnSvBbamtEzb5aGCLikRCvdTEIO/wv5N2bjGuM61DlcoN8q3FbOqYZ01HaqI7oRtPBWtQcGYPx9nV27CLHGNQpvVWNPKMR09ZXq8O+FgxDgFj3fR5uKBERERERERERUTcYAKe+Fz4HD94TjvoNmcirVcfZWVD1ZDYKmqOReV8STsV2yeNvWYYVy7QuA09srcB7m+chrDYHuSX9EKxtloH4RjS1qcOeiJiCxfZ1duxSrghWJ/Je475a1BxWB3zOgJEj1F4iIiIiIiIiIiIfYACc+kXU3FVYHG7C6vsLUacP5JqK8KfHTIi45wGkhKvjVJaGWlTsLkfZrkpU7GuAxUUA2PyReK9avKcO21kaULW7EnVH1GFBP63ZVIlN63NQ8I7Z9qaXDFfMwfxYoKyuQR3jSL/uVaYultFmRt07lWK6cjF9tcP6irVEndiGig8Oif5DML0t+p22qbf0+0RZZ7EeZWIZNQ2Oe9TSUC2WXYV6OdBQpayHfr+73bdHTMq0NS7/TmBBfbX4XKVJbCkREREREREREZHvMQBO/cMQjcUPzkNYdTaytmjR0EZs+lMmqsIX4c9zI9VxQlsjyv7XiMuvNWJ2SirmpqZgtjEBlxuzUeYUSK17Sbz3WGXnAOqRSuSmpGCzSR0WbNNuxys5RkyYnIK07HUo/9Qx0Os5MxqVaLATF+s+a/J4/GxaDiqcVtKyLx+zJ4zH5ORU/D4nGwtTkjF5fCLStmlBdRM2i21YukE2m69F3hKx/k7b1Fva/tv7QqptnVPFOotlTLt2POa+0BHcN/9rnVh2NkrlwLZsZT30+93tvg06hO2/E/sg30XaFPPrWD1LTPuOBT1vk05EREREREREROQeA+DUbwyxy5A5Gyi7fx3KmgHztlXIKDcg5b5FiLGnpbag6i+3Yu4GC27OK8MHBw4oD1L94I083GzJx9zf5PQu7/aeHGTVJuHFf+9X5rthZg+SrrSZUfNMDnIbwrF4qi5wL9Ssd7HuO9fgvy3rMHtpEezx+7Za5N2Zjb0xWdj5n/3Ys7UU7x3Yi40LgE33rkGp0so7Guni83sejhf98ciusM0v/Qr5ng/tycTC7RPx4ru2+X/ybjEyJ4njdG8mNh20TRJyU554bzNS5cCCzbbpNjilrHG1bw2xMM4xwFK4BRVOx828uxTFiMT8Gxz3IRERERERERERka8wAE79yICEO1chAYXIWJWN1dklwPQ1WBGveyjjkR3Ie8yEmIzHkTkpFIYA22hDaDwy/5qBGNM65O3oRcKM5hlY++gcRIWoy1Tn35VNK42YJh/+qHXXxGHallD8eetmrIjWr3sJch8xISHrKcd1D5+OzIw5MJSvwyYtB3pjDaoagLip8YjQZhEQjLhlm7GnYhUSR6njeqpuB3JzcrC6U7fDlsZEr3kiHnxY7BOtGXZwJFLS0hCFclRWebGv3ezbmBsXIay5CNvf1kfAzSjbJo5/dBISnFLfEBERERERERER+QoD4N46Wg8cLAc+ewk4sAHYvxqo+gPwrzuBykXA3nQx7hHgo6fENC8CX+0CzHXqhwnnTEfmH+JRvzEfBYfj8eeVUxzSX1jeq0IpImG82kVUNDwWxkuB0j1V9tzTXouPxiVBan9PjQ6GoboQf5Qt2XUpWWzrHg/jpFB1TAfDlRNxMxpQVat+ICQKMWKysifXodjU2JHf3BCMkGBdUL2nDtdgb3k5yjp19R2t0DWx8YhxDriPMGCMeDE3e7Gn3e3bS+ORHGrBpl2643bkdZRuAxJumYowdRQREREREREREZGvMQDuCRnA/vCvQNlNwGvTgH8vAd7NBGofBUwFQH0J8PVu4NBbQMM/xLhC4D+PiWkeBPYsF5+bZftc7Vrgu/3qTAevsKS5tlQav54L4znKKDvzNzLvdAzCOseQhVCEXSZevmjs14cm3ryqGFuLdd3WCnxQtgYJjfmY+/uOtCa2dS/H0l9eiPMvcOp+ngpRUlBRr+bVDohE6uNpiDtciKWT4/Czy+MwbWEmCnaZYHbxsE+vTViGv+nX2d7NE3vXiQEYpvb2jUgkzouE5Zkd9jQoMv1JKabAOKkHKWiIiIiIiIiIiIg8xAC4O60/2ILeMnAtA9gfPAGYD6hv9oBsOX7gb8Drs4Htk4H9/we0HFLfHGQC1HBroLuwaxMsLhseW2Dpz8h3V8ZNx4P3TAHKS/DPjmdFCrFY/EQBNha47jbc2NGy3XDpPGzcsx97dm7G2pUzEPHtdmSlJmKCcV3v8pyfgiLikxCDQpRXyg1rxKsv7wCmG5HQ21QvREREREREREREXWAA3Fm7xRao3pFoC3rLwLWvycC36Vlgp9oqXAbbSRFyXjgMqEbdZ+oIvbYG1FUDhosiHB+++K0FTWqv3eFG9HXimeCxMnlHFRq0rCbnyuB2E0IiYhF3tZvuIn3CFyHAgJDwaBhnp2Ht5grsyZsjykQOCnadKpF+Hxk3FcnxQMHLr8N8sBzb5cNPkyY6pL+RYu7ei/f2LuvcSp2IiIiIiIiIiKgHGADXWNtseb21oPSJ79U3+pA+2F73tG14sLt0Im4OakDu5vJOeb4tbxYht8GAm6+OVMcAhpGhwL4tqDCpIxQWVLxU1Plhjz4lllG6XbxOQZTWqDtmIlKCavHU3ys7rTtq87H0/nyUfmZ7x1xdhIyFmSg9ogzaBZ8XjgjxanbdBP40FoLrZ04BSoqR+3QRyoKSMPVKF7nOWxtx6KjaT0RERERERERE1EsMgEutzcBbqba83icjLYkMttfkAOW3AJZv1ZGDlCEWKx6eDmxMxbR7C1H1mRnmIw2o2piOaXMLgelrsOLqjsBp1A0yp3U1MpJTkPVCOSp2FSErJQ6rLfGIU6fprfr3K1GxW+vKsWl9DjLEMmZvaEDEXbciQWvGLNZ98UPxOLQhFbPSC1FhahTr3oi6Xesw+9ZsFP/LjDHn2NY9+LyRaHqzEEvvyEGZMp0Z9fvEui/JRk3QdCTHd7RxD7k4GmEox1NrClG2uxr1nsTGvzXp1tmp+6inrcvDxf4WL4WrsLrE+/kETzTCiB3IW1+NsDuSENcp/l2L3GmJmHytEXkfqaOIiIiIiIiIiIh6gQFwGXB+Yw7w7V51xEn0vQko/zXwg6v8H4NH8A1r8ObGZRizKxOzEsbj8vEJmHX/doxZVoA310xxTJsxbg7+ukEGwSuRd28qZqdmo+bKXPz1jkj5bEefKP3fFMxO0bpUpGUXogITkV5QgZ3Loh2WEzIzT6z7Iowpz8bsyXFi3eMwOXUdDsQsw8bnliFGm3jUFPz5pSz8d3M+5irTjce1xnRsMiRh7T/WIFGfG/vSedjw0HQ0bcvE3JRkPKXk0e7Gvnwsta+zU/eSQ3N5LwTDmJGH1ItNyF0i5nPPFu/SzARPRPJtcgeEIjm+oxV/h2CEhIn3g8Iwxjk3ChERERERERERUQ/4WQW1X6ENKv+L/vb2dmVcm3iV/W2trTghOkuLBU3mJlx//TRlenc++bgXD47sazLQ/Nb8U+9hlIFnAHFPAKMuU0cMXpYjZiWdiCE4GIYA2zh35LTwYLr+YjGLdW8TPUHBCO4iGm+fziCmC7KNc6nNAvNRuYldzKy/NIt1Fuvr3b62oCJzPGZXpWFn8Rwl1YtLcl+cIseQiIiIiIiIiIh864MPP8TI4JEwDDNgSGAgAkTn7++PANH5+fkp/aIHfur0clxvDN4W4Me+BN6YfeoFvyX5UMw3bgWOvK+OGLwMo4IRLDpPAq1y2lMl+C3JoL1c9+7i1fbpugp+SwGGUyP4LQX1YF8feR2biyyISY53H/yWGPwmIiIiIiIiIiIfGZwBcPnAyz3/0z8PuuyNPSts+cmJTme1JcjKycbSW5ejeOw83DczVH2DiIiIiIiIiIiobw3OAHjNWuC7GnXgFCZbp1fdpw4QnZ7qqregorwSdWGLsPG5NMR019KdiIiIiIiIiIjIRwZfDvBDbwGVi9QBd/yBH48HgkIBw2jA8g3ww+fA4Wr1/R6SOb3PGAcMGwOcaAKO1gPf7BX7uVWdwI3LVgLn/1odICIiIiIiIiIiIjo99XcO8MEVAJepT3ZMBVoa1RHOxM69eD5wwS3A0NHqOJ1jXwOmZ22dN85LBi78LRB0rjpCp/V74LOXgdpcoF0+7tEFfwOQuAsYcqY6goiIiIiIiIiIiOj0w4dg9qWGV90Hv4eMBK7OB352p+vgtzT8bOCS/wFi1wEBw9WRXZCB6wlrgMvvcx38lgLPBMJTgIRNthbnrsjA+CfPqwNERERERERERERE5InBFQD/cL3a48KVjwBnxagD3RjzX0DMg+pAFy5LA8ZOUge6ccb5wC8fFUdkqDrCiem57lOlEBEREREREREREZHd4AmAf/0GcPQzdcDJuFlAyC/VgQ7tbW1465/l+PtTT+Of27ajpfmY+o7wkynA2OvUARfOvkbM91fqQDfz0gRHAJFL1AEnxw8Dn76kDhARERERERERERFRdwZPDvDdc4Fv96oDTq4rAUb8VB2waT56FCsXLsH7ezsefHle+AV4eH0uQs452zbi8LvAm7fZ+p3911PAj69Uel3N6/yIcGVePz57jDpG1XYMeDVevLaoI3RGhIl13aoOEBEREREREREREZ1emAO8L8gHTboLfo+6tFPwW3os6xGHgLX0qeljPHRPujokjP6F69zew8fag9/SX/70cKd5fVJnwoP/s1Id0pG5xc+dqg44OVoPNH2gDhARERERERERERFRVwZHALzxbbXHheAL1Z4OR3/4AaWvlKhDjvZVvYuPav6jDgkuPq+kMlGZv/sO24tdt9pW5lWrm5dG9/lO3AXyiYiIiIiIiIiIiMjB4AiAf/OO2uPCsBC1p8M3Xzeqfa41fn1I7ROGO6UwkXTzbOxuXge/Vvt0XKyT3TddBPOJnLVZYDZb1IG+YTlihqVNHSCiLp3U88VihrlZ7aee6YdrKp2mRNmo31eJit2iq+267ke+ZTGzHkKnN9aliYiI+t4gCYB3ETRu6XyTEjouDEOHDlWHOov4+cVqn9DyjdqjY/lW7QF+ev44BA4JVIc6C7/4IrVPx3JY7XHhmyq153TRiLLMZEwzZqL0iDrKBfP2TDFNMjJ2DfCbxup8sZ1G5DlmxOkjZpSuHI/LfzEeq/toeebt6bh8vFjGX/plgwalqvVGUWby4emZb/moBFkLE3H5BRfi/AvGY/LCdag4qL7ZnYOVyF0ol5eOUlenYmM1Cu5PxoRL5LwvxOWT70TubnfnrAV1JdlYOHm8Mu35l8Rh1op8VDhN3rgtXTknuuqcz5debaOvNe5Amot1dKVX54sXy3HNhLxpYtmXpKJ4gF9m+07fX1PpNNXWgE2/G49rjSmYnZKCuVsa1Deoz30k6lXinPzZ70pEjZNOHdXIk9/h691cLD8rwlLx/qzFhagZ5H9T7N+6dCNK02XdyvN6JRER0UAx8APgMv+3+SN1wIXmL9WeDjLx+u13LVSHHM1InoWzx56jDglHXdzkNH+h9gBDhg7F7Xf+Th1yNOPmm3DOuT9Rh3RczVNzoqnr7TnlhCBhzgwY9hUi6/FKuKzjNpdj9QpRAT4rCYsnddH6fSBoa0TNvlo09ksrDwPCfhaLqMgpiBirjvIxQ1g44iIjkRgRqo4hnztcK8qMZ7f1lneyMS1xOQrqI7H40QJseDgJY/bkYPZ1qdjUVTymzYyaF9IxOS4Fq7fbltfkXEYbijA3IRkZ24bh+pVrsDEvCymRDchNScC0x6qdzm0LqrKNmLwkHxVjZyD94TXIvnsqhpVnY3aC47oYxkQiIT7eZTd+jFivfSaY1WmlHm9jX2lrwiEPz+lenS9eLMe1YIRdFY2oK6IRFqyOOmV1Ezg5afr+mkp9pW/LlHn7GqTtCkHqhr3Kw98/uCdafYf6XHAY4q6IRExMqLjK0amkUXxn1bhq0yPrEzemo7g5Fvc9PAdRBnX8qaqPG6/0b106GIYAz+uVREREA8nAD4CbP1Z73PhW1GaOd26a/Ou5KVi56kGEnHO2Mhw8ciQWLLsLd993rzKsOPq5mP8BdUDHbAJ++EQdAH6TersyrzFq4FyZ1/K7cXf6PcqwA2sb8NUudcCN77vZplNN+Bw8eE846jdkIq9WHWdnQdWT2ShojkbmfUkY4OHvfmZA1Nw8bN26Bkbd32x8yRA5Dxu2FmPtdB65k66tFnkr81EXn4WdxWuQOj0WCTelYeObm5E6thxpfyxy0zrOhDxjHKY9UIPxDxdja1a8Ol7Pgor8TJSNnifmV4DM2dMRNykJKx4V02dEoiZnHV7Rt8Del4+l602IWLAZewoykHrTdNy8IKNjXZYUol6dNPiKOVixbJmLbg4i2xqA6DQYL1Mn7vE2nhpO7vkSgsSHNmPr5kWIOdWDDYLbwMlJ1ffXVOo7fVmmLM3fi/8jEP4zNQQbYHuhfnDOFGRuLsaLC6PFGUqnvOZqZN2ejrKxSXjib2mICVLHn8r6uPFK/9YNDBg5Qu0lIiIaZAZ+AFy2mO5KuwWofUwdcDRlxo3YvGsb8l9+Hi+/uRO3zP+t0jrcrmaN+M9q63cgxu2X73WQ89r02j865jXvNsd5aT7ZBBxzkRdc77i+PeTpIWruKiwON2H1/YWo01cgTUX402MmRNzzAFLC1XEqS0MtKnaXo2xXJSr2NbjMjWf+SLxXLd5Th+0sDajaXYk63d829NOaTZXYtD4HBe90vy+7Ww9LQ7V437GVqo0ZdWIdqhpctnsXFWrx/juVYr7lHm2f2SSWI6Yt2+1YCXc3XnK7bvpl76522E8OupvOxX62E+/ViPdsn61FvYvd4LB+R0xiXrb9XGXysox7uj1Cx/6qRM1nbo6NpM7TNq2beeq3X25vSSFW5+ywB3cV+v3wjthWF8fZrhf7wFJZhFxTKFYsS0KYPvgSFI3Fd08Bygvxqkkd58AMy+hF2PhaMbJvisQYlxmbalFZZIFh5lTEOd2sRkxKQhzKYdK1vq56bR3qg+Ygc5lTQEKsy4qMOTBU56O40x/DHFneycfj5Qak3JWECHV7er6N7tnKYHXn8imOhczl2+n8lTfCYnyFq+OjL4eujrWn54v4bGMXRbPb5bjR1fWyu3O1M7EO2v5Rcx978nnPrqdVtnOooUo5Bi7X2RXl/Ol6PfT7QFkXMa1yLXB3nXbS+Zqq3w+646JvXeduvMphnuburgFOy9tdhLycQlQ5Tapc59zNRy3bNS7/WiSOZbXYR5X6bbRxuHa62l/68q2cJy6Wr5S1rrbPRl9Out8PXZe/XpWp7s4NdZsrPpDPhjkE09uiX9sHLtnW212OcNu6is87bfLJrg/1WRl1V568KqO65TlTzz2Pv8ed+PaaqfLisx7XVzyYp/4YyvJUvDEHq7c7/WxKPx8vvls8JoPfxmTkfRWP7L9lIdFVg2dv1sGDaR2O4cGO88iza75a1t631erq3xf9YnkdZc19Wbftb1G3cPncDdvn5PeBMiexHV2dox5/V+n3h3qdsK9HFx8jIiIaNKxO2tvbla5Ndm1t1hMnTliPHz9uPdbSYj3a3Gw1m83Wbw8ftn755VfW/3zwgfW88yO67E66z7dYra9c1n33eYn6AQ999JTr+ei7T19UJ/bQD59arVuucD0vfffhevUDp5eWilXWa86Pst7+0iF1zCHr87eLcnLdGuveFnWU1HrI+s+HZlgvdipLF9+4yvpP7aOqvavEe7dvFnNy8tVm6+3iM396Wx0WbNM+ZX1+Tce8by/q9MkOrfXWV1eq08ZMtd4YG2Vbj6THrfuPqtMIh4rmi/GrrHvV4Q5V1j85L+PtVco8/vSaeO862zrYu+tWWl+tV6dT2db5Wesrf3HaH9eJ5TXVW1+5M8ZxHjGLrK/o5uFq3Vref8r6mxg5fZT1yhunWi9TPjvV+vt/OC7co+lc7GfpUNkq641RcnpdFzXD+qcyx/2trd8/xX65Tj+t6K5bVWXVFwt3PN0e69E66zPO+0t0l935rPWA7nhKLR8+a/2dMk99F2P9XWGd4zpp2//3LbrpO/a3u/3wfxVN6hQdPi9apK671kVZb1xTZf2nLAMuy5ejvWtk+VxlfatVHaF3aIuynh3nnhPdZ2zHZL71+a/UEd2pe9Z6o5h3RxlQz2tX56Wk7jO366JQ5/ErcWx069arbXSn5nFxXYqw/rHMsbQdeHqGWJY430U5dKBMH2X945vq9FoZEOf0/91ou0bYO6fzsVfnizfLccPV9fLzf6xUlx1jve7GWNt1JirJ+tj73Z196vXt7xXW5+c7rU+UKD/O6+Phdd1W/hyncVuWNG7mrZxrbztuh7YP3hLLcZxefDcVdb8TO19T1f3w9BbrY07HRV7Dmur11wZbd9mdW6yfdzrnRLkW59KvnMrBZbc+5fB9Y19e/mZdOdCdr55c51oqrH8Uy+lUtqWm7dYlYvpr/lKjjhDcznOzw3bYy2iR8zbb9m3T22ucyrntGudwhNyWkzXWtxwum56Xvx6VKcGb89JhGtE5n+MdWqxvPSDWN0qUoU7XsSbrq3eLz18j6hjae6dIfah/yqiuPHlVRtXlOa2/19/jLo6Zb6+ZXnzWi/qKt/Wt51/S1TV0+9eb+opnbMfFvgxRp1bOVXl+itsdV7xZB0+n1Y7h/rIHnOqZ4vqzcrvjNawTtS7i8Dl9WeuirH/6rPVXDtPqqPWO321R19XtOer5d5Wr/XHxfPH5v8vjrjv/BGXeDt9jREREJ4eMKcvYsowxy1izjDnL2LOMQctYtIxJy9i0FqfurYEfADcVug4id+p+YQtqW7vZqe2iplSz1sXnXXQlE6zWL19TP9gNc53VuuMG1/Nx7vY9rH7odNNi/ecfZOX3Aes/RQW+6R/LRKUuyinw1GLdu2aqKDtTrX98rd7aolZMW+rLrH+UAWOnYLmrmxOFu8pkVJT1svnPWvcfUmfSRcW3acsiZT3+9KauMv2pmK+oYF63riM4oN1UdK5IurgpUwPgF4tK+u+LaqxNnmyfWOcb7+kImDRV2ILFF4vxv/pLlX0eTTVPKdusv2HstG6tNdbHrrFVig9oy2ltsr61SuzzqGXWVw9r4zyczsV+bqlao6zfdQ+UWT/XPttSb/3nA7bj+n9VHRtoW78ocTMvyoRWOZc3/Mq0US5vSB14up5KYEGWPXGj+XbH8Wx607YvrxH7zL5WX21RjvHFspzY90eTdf/T8iZE/wccQd1+eTzt5VUrU+IGp/N+qLO+co/cNsebkRZRLuTNkJz2kPb5phrrM/NjxLxFGej2RuWQ9ZX5bs4FhdONaBdsx8Rx/dwT5+uqeLFfH7C+1XFYrW89JJYlgzjqsAOxrfJm7uI17tfFtj90QWaF77bRkVqGHD6nu+l12g5bYFyUV23V1DJwWUySwzWrqeZZpRydd+cWUfpULs4XezkR57i9DItyYgvq6Y6DN8txo9P18vAW6+/ksldV2K8j9iDFdbognEu2/X1xVIz19qc7rmX29XE4Tt5d1709ltp1RL8e9u3QBxMFh+8BbYcp55rT/naj8/Ve2w/imr5FDUpo1yB5bZPXnCp1QfI6km9bV30ZsM0zxnpZrNP16e3HlWDjxXdv1x1bx/2ufZXZrjvyHJHXufnWZ97Xzed92zG5eP4W+zHZu0pO53jeSvJ7+bzzZ1ifEVUSG+3a6TRPdd0cr53q9TDmVt02q8dB7ovrlllf0QJf4jr/6h/EtcNpn+9f56Kc1G2x/l6WE4cy5U35k7wrU958j0leXTeVa6Dz9U1QA7s3Pq3t/FOnPuS7Mup5efK8jNqW51DX6sH3uKv6hk+vmR5/1vP6So/rW5+q47VlelFf8ZzunBPb+aqclwxQO/1R0s6bdfBiWltZF2X3xpXWV+q07W6xHtiy0jYPXX3eLbXu3rmMdFXWO/6Q/7kyssP+dfLa17ke4fYc1YqBu++qevXaq59WrUvLewWX+4QBcCIiOgX0dwB84KdA8ThdSDtQ+xfgzd8CX7wKWFvV8SqZKqW+BHh9NnBggzqyG20twJ7lwHt/cngwpgP5kE5TAVB+i5jGwye4He8mrcspy4CEO1chAYXIWJWN1dlif05fgxXxuiQJR3Yg7zETYjIeR+akUBjUNAeG0Hhk/jUDMaZ1yNvh6TF1oXkG1j46B1Eh6jK7yNNZV7ND/B+PxKuDbSOkcUl44s0KbF0QqY7ooZnLkHlTJIL12/foMkS42j6xzisemG5P+RAcuwzpohhaMAf3LYq2zyM4cg5uleM/qnOfB7mxBlWimMVNjUeEttsDghG3bDP2VKxC4ih1nKfTdWJGaf461EVn4Ik/xCNM+6whFAl/EMc02oTc/B3KT3A7hGDxqgwkaDl1A0KQkJaGFLGFpW93kyfD0/WUD2EVxc348FNYfEXH8Qy+Og1r/xCL4Mpy1Kg/D60pWoMysW+fkOVE+7yYZ9RtuXhC7N+ynCLUOP3MNmL5mo7yqhwPM4rX5Si5qjc47IdwGB/KQEpQOR5/Wdu2RryyLh/16j4LUY+nOKBIeXQ1blYHu9PS5U+VQxHuKrV3L1neycHv1zci4aFFiNO2UbjkyiQYGtZh8y6nstzWgE1PFoojKz7r9ue42v5Iw/yrdTMV+mYbIxF3gwGWbZWoUcfAXI3KcgNSM9IQ1bAde+1pVUyoeEkct+mxGO+4ahj52wccrlnyfFxxRyiwrQZ1tlEuqOUkfBnWrpreUYZFObn54TVIufQQKisdvxd6thw3TDUoFS8J14lzQCt3AaG4+dEK7ClehChtXBcs8Q9g7W0d1zK5Pun3iOtjeSX2aoe/j6/rITNz8cbOUjyhWw9lO+bOARqqsd/5gtg8EQ/KB69plwJ5rolrThTKUemcp8FDlhvENX26+nt+5Rpku4ZhzkosjlYXJK8js2cr4+s+dV4pM8Yvf9zx+nTFIvxt1RRYSh7HK06pfSw3rFa2V/sqU647+4qweheQkpsryo5uPpeK61mu2Be71mDTPtu4mBsXIay5CNvf1p+IZpRtExfK6CQkaCnJlGtnCFLznOYp1m3tymjUr38WZU67LOLODN02i+Og7FsL4u5YCeM422h5nU/87Txln9vTJx0pQe4jJiRkPeVYTsKnI1OmTipfh01OXwkelT+v9eR7zAvRM7A41IJNu6qUa6HG/HoxihGN5Hh1559C9SEbX5RRz8uTx2XUhZ58j3ukN9dMTz/rcX2lJ+U0HCvWiPrWOP0x96a+0hNNqHpkPha+YELE3auw4gptAXrerEMP1rc5GDc/kAFjuLbdBkRMz8LaZaGoe6QIZfoi1gMuy7qo214/cwpQvQVlnyljbdpqUfb3Bhhum+JQb3LJw++qqsJMlAWp5V2bVtalRTlIv7iXG0dERDSADPwAuJ+f2uOhw+8C76QB//gvoOJ3tjzfb80Xw1eLGsYfRD3uP+qEXvh0M7DzBuCNW23zkIH2dx8U818AbIsH9q+2Bdg95e02nUrOETeyosJavzEfBYfj8eeVUxye2m95r0rcIETCeLWLO5vwWBgvBUr3ON40eiU+Gpd4+MCdiChRcUUhcp9wzOFnGBVivxHtqZQb4js/rClyKm51tX2d1ll9gM2EcMc8yJ482CYkCjGh4ubvyXUoNjV25BE1BCMkWLdGnk7nzFKFqm1A1IxYe95mu4BwxM2IFGW+CnsdNnAqYpz/nmAYqeyf+qZujrSH61lXtR31SIJxkr602UTNLcDW4mXqgwFN2PtaAzBzYqdc13L/ygc/QgZEP1VHqeIuc05gb9sPCdPFzZnzfjDEIH6m2La3a2x/qFCCrW72WVCsMu2pyPJRIRb+Nh9Y8CyemOn44Kbg6xYhcxJQkJqA2Y+VoMrUoOTGzLolERmIRYI6nSv23N8yz7c6rq/FXDcHBl2g21JViWJRXmJ/HYvE0FqUV6vByoNVKN8HGKde5XDdkhKv6PxHsTFjI8T/JhzQPyBUTztf5kztHDgZNQWZxcVYO1MNqqp6tBx3wqOQKF4KxPlTpc8vGyTOHw+vk3FXR3faFyNHjBH/f48m9brZ59f1AAPCwkM7rqltFpiPNKLmP/KAVqLhK9tou9h4xDj/EW+EAXKtzc09W4uECVEdy5fUa1hchOPx08Z3Fg/jxM4PQAueaIQRujKo6rQ8QbvOxcd2XoIhdiJuRgNKq9RCfmk8kp2DsEdeR6m8bt0y1X7uKfMMmoFEF/MMu2oGoiDO74/UEapO10N130ac57R96niNrZyI/TDJaZ8Jhitt61/llDvbk/LntR59j3khIFLs41BYinbo5mFG2fYdohgk4Xr1jwSnUn3Ixkdl1NPy5GEZ7axn3+Me6c0108PPelxf6VE5jcX4i9RejTf1lZ5Yn4pZ6xsQFRmOuuzlyHUVS/dmHXqyvrLMuQi8R4myECbq+ZXqHwZ7ylVZl2znRjWKd+v+kF1bjs0Noo5zY6yb7wIdj76ralGxTfT/egoSnMugVg6IiIhIMfAD4EN/pPZ4SbbebqwUNdFngG/eBtqPq2/0wpH3Ra2sBDiQD3z2opj/vzu3NPfEUOfa0OklLGkuUmXPr+fCqLX6VZm/kZXEGIR1vv8VQhF2mXj5otGpRUvfCL5hJZ74dTgqHknGtZdciMsnp2BpTpHu4Tc9FYlQ/V2/XTAMZ4mXvtw+ceOd+nga4g4XYunkOPzs8jhMW5iJgl1ODw/ydDpnRxqVB42NH+fyACJsXIz4vwGHnB42Ncz5JsZTHq6n+StxxxUb2dHC1i0zGsSNUFREmMsbE0NYuLhBr0WDqwdp6an7oWxFHM6/4EKn7hLM3Sje3NOgTIPmJuV4x13kIsgh1sI3T+tvQMMetdcXGoqw8FeZqIjNwoZ7nB50KclWbX8txdrZ4difsxyzJidgcoqYPiIDWx9dBBmuTXAODCoasGmV69bf3evFNl46ETcH1WLzbltwsOZfRWorb1vr8LI3q5VjZK6qRBnETWZs58AEhqiv3lDLietj70ZPluPOqOlIz01CRGUOZiVcgvPHJ2L2ihxsesf28CxPGAK7P079cV1vfKcIWQuNmBYntuPCS3D5+DjMWiO+w2HpvC1ilYepvaeOcIR1ji2Kr4UQJdhRUa8LoLihXOcuFfNxffFC+KWibH+l7eVIGO+IhuWZHahQv9LMu0tRKsq3cVLHiijzbF6HWZ2uY6KbnKn8aqKuvsehMQe2clKOpb90sayfp6JAvOu8Hzwpf17r4feYN6KmzENUcyG2V6o7Xw3sJs6cCm3vn0r1IRsflVGPy5NnZbQzH32Pu9Kba6aHn/W4vuKrcupNfaVHDIi5ZzNeLLa1Sl+9JBtVzn+c8mYderK+E8T5ovY6GB2i1Ed8dQ3rJHgijLOBqs3l9l9o1ZQXoT50EYzR6oiuePRdZYFZnHpxory7YvuDIBEREUmDIAA+Uu0ZQHoa1D9VBKjVuUB31bomN+kRLLD0352eEsRLzCrGB/vLsDUvC4uvOhM1hemYdW0cFpb0prJsQmN/bocTw6XzsHHPfuzZuRlrV85AxLfbkZWaiAnGdajS7XdPp3OlyU0rSkuz79P3eLye4gbH093uthWoKIDeHLq4ux7HxoIC191fxbqq00mHjvSmUITgQtmqy91NqlncMIobzjjnFpg90ViOjNvTUXHeIjz3aFLnFlgacf4YH9qM9w7sx3t79+K9/+zH1ixx83+0QbkRjDi387pYducjq9pd6+8+3Ea11VjN61VoVFtTJVxja1kadVUSUFKJveKitLeyBIiPR5yP/wbZu2PfO2E3ZGHre/vxRnEesu+IRfD7hUhLTsCEJSU9b/HnUt9d12ueEOd7ciZeaY1B8so8vLizAu+9ux8fPD1HneL00FWKnzHDPAz0irLk5uoLs1MgLOzqGYhBIcqVIGwjXn15BzDdiATn8h2ShExX1zC1W3yliz8I9VgsFj/hejmy23CjF38s6qU+/R4Lj0dyNFCwq1I5Xo27tqAU02Gc6LwvT5H6kMonZdSL8uRxGXXBV9/jznpzzfT4s17UV3xVTr2pr3jlplw8tzAShoBwpDyahYSv8vGbJUWod1GWvFkHr9ZX7KIWtdeVYIOHZddrBsRNnQPDviJUyL+vt1Wj+MkGhN0S71GKMW/s/aqPgvhEREQDyCAIgPc+UlH3FXDMYlWHeu5EqxUfdt9ApnunewC8CyHnhYvqYjXq9PnyNG0NqKsW1cmLIuwtpBTfWsQtopPDjd7nw3UnKFT5mWTqQ49j555SpEebUfpIUUe+YIVYB+d7EHMjDqm9jiyoa3BxayO2z7RHBnSdtq8vBBgQEh4N4+w0rN1cgT15c4DaHHEz7rRenk6nCYlARBCUlBeu1JvEAQwKx4W+3sBu1jMsIh5oqEFdVy2hlJsxWx7p+hqTy5tPs6kG9YhHuOsGVx1CQpWbr6bR4Yi7OtZNF2776b46ret91ogDTukF3Im4bDrQvAUVLn7Ka3m/EqVi22KjulvxbjRXI+s3siXmPDy3eRli3Pzk2/xRJSp2q/tQHJvgUcHiBlN5C+Z/laIM0xETZRvu0IBNOYWwdNH6u++20YDxsWLeMm/wO1UobYhEfLStkBpiYmFEESqrqlD5MhAzKdZ352c354uii4CTz8gUIpfG4+YFGXhi517s/EM0zNvWdMq33FM9uq57rBqlj5tgmJ2LN/MykDI9FjHhIQiWBe6EOslpoRJ1rlIyiGtOhXiJcdPKU6/L69wRE2pEMXP45cW4GUi5ASh4+XWYD5Zju0w9lDTRIaVIyHmx4jIkyseVrq5hti4m1DfBo5BzZXC7SZwWrpejdBf5MtjuRr98j4Xi+lvE8dq4BWWirvDPbeUw3JaEBN3mnXr1od6XUa/LkwdltLOefY9bfuhUicMh5/RJmt5cM7v5rMf1FV+VU2/qKz0xWpf2KTRJfR5BJjJe1q13D+pMXq1vZS3qXPydwPJBraiPiPmd1+Ot65YhVpThoFo8tUMc4OrtKGiOxvwbfJmWxFbeLbtluXZmwf73ytX+DjF378V7e0UdTh0mIiIaLNgCvAsyYP0/+f5IfSwQc9YEor6x50HwI98D8x8LwO/WBYpXfzQdVd/oCcNotWcAUlIRNCB3c7lssOHA8mYRchsMuPnqjoqjYaS4g9m3xdayws6CipeKXLcS9VgDyh5ZjoUbnO5mAkRFWS7+sMW+foagM8X/RWoLpQ7124pQrPY7K316C+qcAlv1L68TFWOxfVf2Xb4+c3URMhZmotTpxipY3GjLGwqz2tTM0+k6CYhEfJIB9U8Wocz5J67N5dj8ZAMM4ubVVy1fPF3PkCsnior+DuQ+73x3akHZ/Zfg/GvXoUZZpxDETYoGtj3b6YFeaDPhlad3ANETEeeUuqeTgBjE32ZATX4RKpz3Q1st8lZkIm97g60MqdO63Ge1RXiq872LS8GTksRNljh3nnJ66FVbA17ZUAhLeBIS9EXroKnrG2xnYj6bltyKPMzBhufS3Aa/FXVFmJ1ixOpyp3LSXI3cv+xA2IJbHQI9Utetv2283kYvBF+dCCNKkLdqC2pCp2K81tA0OBqx8RaU/l8OSpsjYbyq+yCPx+zny1OdyjBMhUqagKXbXYVwfKN+Vw6WLszv9DC4sAvkXycau3hQqZe8vK57yyLOmzE/GdMRZFGI74Fd29X+00Etnvq7rTVwB7ENfxfHx13aHSfada5gi/PFS5ySW55FKaIRf6U+GhaMhKlTgJJi5D4trj9BSZh6peNeDIudorTAfVYfrFKZt2dj6f8WoofPDe0sZqItUNRpPwi1+Vh6fz5K9bmT+0o/fY+FTJqBRHHNKV6Xj83l4hyYFONYhk+Z+pCm92XU+/LUfRntzMvv8aCRSrB0k9oa365hOzaXqP2q3lwzPf2sx/UVX5VTb+orUpsZdSbP02Q5M8Sn4bkFoSi7dz6y3lHn2oM6k8frq3BR5mS9oaBQCconXKyO6wsB0TDeEYr6v29BVrH8I/8MJGgPA/aJEFx/03QYqrOR8YLjNsoHlWfJlDDOWhtxqDf3oURERKepgR8AP1NGMXr20Mj3P/XDXpNtFx3+3g93rQ/AgS+VQa8cPAIsyQvA5422eZm+8scb+3vxIMszz1d7BiBDLFY8PB3YmIpp94oboc/MMB9pQNXGdEybKyqq09dgha51aNQN88SNQjUyklOQ9UI5KnYVISslDqst8YhTp+mZUIw5oxal/7scaRurUX9ErMdBEyrWL8dSUZmMuHOqWK6NLShnQcHiZCxdX4KK3SXIW2LEtJpQ3KxO4ygSKZGvY9bt61BmarRv39x7y4FJq7DY69zHngs+bySa3izE0jty1GWbUb9P7LMl2agJmo7keFtwxNPpOjMg7u41MIqbjbnGdBS802D77DuFSDPKlsPTsfZuDx784yGP13NcEu67Kxx1jyRjVk4JamS5+qwWxTm3YqE4ngnLkqA1SA779QNYES7KlDEZq3eZ0Cjm2Wgqx+pbjMioDseKh+Z08fAtjdgPd6xCwuF8zE0W+2F3x3xyb78VWS9XwjwmRN0PYtp5GUgQ+2yhnFbZZ42o27UOs+dXIyxemah74tyZ/4CYuOROzBLnTtk7tah5p0Qsz4i08nCkrprXcSN8sAhz4xIx+Zp0lHoUvGpE8e8SkbYrGMbfToThQ9nC26n7qGNGwVOXI1s+BFOcE2kby1G1rxZVJflYmiwD6PNEGXDKG95m6rb1t8KbbRTqnknBNGM2yjzZRiXQDVRVi30uf56sjpY3l+MnRqJejK/XB8Z9Qi0nKLEde6WciGO/W5wvv8tEVfgipLh4EJqvhI01oEYGnVZq11q5bHGcVohrrVh2oswx7AteXtdlruGoG8RL4SqsLnEsW51F2v6A9PgD9vPV/Fk1CtKTkfW6r640/SEeEZ8txm+crk9zNzQgJmM5jJ78mG3cHDx4TziqMo3iOleOuoNiPuJ7qywnGdMyqxFxzwNIcQq8BE8R31/Ygbz1otzfkYQ4512mXjvL0o3KA22VdVOP3W9WyID0SIT5qoiKcrL4oXgc2pCKWemFqFCu6eq18NZsFP/LjDHn9PSYelOm+ul7bNREGMVpUbo+H1Whi5Ds/GDIU6Y+pPFFGfW+PHVbRl3w6ns8eCKSxTXEsnExZi3JR7H4PisWdb1pxlqE3aROo+rNNdPjz3pcX/FVOfWmviK+Ix+Jw+TJCZiW3/kPbZ6ROcFt+cDzfrtS/eOvN+vg3foqLhVf7vnzRX2kUrkuNpoqUbBSDMtfFDzoWG9wSXuA6cM5SvnwqvGAoDxssyEfeRstSPztDA/qkN6Rda4/y+eV3JuICamZWJ2Tg6wVyZjw20MwLnOuRNYid5qo/11rRJ6HvzAkIiIaKAZ+ANx/KPCjnrUsG/sjYGhgR6vvpqP+WLguAOu2+cHk7meROl8dBp7Z5Y/frg1A/Tf6gLcV4WN72Jp8iLgzGOHTpgOnnOAb1uDNjcswZlcmZiWMx+XjEzDr/u0Ys6wAb66Z4vizRnHD/9cN8qavEnn3pmJ2ajZqrszFX++IdKz89kDUgmexcdkYvHp/Mq4dL9YjLhGz/1KN8Wmb8eICXZkSN6npL2XAGGxCcfZyzE5ZieIRc/DiyhmO62o3BpF3ZGHtRduxcHKcffuCZ6/Bzr9Od/w5s6+NmoI/v5SF/24WNw7KssfjWnHjtMmQhLX/WINE7QbW0+lcEZ9dW1aAFWO3ixvxBNtnkzPx6thl2FjWzWe95fF6ihuuZcXY+fAMWPLFja0sVwlGLM234L8fLsYTM3V7PSASi4tLkT3DgjyZS1zMc8LkVOQ1z0B2aTEWe3o5OWc6Noj9sHhsObJSOuaTWxeDFRufxYpoXQmVPwuW24Et6j4TN5hLtiPmL7lYLPNeeyjspjzsfHQOgsW5MzfZiGnJy+3LS79Ct7ygEIyR+2asePXoRGlAzS7ZnqoRxfeL8ywlpXP3ku5mWD4E89FiZM80iPMnFbOMRsxakoOKsYuw0UXrccubhd22/tZ4vI3CofpK1BwJxhiPAnQh+H83yBtFAxKdfoURcbUt4OAYGPcRtZwo54tSTsSxT1HPl+eWIaa3F7KuRC7Ccw7XWrnsHOyNScOLLy7qdQtXPa+u62LImJGH1ItNyF0iytY9W7pI4WBA3D3FyJzUYD9fL0+4FZtxJ5540PnG/1QWjsWPPovE6gd016cGxKUV47nbPP+rS9RC7TqXislxYj7ie2uucp0rxdaFLi5e4vtr6m2ykIUiOd7Vxa3j2tnytHrtVI+dYWYWtj7q2++skJl5opwswpjybMxWrumiTKauw4GY3p4P3pQpoV++x4KRkDRHqauEJbnOCXyq1IdsfFFGe1Ceui2jLnj1Pa5eQ6YHo64kG0vF99nvtwzDrX9Pg9H5B5e9uWZ6/Fkv6iu+Kqde1FeCx8jUPAaEhXj0xeqalg8cJVh6r5oP3Js6kzfTSmdNQebflgN/t10XJ0xOQcauYKTkliIz3oOzY9R0PCjOrSjTOqV8LHXxK5suRc7A/GjZMwWJV/div7kjn7nyaBl25i1DnKUKZeXlqMFUPPHaGtw8Vp3GLhgh8knJQWEe1o2IiIgGDj+roPYrtEHlf9Hf3t6ujGsTr7K/rbUVJ0RnabGgydyE66+fpkzvzicfH1D7TqL9/weYnlUHvLO7FsjYGIB2a+cW2yEjrbjq4naMPtPxPXOzFXvr/PF5o+tW3ndPa8PMuB4GwH8yGbhSbM8gYTlie6CXITgYhm6CMXJaeDCd19osMJttP6bsbj0sZrEOI7xYh2YzlFkHdeRI7i9yXZWfsBrEsrtIZ+HpdC5p29eTz3rJ4/X04niKmYrzWbzKPNa9OUBezEfbDk/KfFeUc6er5cl9Jfn6fHGm7e8+KONdb2MDNs1OQNoFefjgoXhxu34a6MfzxZl2re2Pa5E313W5Tyxif3h0LpzE/dcbjS+kYsK94Xjx4zTll0U+uQZ4c53zglfHrpfs13Rfl0lvypTUh+XKsjsTl6dUIX1nMVK6iSF7s+/ltL6sD/VJGRX6pTx58z0up23z7Dhr696T8unxZ705j31VTj3ZX/K87KvjJXl7zLqYtir7Qsz6KAt7NiQpf1zpbZmzHLHAMMrLAy7qIwWzEpAR2f/1Edu5C2RX5OFmffq+vj6GREREHvjgww8xMngkDMMMGBIYiADR+fv7I0B0fn5+Sr/osef0kON6Y+C3AJd+PF7t8Z5Mr/jogjaMPqNzwLqxyQ8lewKUVt767uVKme6k84EZMcyK/53T2vPgt3RWz7fldGSQD88TnSeVVDltn9xAyUq1h+vhdYVa3vzIefdnbVgl11VZdjc3Sp5O55K2fT4OGrji8Xp6cTzFTG3T9vYAeTEfbTt6W5aVc6er5cn593IZHtH2dx+U8S63sa0epkrAGOuUV/dU1o/nizPtWtsXx8mZN9d1uU88PhdO4v7zJZ9cA7y5znnBq2PXS/Zruq/LpDdlSuqzcmVGWVERLNFJSPCgAbU3+15O25fHyKffU328rt58/yrTenictXXvSfn0+LPenMe+Kqee7K++PF6St8fM02mF3pY574PfQu0WPCt/5TbVdykAHTSbsCk7H1XyDwEOzKgoLwdCo3GJ87Nr+voYEhERnYIGRwD8rN495/qSccCzK1pxU1w7DEO8D14HiErG1Jh2PLusDf/l4a833epFMJ+IaND41IQKTO+bnxsTEfVUWy2K/zcHWUtuxdKSUKSu7D71ExGdfhrL87E6JxOzb81B/aQMzHfO8+8rDeXYXJiN3yQvR15Jpe25K7tLkJuSgKXbDA7PuSEiIhrMBkcAfMjIXrecDjL44c5p7Si+rxX3zmrD5F+04achMhjuOiA+drQV8Ze0Y8n0NhSntyLtpnaMOlN9s6eGnwMEe5EQmIhosBodi/QXlyOB8W861QWFIOpSX2bSplPap1UofrscFZ+GYrGLZxecklhG6XQ1OhJR54j7wH5nxv7dW1BWXgXcIPPbJyGsr1pdXzQPL75WgMVhtcozDpTnrqQsR97RiUgvKMMGfd54IiKiQWxw5ACXDr4O/PtudcB32tqs+PZ74MgPfmgXO230GcBZZwKBgeoEvnTpvcAFs9UBIiIiIiIiIhuL2QJDT3LzEBER9TPmAO8r50wEzhinDvhOQIAfxvzIDxeHAj8PA84e1UfB7yFnAufdpA4QERERERERdWDwm4iIyLXBEwCXLrxd7TkNnf9rcbRYoSEiIiIiIiIiIiLy1OAKgIfNAIaOVgdOI36BQDhTnxARERERERERERF5Y3AFwP0CgF/crw6cRiLvBoaOUgeIiIiIiIiIiIiIyBODKwAujf1/wAW3qAOngTH/BUTcpg4QERERERERERERkacGXwBcuuR/gJEXqwOnsGFjgCv/rA4QERERERERERERkTcGZwBc5tSekAMMOVMdcYqasAYIPMXXkYiIiIiIiIiIiOgUNTgD4FLQuUD888DwseqIU4gMzF9bCIy6VB1BRERERERERERERN4avAFwKSgUiP878KNIdcQpQAnMb2Lwm4iIiIiIiIiIiKiXBncAXBo6CrjmGeCceHXESSSD3jL4LYPgRERERERERERERNQrDIBL/kOBX64FxmcDQT9RR/YjGYS/NA245ulTPy85ERERERERERER0WmCAXA7PyD0emDSFuCS/wGGBKvj+1DAMOCi+cDkfwAX3GJ7OCcRERERERERERER+QQD4M78hwDhtwJTXgV+thAY+XP1DR8643wg4jZg8jbg53cBgSPUN4iIiIiIiIiIiIjIV/ysgtqv0AaV/0V/e3u7Mq5NvMr+ttZWnBCdpcWCJnMTrr9+mjK9O598fEDtO42daAK+rgAOvSU68Wr5Vn3DQ7I1echVwJhY4JxrAcOP1TeIiIiIiIiIiIiIBo8PPvwQI4NHwjDMgCGBgQgQnb+/PwJE5+fnp/SLHpmvQyHH9QYD4D3R+gNgOQIc/068Hhavar+1HRj6I8Aw2pbX2yD65Wt/pFMhIiIiIiIiIiIiOsUxAE5EREREREREREREA1J/B8CZA5yIiIiIiIiIiIiIBiQGwImIiIiIiIiIiIhoQGIAnIiIiIiIiIiIiIgGJAbAiYiIiIiIiIiIiGhAYgCciIiIiIiIiIiIiAYkBsCJiIiIiIiIiIiIaEBiAJyIiIiIiIiIiIiIBiQGwImIiIiIiIiIiIhoQGIAnIiIiIiIiIiIiIgGJAbAiYiIiIiIiIiIiGhAYgCciIiIiIiIiIiIiAYkBsCJiIiIiIiIiIiIaEBiAJyIiIiIiIiIiIiIBiQGwImIiIiIiIiIiIhoQGIAnIiIiIiIiIiIiIgGJAbAiYiIiIiIiIiIiGhAYgCciIiIiIiIiIiIiAYkBsCJiIiIiIiIiIiIaEDyLgButao9RERERERERERERES91Mcx5961APfzU3uIiIiIiIiIiIiIiLrRzzHlHgXA/Rj4JiIiIiIiIiIiIqIe6q8Yc6cAuH3BbpqeM/hNRERERERERERERL3lNtasxqZ9EYvutgW4thDnRcnxTd814dxzf6KOISIiIiIiIiIiIiJyrbW1FX7yn1NgWxvyRcDbmUcpUOxBcH0wXPx31lmj8Ytf/EIZR0RERERERERERETkznfffYeAAH8ltmyLNOtizn0Q/JY8CoDrk6EoKyI6f39/BIjul7+cgPPOO099l4iIiIiIiIiIiIios2++PazElGVsWcaY9UFv1wm5e89tAFwu3L4CVmvHsPoqVzIwMBDR0dGYNu0GDBkyxDYtEREREREREREREZHOgbo6JQWKjCnL2LI+1qz06/J+K8M+4mcV1H47bZTyv+hvb29XxrWJ1/a2NrSJTq5si+U4jh79Ad+bv8feqirs31+Dd955B1988aXyeemTjw+ofUREREREREREREQ0WFgsx/Ht4W9x8ODXSsqTM4PPxIgRZ2CYYagSCA8ICIC/6GSrcBn0trcMt33cJ4FwrwLg8lXrWtvacOLECbQca8EPzUdx7GgzmltacOL4cZxobVU+J/85c7E4IiIiIiIiIiIiIjoNuQpSKyFsMX5IYCCGDB2KoGHDMHxEEM4IGoFhw4cp2UQCZfDb35YORWsR3m8BcEmO1t6Sr+1yWA1+y5bgsl8GwY8fPwFLSwtaLC3i1QKLDICfOGH7rOzUlXSzGCIiIiIiIiIiIiI6zdmD1WpMWA7LQLdh6FAYhhkwzDBMvA7D0KG24Lefv+0Zk0rwW76qn5Hkqy+C31KXAXCN7O/UClz0y1QoWjqU1hOtOH7iuNKvpElpl0HzdnUORERERERERERERDQY+PvJ4Lafkt5EpjoZOmQoAocE2tOeKKlP/GwtvvWtv/VB734LgOtf7a3A1VelJbiuXwmSy05Or/usnfMwEREREREREREREZ3enILVSgtu+So7Ncit5PlWg9xav9LqW3tV5+H82ltuA+CS/i3ZrwS4Zb8MdothV69yOmUa8SpXsYvZExEREREREREREdEAIgPXMiIsX5UAuHx1DnbrXuU0fdX6W/IoAK5/1fq1NCha8Fu8obyn9EvqsJ7zMBERERERERERERGd3pwD1sqwOs7eult0+uC3DHpL8j3t886vvtBlAFzS3ta/Kp3s11p86zvbREq/Rt9PRERERERERERERAOPPnCt9ItOjpH9Dp3a8lsbtk+ve/UN4P8DFXS0qsSeVBwAAAAASUVORK5CYII="}},"cell_type":"markdown","metadata":{},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLD6ft_gtqMQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1wPLh5pGdCT-FHii6XR3dnSBOD8UBmRar","timestamp":1696667628452}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
